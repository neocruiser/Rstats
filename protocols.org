#+TITLE: RNAseq and Machine Learning Protocols
#+AUTHOR: Sleiman Bassim, PhD
#+DATE: Marine Animal Disease Laboratory @ SoMAS
#+EMAIL: slei.bass@gmail.com

#+STARTUP: content
#+STARTUP: hidestars
#+OPTIONS: toc:5 H:5 num:3
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not LOGBOOK) date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t toc:t todo:t |:t
#+LANGUAGE: english
#+LaTeX_HEADER: \usepackage[ttscale=.875]{libertine}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \sectionfont{\normalfont\scshape}
#+LaTeX_HEADER: \subsectionfont{\normalfont\itshape}
#+LATEX_HEADER: \usepackage[innermargin=1.5cm,outermargin=1.25cm,vmargin=3cm]{geometry}
#+LATEX_HEADER: \linespread{1}
#+LATEX_HEADER: \setlength{\itemsep}{-30pt}
#+LATEX_HEADER: \setlength{\parskip}{0pt}
#+LATEX_HEADER: \setlength{\parsep}{-5pt}
#+LATEX_HEADER: \usepackage[hyperref]{xcolor}
#+LATEX_HEADER: \usepackage[colorlinks=true,urlcolor=SteelBlue4,linkcolor=Firebrick4]{hyperref}
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+KEYWORDS:


* QPX and marine sciences
** Ongoing projects
*** Project I: SNP density in virulence and adaptation genes (x4 QPX strains)
1. Compare multiple QPX transcriptomes
2. 4 QPX strains derive from 3 geographically different location
3. First goal was to study the relatedness between strains (or closeness using SNP data)
4. Hierarchical clustering was thus used with =Phylosift= to investigate.
5. Preliminary results were significant
6. Additional analysis were done on variant calling.
7. Differential distribution of SNPs and indels were found between strains.
8. Preferential substitution of SNPs were found between strains.
9. Furthermore, contigs were annotated with Pfam domains.
10. Differential distribution of SNPs inside and outside domains were investigated.

*** Project II: QPX transcripts from nodule samples (x3 conditions)
1. Transcriptomic data were generated on infectious nodules (QPX + hard clam tissue)
2. Samples are from diseased clams and controls
3. First goal is to identify QPX transcripts
4. Time: identify parasite transcripts produced in clam tissue during infection
5. Second goal is to identify a crosstalk with the host (clam transcripts responding to parasite factors).
6. Published work on this data includes: analysis on most abundant transcripts in the nodules (e.g. clam transcripts) and discarded the rare QPX transcripts.
7. Preliminary results shows over 1400 proteins for QPX.
8. It would be significantly better if the clam transcriptome is used in read assembly to discard non QPX genes.
9. Final step was a blastx search over the NR ncbi library.
10. Blastn can also be done with assembled contigs over the NT database.
11. For the crosstalk analysis, the best deal is to look for expression patterns between differentially expressed genes.
12. Over 1400 protein list annotated with hmmer/pfam can be used with blat/S.R.genomev15 and gene expression data to extract differentially transcribed proteins.

*** Project III: Network analysis between immunity genes
1. Biological samples: Crassostrea gigas [[http://www.nature.com/nature/journal/v490/n7418/full/nature11413.html][genome of the Pacific oyster]]
2. Samples from: visceral ganglia and gill, RNAseq in multiplex
3. Aim 1: Generate RNAseq data, 2 conditions, 2 tissues
4. Aim 2: Differential gene expression (edge R. see difference [[http://www.nature.com/nprot/journal/v8/n9/full/nprot.2013.099.html][here]])
5. Aim 3: Gene-gene interactions 
6. Aim 4: Machine learning on SNPs and gene expression data 
7. Hypothesis 1: gene-gene interaction is due to guilt-by-association
8. Hypothesis 2: Multifunctionality among genes and between pathways
9. Preference in exon expression, between 2 conditions (exon-exon junctions + GLM model from DEXSeq for significance)
10. [[id:624baea5-62b1-40b1-813f-8f7350966d50][Second set Bibliography (FPKM vs TPM)]]
11. Use R BitSeq to estimate the number of actual transcribed genes
    1. EdgeR too
    2. BitSeq [[http://bioinformatics.oxfordjournals.org/content/early/2015/08/26/bioinformatics.btv483.long][Hensman 2015]] =bioconductor=
12. [[https://books.google.com/books?hl=en&lr=&id=LNScBAAAQBAJ&oi=fnd&pg=PA325&dq=qpx+parasite&ots=lGKB4qA7-h&sig=AK7xD5RGJhE-WzFRj2DY8HXbeJk#v=onepage&q=qpx%2520parasite&f=false][Text book]]
13. use SNPeff to create a preliminary SNP list for GATK base calling. [[http://snpeff.sourceforge.net/protocol.html][tutorial]]
    1. use RASER [[http://bioinformatics.oxfordjournals.org/content/early/2015/08/29/bioinformatics.btv505.abstract][Ahn 2015]] to map reads specifically for SNP calling
14. Serch for novel transcripts using special filters [[http://www.rna-seqblog.com/current-limitations-of-rna-seq-analysis-for-detection-of-novel-transcripts/][Weirick 2015]]
15. Select common genes OR select by function:
    - immune genes
    - secretion proteins
    - transmembrane proteins
16. Cluster analysis by pathway and gene-gene interaction (String database)
    1. [[http://www.rna-seqblog.com/pathwayseq-pathway-analysis-for-rna-seq-data/][Pathway analysis from RNAseq data]]
17. Journals: [[http://bib.oxfordjournals.org/][Briefings in Bioinformatics (9.6)]]

*** Project IV: Cytometry and machine learning (unsupervised)
Machine learning on hemocyte morphology using flow cytometry data (FCS: flow cytometry standard file format).
1. Use GPU accelerated R package for faster computing [[http://www.r-tutor.com/gpu-computing][rpud]] package

*** Project V: Functional annotation of SNPs 
1. Gather raw data from SRA repository [[http://www.ncbi.nlm.nih.gov/Traces/study/?acc%3DSRP045796][here]]
   - Get raw reads for 40+ bivalve species [[http://rspb.royalsocietypublishing.org/content/282/1801/20142332][Gonzales 2015]]
2. Assemble contigs
3. Annotate peptides with 20+ databases [[https://github.com/neocruiser/Rstats/tree/master/nodule#gene-gene-interaction][here]] or 160 databases [[http://pir.georgetown.edu/pirwww/index.shtml][PIR]]
4. Select common genes OR select by function:
   - immune genes
   - secretion proteins
5. Call SNPs =only if genomes available for at least 50%=
   - which reference to use? genome? transcriptome
   - how many references? one for all ?
6. Machine learning (iris data style)

*** Project VI: Gene classification between parasites
1. Gather raw data from GenBank for more than 60 parasites [[http://www.researchgate.net/profile/Bradic_Martina/publication/268513612_The_impact_of_genomics_on_population_genetics_of_parasitic_diseases/links/546e141c0cf2bc99c2151a4f.pdf][review 2015]]
2. pplacer on 18S/16S
3. Select common genes OR select by function:
   - immune genes
   - secretion proteins
   - transmembrane proteins
   - spore formation proteins
   - virulence genes

** Data
1. [[http://www.marinemicroeukaryotes.org/project_organisms][Marine Mirobial Eukayote Transcriptome Project]]
2. [[http://data.imicrobe.us/project/view/104][iMicrobe]] hosted data: description of the project and fasta files + ftp
3. [[http://data.imicrobe.us/project/view/104][NCBI]] metadata entries for the project
4. [[http://jcvi.org/metarep/][JCVI METAREP]] annotation protocol for the project
5. [[http://www.phi-base.org/release_notes.php][Phi-base]] host-pathogen interactive database [[http://nar.oxfordjournals.org/content/early/2014/11/20/nar.gku1165.full][Urban 2015]]


** Phase 1: Installing and quality checks
*** Short introduction
=fastq.gz= are zipped files that contain paired end sequences. We have 2 files for each sample and we know that the illumina rna-seq was longer than 70 bp per read. fastq files can be extracted from the zipped archive or can be loaded directly into a mapping package (aligner software). fastq.gz are data from illumina prior to alignment to a reference genome.

*Documentation* [[http://angus.readthedocs.org/en/2014/drosophila_rnaseq_bwa_htseq.html][Angus 5.0]] [[http://ged.msu.edu/angus/tutorials-2011/bwa_tutorial.html][Angus 2.0]]
*** Aligners and useful packages (to be installed on Linux)
Of many there is =bowtie= fast but does not handle gaps (indels) correctly and there is =bwa= (Burrows-Wheeler aligner) which can handle different sizes of reads.

Download and set up BWA
#+BEGIN_SRC shell
curl -O -L http://sourceforge.net/projects/bio-bwa/files/bwa-0.7.12.tar.bz2
tar xjvf bwa
cd bwa
make
sudo ln -s ~/data/bwa-0.7.12/bwa /usr/local/bin/bwa
#+END_SRC

Update packages and install pysam (wrapper of samtools) Samtools and bcftools and HTSeq (python)
#+BEGIN_SRC shell
sudo apt-get -y install git curl gcc make g++ python-dev pkg-config libncurses5-dev python-pip
sudo pip install pysam 
curl -O -L http://sourceforge.net/projects/samtools/files/samtools/1.2/samtools-1.2.tar.bz2 # make then ln -s
curl -O -L http://sourceforge.net/projects/samtools/files/samtools/1.2/bcftools-1.2.tar.bz2 # make then ln -s
cd samtools
sudo cp *.pl maq2sam-long maq2sam-short md5fa md5sum-lite wgsim /usr/local/bin/
curl -O -L https://pypi.python.org/packages/source/H/HTSeq/HTSeq-0.6.1p1.tar.gz
tar xzvf HTSeq
cd
sudo python setup.py build
sudo python setup.py install
sudo chmod +x ./scripts/htseq-count
#+END_SRC

Install Bio::Perl. if problems occur visit [[http://bioperl.org/wiki/Installing_BioPerl_on_Unix][here]]
#+BEGIN_SRC shell
perl -MCPAN -e shell
sudo cpan
cpan>install Bundle::CPAN
cpan>install Module::Build
cpan>o conf prefer_installer MB
cpan>o conf commit
cpan>q
wget http://sourceforge.net/projects/expat/files/expat/2.0.1/expat-2.0.1.tar.gz
tar xzvf expat-2.0.1.tar.gz
./configure
make
sudo make install
sudo cpan
cpan>d /bioperl/
cpan>install CJFIELDS/BioPerl-1.6.924.tar.gz
#+END_SRC

Download additional tools from github for sequence counting.
#+BEGIN_SRC shell
git clone https://github.com/scottcain/chado_test.git
#+END_SRC

Install GATK. Downlaod then =make=. Must register first.

*** Quality controls
1. Download FastQC =on linux= (sorry)
2. Windows users download from [[http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/][here]]
3. Java simulation of Q/C
#+BEGIN_SRC shell
curl -O http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/fastqc_v0.11.2.zip
7z x fastqc_v0.11.2.zip
cd FastQC
chmod 755 fastqc
sudo ln -s /path/to/FastQC/fastqc /usr/local/bin/fastqc
#+END_SRC

Load =FastQC= directly or in the shell. (every line is an option)
#+BEGIN_SRC shell
fastqc & # open a GUI
fastqc <file>.txt
zcat file1.fastq.gz | fastqc file1.fastq.gz # stream the content of gz files
#+END_SRC

Have a fastq.gz of the sequences. Run fastqc. Results are outputed in html format.
#+BEGIN_SRC shell
./fastqc <file.fasta>
Firefox report.html
#+END_SRC

Run quality controls on A1 A2 A3 samples
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=1:ppn=16:native
#PBS -l walltime=02:10:00
#PBS -N fastqc.out
#PBS -e fastqc.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V
oasis=/oasis/projects/nsf/sun108/
for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
do
    for j in 1 2
    do
zcat ${oasis}/silo/data/ganglia/br.${i}.R${j}.fastq.gz | \
fastqc/fastqc ${oasis}/silo/data/ganglia/br.${i}.R${j}.fastq.gz \
--outdir=${oasis}/silo/ganglia/fastqc/
done
done
#+END_SRC

*** Sampling
It is costumed fist to work on a small subset of the original data. When testing code its not smart to load all big data just for testing and optimizing the procedure. 

Some option to sampling from fastq.gz [[https://www.biostars.org/p/6544/][biostars link]]

Clone =seqtk= and compile it.
#+BEGIN_SRC shell
git clone https://github.com/neocruiser/seqtk.git
cd seqtk
make
sudo ln -s /path/to/seqtk/seqtk /usr/local/bin/seqtk
#+END_SRC

Sample from *fastq.gz file
#+BEGIN_SRC shell
seqtk sample -s123 *1.fastq.gz 250 > sample1.fq
seqtk sample -s123 *2.fastq.gz 250 > sample2.fq
#+END_SRC

Sampling from each of the A1, A2, and A3 Hardclam nodules. Additional cool jobs that can be done using =seqtk= [[http://ged.msu.edu/angus/tutorials-2013/seqtk_tools.html][here]].
#+BEGIN_SRC shell
seqtk sample -s123 1_Index_1.A1_R1.fastq.gz 1000 > sampleA1R1.fq && \
seqtk sample -s123 1_Index_2.A2_R1.fastq.gz 1000 > sampleA2R1.fq && \
seqtk sample -s123 2_Index_8.A3_R1.fastq.gz 1000 > sampleA3R1.fq && \
seqtk sample -s123 HI.0615.001.Index_1.A1_R2.fastq.gz 1000 > sampleA1R2.fq && \
seqtk sample -s123 HI.0615.001.Index_2.A2_R2.fastq.gz 1000 > sampleA2R2.fq && \
seqtk sample -s123 HI.0615.002.Index_8.A3_R2.fastq.gz 1000 > sampleA3R2.fq
#+END_SRC

#+CAPTION: Per base sequence quality
| Sample | Mean QC | Trim@ |
|--------+---------+-------|
| A1R1   |      34 |    97 |
| A2R1   |      34 |    97 |
| A3R1   |      34 |    97 |
| A1R2   |      34 |    97 |
| A2R2   |      34 |    97 |
| A3R2   |      34 |    97 |

*** Align to K-mer content from fastQC report
TATGCCG
CGATCTC
CGTATGC
CACGATC
GCCGTCT
CCGTCTT
TGCCGTC
ATGCCGT
GCCCGGG
GGCCCGG
CTCGTAT
ATCTCGT
GATCTCG
TCTCGTA
ACGATCT
ACTAGCC
TGGCCCG
CCGGGGG
TCGTATG

TATGCCG
CGTATGC
TGCCGTC
CCGTCTT
CTCGTAT
ATCTCGT
GCCGTCT
CGATGTA
ATGCCGT
CTTCTGC
ACCGATG
TGCTTGA
TCTCGTA
TCTCGTA
TATCTCG
CCCGGTT
ACTAGCC
TCGTATG
TCTGCTT

TATGCCG
CGTATGC
TGCCGTC
CCGTCTT
CTCGTAT
GCCGTCT
ATCTCGT
ATGCCGT
GAATCTC
ACTAGCC
CTTCTGC
TGCTTGA
TCTCGTA
AATCTCG
CGCGGCT
CCCGGTT
TCGTATG
GGCCCGG
CGGGGTT
CCCCAAT

CGCCGTA
GTCGCCG
CCGTATC
GGTCGCC
GGCCCGG
TCGGTGG
TGGCCCG
TCGCCGT
GCCCGGG
GATCTCG
ACTAGCC
TCTCGGT
TGGTCGC
CCGGGGG
GCCGTAT
TCGGGGG
GGGCGCC
CTCGGTG
GTGGTCG
ATCTCGG

CGCCGTA
GTCGCCG
CCGTATC
TCGGTGG
TGGTCGC
TCTCGGT
GATCTCG
GGGCGCC
TCGGGGG
GGTCGCC
TCGCCGT
ACTAGCC
CCCCAAT
GGTGGTC
GGGGCGC
GCCGTAT
CCCCGAT
CCGGGAT
CGCAAAC
CCCGGTT

CGCCGTA
CCGTATC
GTCGCCG
TGGTCGC
TCGGTGG
TCTCGGT
GGTCGCC
GATCTCG
TCGCCGT
ACTAGCC
CCCGGTT
GGCCCGG
GGTGGTC
GGGCGCC
CCCCAAT
CCCCGAT
GCCCGGG
TGGCCCG
GCCGTAT

*** Quality Control on trimmed remains
Batch quality control testing on 0, 4, and 5 trimmed data. Shell scripts are saved in file =qcheck.sh=.

All trimmed data is deleted after this step except the first dataset of option 0. cf table above. 
Quality control reports are saved in =fastQC= directory. All the unmapped reads, meaning those reads that were trimmed out are saved in =unmappedReads= directory. The original eaw files of the samples are saved in =original= directory and QPX transcriptome in =MMETSP0098= directory. The reference transcriptome was referenced and sequences were counted and both batch of reports are saved in =Align=. Good quality remaining trimmed reads are saved in =trimmed=. The mapping of samples to QPX transcriptome (below) will be saved in =mapping= directory.

#+CAPTION: Different tests
| Adapters | Reference              |
|----------+------------------------|
| TrueSeq2 | Genome (555 c)         |
| TrueSeq3 | Transcriptome (1152 c) |
|          |                        |

See also the N50 score calculation.

** Phase 2: Assembling contigs and mapping
*** Reference QPX: the assembly
Two options: either the reference I assembled following the steps above or the one already done (but I know nothing about the parameters used to assemble it). 
**** Count and index the reference genomes/transcriptomes
Generate counts of the reference transcriptome using a perl script for HTSeq. this will generate a file with gff3 format
#+BEGIN_SRC shell
~/data/gmod_fasta2gff3.pl --fasta_dir QPX_Genome_v017.fasta --gfffilename QPX_Genome_v017.gff3 --type CDS --nosequence
#+END_SRC

Index with =bwa=.
#+BEGIN_SRC shell
bwa index <file.fa>
#+END_SRC
**** QPX already assembled transcriptome and genome of Steve Roberts
Count the number of sequences in the fasta file
#+BEGIN_SRC shell
cd ~/data/QPX
grep '>' QPX_Genome_v017.fasta | wc -l
#+END_SRC

Index the genome with =bwa= for mapping and to be used as a reference.
#+BEGIN_SRC shell
bwa index QPX_Genome_v017.fasta
bwa index QPX_transcriptome_v2orf.fasta
#+END_SRC

Or index the reference with samtools
#+BEGIN_SRC shell
samtools faidx QPX_Genome_v017.fasta
#+END_SRC
**** QPX Trinity assembled transcriptome
Get trinity (2.0.6) and bowtie 2 (2.2.6) and samtools. Build them all using =make=. And export PATH to bowtie and samtools directories. For more info on job scripts for big data visit [[http://www.psc.edu/index.php/trinity/753-trinity-example-files][here]].
#+BEGIN_SRC shell
wget http://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.6/bowtie2-2.2.6-source.zip
wget https://github.com/trinityrnaseq/trinityrnaseq/archive/v2.0.6.tar.gz
#+END_SRC

Change names of fastq files. Rename files following the code below. The ganglia project. All samples were sequenced with =Illumina Truseq Stranded RNA kits=. Which means R1 contains Reverse (sense) reads and R2 contains (antisense) reads.
#+BEGIN_SRC shell
br.<1-24>.R<1-2>.fastq.gz
gg.<1-24>.R<1-2>.fastq.gz
#+END_SRC

Merge =fastq.gz= files into =right.fastq.gz= and =left.fastq.gz= for R1 and R2 respectively. A combination assembly with trinity is superior to separate assembly. Create a reference transcriptome for contig alignment.
#+BEGIN_SRC shell
time find . -name "*R1*fastq.gz" | xargs zcat | gzip -c > test.fq.gz
#+END_SRC

We assemble a transcriptome to later annotate it.
Using =trinity= I assemble the transcriptome with sequencing reads in =MMETSP0098=. Trimmomatic is integrated in trinity, so I use same default parameters used for the reference genome and transcriptome of Steve Roberts.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=5:ppn=16:native
#PBS -l walltime=05:00:00
#PBS -N trinity6.out
#PBS -e trinity.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V
oasis=/oasis/projects/nsf/sun108/

trinity/Trinity --seqType fq \
--SS_lib_type RF \
--left ${oasis}/silo/data/ganglia/raw.all.R1.fq.gz \
--right ${oasis}/silo/data/ganglia/raw.all.R2.fq.gz \
--quality_trimming_params "ILLUMINACLIP:adapters.fa:2:30:10 LEADING:5 TRAILING:5 SLIDINGWINDOW:4:15 MINLEN:36" \
--normalize_max_read_cov 50 \
--min_contig_length 200 \
--output ./trinity/ganglia6/trinity/ \
--CPU 16 \
--bflyCPU 16 \
--bflyGCThreads 16 \
> trinity_output.log
#+END_SRC

When running trinity on large datasets it is best to cut jobs into phases.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=1:ppn=16:native
#PBS -l walltime=02:00:00
#PBS -N trinity1.out
#PBS -e trinity.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V
# set stack to unlimited
# because of large datasets
ulimit -s unlimited
# echo stdout to output file
set -x
# xsede directories
oasis=/oasis/projects/nsf/sun108
scratch=/oasis/scratch/silo/temp_project
home=/home/silo
# output directories for trinity
jobid=test1
workdir=${scratch}/ganglia/trinity/trinity_out_dir_${jobid}/
mkdir -p ${workdir}
cd ${workdir}
# fastq raw files, reads
sense=${scratch}/ganglia/data/gg.11.R1.fastq.gz
antisense=${scratch}/ganglia/data/gg.11.R2.fastq.gz
# maximum memory for jellyfish to use
# ppn*8
JM=10G

## TRINITY
## Phase 1: construct Kmers
${home}/trinity/Trinity --seqType fq \
--SS_lib_type RF \
--left ${sense} \
--right ${antisense} \
--quality_trimming_params "ILLUMINACLIP:adapters.fa:2:30:10 LEADING:5 TRAILING:5 SLIDINGWINDOW:4:15 MINLEN:36" \
--normalize_max_read_cov 50 \
--min_contig_length 200 \
--output ${workdir} \
--max_memory ${JM} \
--CPU 16 \
--bflyCPU 16 \
--bflyGCThreads 16 \
--no_run_chrysalis \
>& ${home}/trinity.${jobid}_output.log
#+END_SRC

As of today =April-2015= Trinity uses java version 1.7. So must downgrade system to that version. I can comment out in =trinity.pl= java version check but under java v.1.8 trinity can introduce some errors.

With the code above I generated =39946= contigs.
#+BEGIN_SRC shell
grep ">TR" Triniti.fasta | wc -l
39946
#+END_SRC

Calculate the N50 (1) and L50 (2) in bp.
#+BEGIN_SRC shell
cat mmetsp0098Cust.fasta | grep ">" | awk '{print $2}' | sed 's/len=//g' | sort -rn | awk '{sum += $0; print "N50:" $0"\t", sum}' | tac | awk 'NR==1 {halftot=$2/2} lastsize>halftot && $2<halftot {print} {lastsize=$2}'
#+END_SRC

Calculate the total size of contigs in bp.
#+BEGIN_SRC shell
cat mmetsp0098Cust.fasta | grep ">" | awk '{print $2}' | sed 's/len=//g' | head | awk '{sum+=$1}END{print "Total:", sum}out'
#+END_SRC
**** Genome-guided trinity transcriptome assembly
QPX can be considered as gene-dense genome. =--jacard-clip= can be used. In this case [[http://bowtie-bio.sourceforge.net/index.shtml][Bowtie]] have to be installed.1

#+BEGIN_SRC shell
wget http://sourceforge.net/projects/bowtie-bio/files/bowtie/1.1.2/bowtie-1.1.2-linux-x86_64.zip
sudo ln -n /path/to/bowtie /usr/local/bin/bowtie
bowtie --help | less
#+END_SRC

Assemble reads that are filtered after mapping to reference genome. Those reads were trimmed, mapped, sorted, and duplicates removed from script in =trimmingNodules.sh=. =genome_guided_bam= (below) cannot take multiple bam files. If one has many replicates bam files can be merged together with =Picard MergeSamFiles= function. This step can be done after mapping with BWA to a reference or after Picard MarkDuplicates for discarding duplicate reads. =note= set the output to a destination that does not require root privileges. Merging 3 bam files takes 90 minutes. Assembling a 6Gb bam files can take up to 4h.
#+BEGIN_SRC shell
## Merge bam files for Trinity genome-guided assembly
#! /usr/bin/bash

dir=/media/sf_data/nodule/rmdup/
ddir=/home/neo/data/nodule/trinity

x=A1
y=A2
z=A3
b=A

    java -Xmx10g -jar /home/neo/data/picard/picard.jar \
        MergeSamFiles \
        I=${dir}${x}.nodup.bam \
        I=${dir}${y}.nodup.bam \
        I=${dir}${z}.nodup.bam \
        O=${dir}/${b}.bam \
        SO=coordinate \
        AS=true

/home/neo/data/QPX/trinityrnaseq/Trinity \
--genome_guided_bam ${dir}${b}.bam \
--genome_guided_max_intron 1000 \
--max_memory 10G \
--output ${ddir} \
--CPU 5
#+END_SRC

Check if bam file is sorted
#+BEGIN_SRC shell
samtools view -H file.bam | less
#+END_SRC

*** Trimming
Trimmomatic can be installed separately or used inside Trinity as a plugin.
Download trimmomatic

#+BEGIN_SRC shell
curl -O -L http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.33.zip
#+END_SRC

The trimming is based based on FastQC quality control reports. The reports can be found under the dir "~/data/QPX/Align/"

Trimming is done on paired ends on sequences sampled from the A1 A2 A3 data. Sequencer is Illumina HiSEQ. Very important to choose the adapter sequences. The adapters that have been used here are saved under "~/data/Trimmomatic-0.33/adapters/TrueSeq3-PE-3.fa".

For an in depth review of the parameters of trimmomatic visit [[http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf][here]]. The script below is saved in an executable file named =trim.sh=.

#+BEGIN_SRC shell
java -jar ~/data/Trimmomatic-0.33/trimmomatic-0.33.jar PE \
~/data/QPX/nodules/sampling/sampleA1R1.fq \
~/data/QPX/nodules/sampling/sampleA1R2.fq \
sA1R1.P.fq sA1R1.U.fq sA1R2.P.fq sA1R2.U.fq \
ILLUMINACLIP:~/data/Trimmomatic-0.33/adapters/TrueSeq3-PE-3.fa:2:30:10 \
TRAILING:3 \
SLIDINGWINDOW:4:15 \
CROP:90 \
MINLEN:36
#+END_SRC

Count the number of reads.
#+BEGIN_SRC shell
zcat <filename>.fastq.gz | grep '@HWI' | wc -l
#+END_SRC
Move/rename files in batch.
#+BEGIN_SRC shell
find . -name "pattern" -exec mv {} /to/path \;
#+END_SRC
Delete files in batch.
#+BEGIN_SRC shell
ls "pattern" | xargs rm
#+END_SRC

#+CAPTION: Number of reads that survived the trimming filters

| N | Sample |    Total |   Remain |    Clip | Trail | Size | Slide | Crop | Adapters    |
|---+--------+----------+----------+---------+-------+------+-------+------+-------------|
| 0 | A1     | 30491569 | 30176618 | 2:30:10 | t3    | s36  | no    | no   | TrueSeq3.PE |
|   | A2     | 34515597 | 34136650 | 2:30:10 | t3    | s36  | no    | no   | TrueSeq3.PE |
|   | A3     | 46861893 | 46430064 | 2:30:10 | t3    | s36  | no    | no   | TrueSeq3.PE |
| 1 | A1     |          | 29790935 |         |       |      | 4:15  | no   | TrueSeq3.PE |
|   | A2     |          | 33698155 |         |       |      | 4:15  | no   | TrueSeq3.PE |
|   | A3     |          | 45802956 |         |       |      | 4:15  | no   | TrueSeq3.PE |
| 2 | A1     |          | 30370029 |         | no    |      | no    | 10   | TrueSeq3.PE |
|   | A2     |          | 34362754 |         | no    |      | no    | 10   | TrueSeq3.PE |
|   | A3     |          | 46682292 |         | no    |      | no    | 10   | TrueSeq3.PE |
| 3 | A1     |          | 30300807 |         |       |      | 4:15  | 10   | TrueSeq3.PE |
|   | A2     |          | 34282437 |         |       |      | 4:15  | 10   | TrueSeq3.PE |
|   | A3     |          | 46600781 |         |       |      | 4:15  | 10   | TrueSeq3.PE |
| 4 | A1     |          | 30175710 | 2:30:10 | t3    | s36  | no    | no   | TrueSeq2.SE |
|   | A2     |          | 34135918 | 2:30:10 | t3    | s36  | no    | no   | TrueSeq2.SE |
|   | A3     |          | 46428610 | 2:30:10 | t3    | s36  | no    | no   | TrueSeq2.SE |
| 5 | A1     |          | 29790528 |         |       |      | 4:15  | no   | TrueSeq2.SE |
|   | A2     |          | 33697784 |         |       |      | 4:15  | no   | TrueSeq2.SE |
|   | A3     |          | 45802292 |         |       |      | 4:15  | no   | TrueSeq2.SE |
| 6 | A6     |          | 30223128 |      no | t3    | s36  | no    | no   | no          |
|   |        |          |          |         |       |      |       |      |             |
*** Mapping to reference Sort, then count mapped reads
We map reads to a reference for later calling SNPs.
Download and install =bwa= if not done yet.
#+BEGIN_SRC shell
git clone https://github.com/lh3/bwa.git
cd bwa && make
sudo ln -s /path/to/bwa /usr/local/bin/bwa
#+END_SRC

Run bwa over reference genome of QPX for every paired samples (A1 A2 A3). Scripts are saved in =mapping.sh=.

dont forget to index the reference with =bwa index= before mapping.

Additional tools needed are HTSeq for sequence count (for reference) and samtools for conversion of sam bam files, indexing, removing duplications, and sorting reads (for samples).

This [[https://www.biostars.org/p/43677/][biostars tutorial ]] is a short introduction to pipelining. [[http://statisticalrecipes.blogspot.com/2013/06/getting-started-with-samtools-and.html][This intro]] is testing basic samtools commends. [[http://zlib.net/pigz/][This tool]] is a modified version of gzip for parallel zipping of big sam files. [[https://github.com/neocruiser/bwa][BWA website ]]on github for introduction and description of some functions.


The following script will generate bam files with bwa.
#+BEGIN_SRC shell
#! /user/bin/bash

sample[1]=A1
sample[2]=A2
sample[3]=A3

ir=./trimmed/
dir=mapping5
ddir=rmdup5

extension=.trimmed.P.fastq.gz
reference=./genomeSRv015/QPX_v015.fasta
count=./genomeSRv015/QPX_v015.gff3

for i in 1 2 3
do
    sample=${sample[${i}]}
    bwa mem ${reference} \
        ${ir}${sample}R1${extension} \
        ${ir}${sample}R2${extension} | \
        samtools view -Shu - | \
        samtools sort - ./${dir}/${sample}.sorted

    htseq-count --format=bam \
        --stranded=no \
        --type=CDS --order=pos \
        --idattr=Name ./${dir}/${sample}.sorted.bam ${count} \
        > ./${ddir}/${sample}.htseq.counts.txt

done
#+END_SRC

When aligning to reference BWA will use its default value to consider 4 or fewer mismatch to a given read as a good score. Here I applied the default values of =4%=.

Display reads with =tview=. Press =?= for additional help inside tview.
#+BEGIN_SRC shell
samtools tview -d -H <file>.bam QPX_Genome_v017.fasta
#+END_SRC

Another lightweight tool for displaying alignments is =Tablet Viewer=. [[http://ics.hutton.ac.uk/tablet/][Link]] to download and manual.

Calculate the number of reads per sample. =htseq= is blazing fast and accurate.
#+BEGIN_SRC shell
time cat sample.htseq.counts.txt | awk '{s+=$2; print s}' | tail -n 1
## OR
time samtools view -c sample.bam
#+END_SRC

Get the number of mapped reads.
#+BEGIN_SRC shell
## mapped
samtools view -c -F 4 sample.bam
## unmapped
samtools view -c -f 4 sample.bam
#+END_SRC

Get the number of reads from paired ends where both the forward and reverse mate are mapped.
#+BEGIN_SRC shell
samtools -c -f 1 -F 12 sample.bam
#+END_SRC

Get a summary on reads.
#+BEGIN_SRC shell
samtools flagstat sample.bam
#+END_SRC

*** Remove duplicates (redup)
There is 2 options either with samtools function/module =rmdup= or with =Picard=. Picard is recommended for better alignment of PE reads. [[https://broadinstitute.github.io/picard/command-line-overview.html][Download]] and description of functions can be found on Broad Institute website. Some troubleshooting and sorting issues due to compatibility problems between samtools and picard, check this [[http://seqanswers.com/forums/showthread.php?s%3Dbbb083294ce9bad821e6973185d1f3bc&t%3D5494][thread]]. 

Remove optical duplicate reads with Picard =MarkDuplicates= function.
#+BEGIN_SRC shell
java -Xmx2g -jar ~/data/picard/picard.jar \
MarkDuplicates \
INPUT=../mapping/A1.sorted.bam \
OUTPUT=./A1.nodup.bam \
METRICS_FILE=./A1.dup.metrics \
REMOVE_DUPLICATES=true \
ASSUME_SORTED=true
#+END_SRC

*** Summary in one code snippet
The code below generate a bam file of mapped reads to a reference transcriptome without duplicated PCR reads. It generates also a counting of contigs before duplication elimination and after of the mapped reads.
#+BEGIN_SRC shell
#! /user/bin/bash

:'
this script accomplish 5 things:
1. map all paired end samples to reference woth bwa
2. sort the mapped contigs with samtools
3. remove duplicate contigs with picard
4. index contigs with samtools
5. count contigs with htseq
'

sample[1]=A1
sample[2]=A2
sample[3]=A3

dir=mapping3
ddir=rmdup3

extension=./trimmed/.trimmed.P.fastq.gz
reference=./mmetsp0098/contigs.fa

count=./mmetsp0098/MMETSP0098.gff3
htseq=./${dir}/${sample}.htseq.counts
sorted=./${dir}/${sample}.sorted

nodup=./${ddir}/${sample}.nodup
metrics=./${ddir}/${sample}.dup.metrics



for i in 1 2 3
do
    sample=${sample[${i}]}
    bwa mem ${reference} \
        ~/data/QPX/trimmed/${sample}R1${extension} \
        ~/data/QPX/trimmed/${sample}R2${extension} | \
        samtools view -Shu - | \
        samtools sort - ${sorted}

    htseq-count --format=bam \
        --stranded=no \
        --type=CDS --order=pos \
        --idattr=Name ${sorted}.bam ${count} \
        > ${htseq}.txt

    java -jar ~/data/picard/picard.jar \
    MarkDuplicates \
        INPUT=${sorted}.bam \
        OUTPUT=${nodup}.bam \
        METRICS_FILE= ${metrics} \
        REMOVE_DUPLICATES=true \
        ASSUME_SORTED=true        

    samtools index ${sorted}.bam
    
    rm -rf ${sorted}.bam

    htseq-count --format=bam \
        --stranded=no \
        --type=CDS --order=pos \
        --idattr=Name ${nodup}.bam ${count} \
        > ${htseq}.nodup.txt


done
#+END_SRC

=Note= Sometimes Picard MarkDuplicates function throws an error. This error might be due to sample fastq.gz files where R1 and R2 reads are not in the correct order, which will cause an incorrect memory handling and stop the analysis. This error was introduced when mapping all strain R1s and R2s to both MMETSP0098 and Steve Roberts genome v015 (approx 21,000). No duplicates where removed. Proceeded to the next step for SNP calling.
#+BEGIN_SRC shell
[Wed Apr 15 11:51:44 EDT 2015] picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 0.47 minutes.
Runtime.totalMemory()=2556952576
To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
Exception in thread "main" htsjdk.samtools.SAMException: /tmp/neo/CSPI.7539378699724755388.tmp/3744.tmpnot found
	at htsjdk.samtools.util.FileAppendStreamLRUCache$Functor.makeValue(FileAppendStreamLRUCache.java:63)
	<...>
#+END_SRC

The above error is due to RAM memory limitations attributed to java when =-Xmx= is specified. On powerful servers and with big libraries one should assign higher =-Xmx=.

** Phase 3: Genetic variant calling
*** SNP calling (1)
=aim= Sequence variation between strains. also nucleotide substitution rate
Tool to be used is samtools. GATK can also be used. Or varscan.
1. Generate VCF files from bam =mapped to reference=
2. Map indels with GATK
3. Calculate the depth of coverage with GATK
4. Annotate variants/indels (annovar for which species??) see [[http://annovar.openbioinformatics.org/en/latest/user-guide/startup/][here]]
5. Filter SNPs (flag dbSNP, might not be causal for difference)
6. Extract nonsynonymous SNPs (loss of function (LoF) amorphic - gain of function (GoF) neomorphic - dominant negative antimorphic - indels (frameshift, stop loss, missense) - composite insertions - substitution events (transition, transversions) - synonymous mutation)
7. SNPs in Low coverage areas might be wrong (reanalyze w/ depth of coverage)
8. Annotate variants (find a suitable library)
9. Rank variants with data from GO genes from other species

**** step 1
Trim raw reads then map them to reference. The whole script is available in =mapNoCount.sh=. It contains all =6 libraries= mapped to =3 references=.
#+BEGIN_SRC shell
#! /user/bin/bash

:'
this script accomplishes 4 things:
1. map all paired end samples to reference woth bwa
2. sort the mapped contigs with samtools
3. remove duplicate contigs with picard
4. index contigs with samtools
'

java -jar /home/neo/data/Trimmomatic-0.33/trimmomatic-0.33.jar PE \
/media/sf_docs/data/QPX-RNA-Seq/mmetsp0098.1.NY.fastq.gz \
/media/sf_docs/data/QPX-RNA-Seq/mmetsp0098.2.NY.fastq.gz \
/media/sf_docs/data/QPX-RNA-Seq/trimmed/mmetsp0098.1.trimmed.P.NY.fastq.gz \
/media/sf_docs/data/QPX-RNA-Seq/trimmed/mmetsp0098.1.trimmed.U.NY.fastq.gz \
/media/sf_docs/data/QPX-RNA-Seq/trimmed/mmetsp0098.2.trimmed.P.NY.fastq.gz \
/media/sf_docs/data/QPX-RNA-Seq/trimmed/mmetsp0098.2.trimmed.U.NY.fastq.gz \
ILLUMINACLIP:TrueSeq3-PE-3.fa:2:30:10 \
SLIDINGWINDOW:4:15 \
TRAILING:5 \
MINLEN:45

sample[1]=mmetsp0098
sample[2]=mmetsp001433
sample[3]=mmetsp00992
sample[4]=mmetsp001002
sample[5]=mmetsp0099
sample[6]=mmetsp00100

ir=/media/sf_docs/data/QPX-RNA-Seq/trimmed
dir=/media/sf_docs/data/mappingX3
ddir=/media/sf_docs/data/rmdupX3
extension=.trimmed.P.NY.fastq.gz

reference=/media/sf_docs/data/genomeSRv015/QPX_v015.fasta
count=/media/sf_docs/data/genomeSRv015/QPX_v015.gff3

for i in 1 2 3 4 5 6
do
    sample=${sample[${i}]}
    bwa mem ${reference} \
        ${ir}/${sample}.1${extension} \
        ${ir}/${sample}.2${extension} | \
        samtools view -Shu - | \
        samtools sort - ${dir}/${sample}.sorted

    java -jar /home/neo/data/picard/picard.jar \
        MarkDuplicates \
        INPUT=${dir}/${sample}.sorted.bam \
        OUTPUT=${ddir}/${sample}.nodup.bam \
        METRICS_FILE=${ddir}/${sample}.dup.metrics \
        REMOVE_DUPLICATES=true \
        ASSUME_SORTED=true

    samtools index ${ddir}/${sample}.nodup.bam

done

#+END_SRC

Or index the reference with samtools
#+BEGIN_SRC shell
samtools faidx QPX_Genome_v017.fasta
#+END_SRC
**** step 2
Create a probability per variant =vcf= file with samtools. Description of command line [[http://samtools.sourceforge.net/mpileup.shtml][here]].
#+BEGIN_SRC shell
#! /usr/bin/bash

:'
samtools -u for ouputing an uncompressed bcf file
-B : no baq computing for faster jobs
-d : depth of covreage, increase it to get precise depth of coverage
-f : decalre reference
-D : control the number of variant to keep per sample based on the depth of coverage
-C : reduce effect of reads with high mismatches
--min-ac : minimum of the percentage of most frequent variants
-g : yes or no for homoz/heteroz/missing nucleotides
'

reference=/media/sf_docs/data/QPX-RNA-Seq/Steve_Roberts/QPXTranscriptome_v21/QPX_transcriptome_v2orf.fasta

dir=/media/sf_docs/data/mappingX
ddir=/media/sf_docs/data/callingX

sample[1]=mmetsp0098
sample[2]=mmetsp001433
sample[3]=mmetsp00992
sample[4]=mmetsp001002
sample[5]=mmetsp0099
sample[6]=mmetsp00100

for i in 1 2 3 4 5 6
do
    sample=${sample[${i}]}
    samtools mpileup -u -C50 -BQ0 -d1000 -f ${reference} \
        ${dir}/${sample}.sorted.bam | \
    bcftools view --min-ac 0 -g "^miss" | \
    /home/neo/data/bcftools-1.2/vcfutils.pl varFilter -D100 \
        > ${ddir}/${sample}.var.vcf

done
#+END_SRC
**** step 3
Call SNPs with bcftools. See script one step above. When finished with calling SNPs with samtools, enumerate the number of SNPs called for each reference.
#+BEGIN_SRC shell
#for example
grep "MMETSP0098" fileName.var.vcf | wc -l
#+END_SRC

Six samples where analyzed.
#+CAPTION: Samples and references used for SNP calling
| Sample       | Reference            |
|--------------+----------------------|
| mmetsp0098   | SR transcriptome v21 |
| mmetsp00992  | mmetsp0098           |
| mmetsp001002 | SR genome v015       |
| mmetsp001433 |                      |
| mmetsp0099   |                      |
| mmetsp00100  |                      |

**** step 4
Convert vcf file to fasta. either use =seqtk= or =vcftools=. Many tests are available. BLAST can be done on the fasta file.
#+BEGIN_SRC shell
./bcftools/vcfutils.pl vcf2fq fileName.vcf > fileName.fq
seqtk seq -a fileName.fq > fileName.fasta
#+END_SRC 
*** SNP calling (2)
SAM format specifications, in this [[https://samtools.github.io/hts-specs/SAMv1.pdf][PDF,]] describe the @RG =read group= format. This @RG is essential to run GATK, which is an other way to call SNPs.
#+BEGIN_SRC shell
@RG\tID:mmetsp0098\tSM:NY1\tPL:illumina\tLB:mmetsp0098\tPU:unit1
#+END_SRC

The script for mapping all QPX reads of all libraries. This script can be run in parallel for fast computing and mapping to several available references. This script is compiles in =mappingV2.sh=.
#+BEGIN_SRC shell
#! /user/bin/bash

:'
this script accomplish 5 things:
1. map all paired end samples to reference woth bwa
2. sort the mapped contigs with samtools
3. remove duplicate contigs with picard
4. index contigs with samtools
5. count contigs with htseq
-M: bwa mark shorter hits as secondary, increase picard comaptibility
'

sample[1]=mmetsp0098
sample[2]=mmetsp001433
sample[3]=mmetsp00992
sample[4]=mmetsp001002
sample[5]=mmetsp0099
sample[6]=mmetsp00100

ir=/media/sf_docs/data/QPX-RNA-Seq/trimmed
dir=/media/sf_docs/data/mappingY
ddir=/media/sf_docs/data/rmdupY

extension=.trimmed.P.NY.fastq.gz
reference=/media/sf_docs/data/QPX-RNA-Seq/Steve_Roberts/QPXTranscriptome_v21/QPX_transcriptome_v2orf.fasta

RG[1]='@RG\tID:mmetsp0098\tSM:NY1\tPL:illumina\tLB:mmetsp0098\tPU:QPXtrxSRv21'
RG[2]='@RG\tID:mmetsp001433\tSM:NY1\tPL:illumina\tLB:mmetsp001433\tPU:QPXtrxSRv21'
RG[3]='@RG\tID:mmetsp00992\tSM:MA1\tPL:illumina\tLB:mmetsp00992\tPU:QPXtrxSRv21'
RG[4]='@RG\tID:mmetsp001002\tSM:VA1\tPL:illumina\tLB:mmetsp001002\tPU:QPXtrxSRv21'
RG[5]='@RG\tID:mmetsp0099\tSM:MA2\tPL:illumina\tLB:mmetsp0099\tPU:QPXtrxSRv21'
RG[6]='@RG\tID:mmetsp00100\tSM:VA2\tPL:illumina\tLB:mmetsp00100\tPU:QPXtrxSRv21'

    java -jar /home/neo/data/picard/picard.jar \
        CreateSequenceDictionary \
        R=${reference} \
        O=/media/sf_docs/data/QPX-RNA-Seq/Steve_Roberts/QPXTranscriptome_v21/QPX_transcriptome_v2orf.dict

    samtools faidx ${reference}

for i in 1 2 3 4 5 6
do
    sample=${sample[${i}]}
    RG=${RG[${i}]}
    bwa mem -M \
        -R ${RG} \
        -p ${reference} \
        ${ir}/${sample}.1${extension} \
        ${ir}/${sample}.2${extension} \
    > ${dir}/${sample}.sam

    java -jar /home/neo/data/picard/picard.jar \
        SortSam \
        INPUT=${dir}/${sample}.sam \
        OUTPUT=${ddir}/${sample}.sorted.bam \
        SORT_ORDER=coordinate

    java -jar /home/neo/data/picard/picard.jar \
        MarkDuplicates \
        INPUT=${ddir}/${sample}.sorted.bam \
        OUTPUT=${ddir}/${sample}.nodup.bam \
        METRICS_FILE=${ddir}/${sample}.dup.metrics \
        REMOVE_DUPLICATES=true \
        ASSUME_SORTED=true


done
#+END_SRC

1. Create a custom read group for each library. Samtools/Picard can do it too.
2. Create a dictionary index with Picard of the reference
3. Create an index of each read with samttools
4. For loop over all libraries to align reads to each reference
5. Sort the generated sam output with Picard
6. Mark duplicate reads and remove them with Picard
7. Realign reads around indels with GATK
8. Recalibrate SNP calls
9. Call SNPs on recalibrated bam files

This script is compiled in =mappingV5.sh=. It can be combined with the one above.
#+BEGIN_SRC shell
#! /user/bin/bash

:'
Note: For more info refer to GATK best practices on official site

This script accomplishes 3 things;
1. sort sam files into bam
2. removes duplicate reads
3. calls SNPs

A. This script is the third version of mapping reads into references.
B. It is best to run this script in parallel for each reference.
C. All samples contain raw reads.
D. Raw reads were first trimmed with trimmomatic
E. @RG: read groups were custom build in mappingV2.sh
F. Also reads were mapped with BWA in mappingV2.sh
G. Here we use an alternative step to call SNPs with GATK

a. create a dictionary file with Picard is essential
b. indexing the reference is essential
c. sam/bam convertion is done with Picard
d. sorting was done following read coordinate to reference
e. duplicates (optical) were removed with Picard. usually 30-40% are duplicated reads
f. reads were counted before/after dup removal
g. reads were realigned around indels with GATK (important 2 step process)
h. reads were recalibrated with known SNPs (important 5 step process)
h.1 we have no preliminary SNP data, so discover SNPs with very high phred scores
h.2 use the selected SNPs to calculate a quality score
h.3 use the GATK recalibrator to call again the last batch of SNPs with even higher phred scores

Note(2): h.1 and h.2 can be bootstraped
Note(3): there is a generated R report before after recalibration of quality scores

'


sample[1]=mmetsp0098
sample[2]=mmetsp001433
sample[3]=mmetsp00992
sample[4]=mmetsp001002
sample[5]=mmetsp0099
sample[6]=mmetsp00100

ir=/media/sf_docs/data/QPX-RNA-Seq/trimmed
dir=/media/sf_docs/data/mappingY3
ddir=/media/sf_docs/data/rmdupY3

counts=${ddir}/counts
realign=${ddir}/realign
call=${ddir}/call

extension=.trimmed.P.NY.fastq.gz
reference=/media/sf_docs/data/genomeSRv015/QPX_v015.fasta


java -jar /home/neo/data/picard/picard.jar \
CreateSequenceDictionary \
R=${reference} \
O=/media/sf_docs/data/genomeSRv015/QPX_v015.dict

samtools faidx ${reference}
mkdir -p ${counts} ${realign} ${call}



for i in 1 2 3 4 5 6
do
    sample=${sample[${i}]}

    java -jar /home/neo/data/picard/picard.jar \
        SortSam \
        INPUT=${dir}/${sample}.sam \
        OUTPUT=${ddir}/${sample}.sorted.bam \
        SORT_ORDER=coordinate

    java -jar /home/neo/data/picard/picard.jar \
        MarkDuplicates \
        INPUT=${ddir}/${sample}.sorted.bam \
        OUTPUT=${ddir}/${sample}.nodup.bam \
        METRICS_FILE=${ddir}/${sample}.dup.metrics \
        REMOVE_DUPLICATES=true \
        ASSUME_SORTED=true

    java -jar /home/neo/data/picard/picard.jar \
        BuildBamIndex \
        INPUT=${ddir}/${sample}.nodup.bam


# count and redirect output to a file
# grep the file with $grep "counted"
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T CountReads \
        -R ${reference} \
        -fixMisencodedQuals \
        -I ${ddir}/${sample}.nodup.bam \
        2> ${counts}/${sample}.nodup.count.txt \
        && grep "counted" ${counts}/${sample}.nodup.count.txt

    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T CountReads \
        -R ${reference} \
        -fixMisencodedQuals \
        -I ${ddir}/${sample}.nodup.bam \
        -rf DuplicateRead \
        2> ${counts}/${sample}.dup.count.txt \
        grep "counted" ${counts}/${sample}.dup.count.txt


    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T RealignerTargetCreator \
        -R ${reference} \
        -fixMisencodedQuals \
        -I ${ddir}/${sample}.nodup.bam \
        -o ${realign}/${sample}.target.intervals.list

    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T IndelRealigner \
        -R ${reference} \
        -fixMisencodedQuals \
        -I ${ddir}/${sample}.nodup.bam \
        -targetIntervals ${realign}/${sample}.target.intervals.list \
        -o ${realign}/${sample}.realign.bam



# PHASE 1
# first call = high filters
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T HaplotypeCaller \
        -R ${reference} \
        -I ${realign}/${sample}.realign.bam \
        --genotyping_mode DISCOVERY \
        -stand_emit_conf 15 \
        -stand_call_conf 25 \
        -o ${call}/${sample}.filter.call.vcf

# recalibration
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T BaseRecalibrator \
        -R ${reference} \
        -I ${realign}/${sample}.realign.bam \
        -knownSites ${call}/${sample}.filter.call.vcf \
        -o ${call}/${sample}.recal.1.table


# apply recal
            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T PrintReads \
                -R ${reference} \
                -I ${realign}/${sample}.realign.bam \
                -BQSR ${call}/${sample}.recal.1.table \
                -o ${call}/${sample}.recal.1.bam



# PHASE 2
# first call = high filters
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T HaplotypeCaller \
        -R ${reference} \
        -I ${call}/${sample}.recal.1.bam \
        --genotyping_mode DISCOVERY \
        -stand_emit_conf 25 \
        -stand_call_conf 35 \
        -o ${call}/${sample}.filter.2.call.vcf

# recalibration
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T BaseRecalibrator \
        -R ${reference} \
        -I ${call}/${sample}.recal.1.bam \
        -knownSites ${call}/${sample}.filter.2.call.vcf \
        -o ${call}/${sample}.recal.2.table


# apply recal
            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T PrintReads \
                -R ${reference} \
                -I ${call}/${sample}.recal.1.bam \
                -BQSR ${call}/${sample}.recal.2.table \
                -o ${call}/${sample}.recal.2.bam


# PHASE 3
# first call = high filters
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T HaplotypeCaller \
        -R ${reference} \
        -I ${call}/${sample}.recal.2.bam \
        --genotyping_mode DISCOVERY \
        -stand_emit_conf 35 \
        -stand_call_conf 45 \
        -o ${call}/${sample}.filter.3.call.vcf

# recalibration
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T BaseRecalibrator \
        -R ${reference} \
        -I ${call}/${sample}.recal.2.bam \
        -knownSites ${call}/${sample}.filter.3.call.vcf \
        -o ${call}/${sample}.recal.3.table


# apply recal
            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T PrintReads \
                -R ${reference} \
                -I ${call}/${sample}.recal.2.bam \
                -BQSR ${call}/${sample}.recal.3.table \
                -o ${call}/${sample}.recal.3.bam



# END
# plots
# recal
    java -jar /home/neo/data/GenomeAnalysisTK.jar \
        -T BaseRecalibrator \
        -R ${reference}
        -I ${call}/${sample}.recal.3.bam \
        -knownSites ${call}/${sample}.filter.3.call.vcf \
        -BQSR ${call}/${sample}.recal.3.table \
        -o ${call}/${sample}.postrecal.table

# make sure to install R packages and dependencies
# reshape gplots ggplot2 gsalib
        java -jar /home/neo/data/GenomeAnalysisTK.jar \
            -T AnalyzeCovariates \
            -R ${reference}
            -before ${call}/${sample}.recal.1.table \
            -after ${call}/${sample}.postrecal.table \
            -plots ${call}/${sample}.recal.plots.3.pdf

#final calling
            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T HaplotypeCaller \
                -R ${reference} \
                -I ${call}/${sample}.recal.3.bam \
                --genotyping_mode DISCOVERY \
                -stand_emit_conf 40 \
                -stand_call_conf 50 \
                -o ${call}/${sample}.last.call.3.vcf


done


# print number of snps
for j in 1 2 3 4 5 6
do
    sample=${sample[${j}]}
    grep "QPX_v015" ${call}/${sample}.last.call.3.vcf | wc -l

done
#+END_SRC
*** Hard filtering SNPs 
This is done with =GATK=. first reason for the utility of this step is that we do not have a known list of QPX SNPs that can validate our calls. Second reason is to remove all SNPs that have bad quality, which is calculated with SNP confidence score and depth of coverage.

After calling SNPs and indels with =HaplotypeCaller= we can use =SelectVariants= to pick SNPs and separate them from indels.

The next script is saved in =mappingV6.sh=. The reference used therein is the genome v015 of steve Roberts. The quality by depth of coverage for each SNP and indel (QD) was set to =QD<5.0=. Each element that meets this criteria is discarded. At the end, 2 files are generated and contain either the SNPs or indels. The =ok= SNPs/indels are labelled in these files either with =PASS= or =DISCARD=.

#+BEGIN_SRC shell
#! /user/bin/bash


sample[1]=mmetsp0098
sample[2]=mmetsp001433
sample[3]=mmetsp00992
sample[4]=mmetsp001002
sample[5]=mmetsp0099
sample[6]=mmetsp00100

ddir=/media/sf_docs/data/rmdupY3

counts=${ddir}/counts
realign=${ddir}/realign
call=${ddir}/callV4
hard=${ddir}/hard

reference=/media/sf_docs/data/genomeSRv015/QPX_v015.fasta

mkdir ${hard}

for i in 1 2 3 4 5 6
do
    sample=${sample[${i}]}

# call SNPs
            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T SelectVariants \
                -R ${reference} \
                -V ${call}/${sample}.last.call.2.vcf \
                -selectType SNP \
                -o ${hard}/${sample}.raw.snps.vcf


            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T VariantFiltration \
                -R ${reference} \
                -V ${hard}/${sample}.raw.snps.vcf \
                --filterExpression "QD < 5.0 || FS > 60.0 || MQ < 40.0" \
                --filterName "DISCARD" \
                -o ${hard}/${sample}.filtered.snps.vcf

# call indels
            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T SelectVariants \
                -R ${reference} \
                -V ${call}/${sample}.last.call.2.vcf \
                -selectType INDEL \
                -o ${hard}/${sample}.raw.indel.vcf

            java -jar /home/neo/data/GenomeAnalysisTK.jar \
                -T VariantFiltration \
                -R ${reference} \
                -V ${hard}/${sample}.raw.indel.vcf \
                --filterExpression "QD < 5.0 || FS > 200.0" \
                --filterName "DISCARD" \
                -o ${hard}/${sample}.filtered.indel.vcf


done


    echo "These are SNPS that passed hard filtering\n"
for j in 1 2 3 4 5 6
do
    sample=${sample[${j}]}
    grep "PASS" ${hard}/${sample}.filtered.snps.vcf | wc -l

done


    echo "These are INDELS that passed hard filtering\n"
for k in 1 2 3 4 5 6
do
    sample=${sample[${k}]}
    grep "PASS" ${hard}/${sample}.filtered.indel.vcf | wc -l

done
#+END_SRC

**** Summary of data
References used:
1. Transcriptome SR v21
2. MMETSP0098 published assembly
3. Genome SR v15
4. MMETSP0098 custom assembly with SR genome v15
5. Combined assembly published of all MMETSPs

Libraries used:
1. MMETSP0098
2. MMETSP00992
3. MMETSP001002
4. MMETSP001433
5. MMETSP0099
6. MMETSP00100

#+CAPTION: Files and directories
| Task         | SNPs        | Script    | Directory            | Reference        | Libraries |
|--------------+-------------+-----------+----------------------+------------------+-----------|
| Assembly     |             | assembl   | assembl              | genome SR v15    | 98        |
| mapping/dup  |             | mappingV2 | mappingY             | all              | all       |
| realign/call | x1          | mappingV3 | rmdupY5/call         | Combined assembl | all       |
| realign/call | x1          | mappingV3 | rmdupY4/call         | MMETSP0098 cust  | all       |
| realign/call | x1          | mappingV3 | rmdupY3/call         | Genome SR v15    | all       |
| realign/call | x1          | mappingV3 | rmdupY2/call         | MMETSP0098 pub   | all       |
| realign/call | x1          | mappingV3 | rmdupY/call          | Transcriptome SR | all       |
| realign/call | x2          | mappingV4 | rmdupY4/callV4       | MMETSP0098 cust  | all       |
| realign/call | x2          | mappingV4 | rmdupY3/callV4       | Genome SR v15    | all       |
| realign/call | x2          | mappingV4 | rmdupY2/callV4       | MMETSP0098 pub   | all       |
| realign/call | x2          | mappingV4 | rmdupY/callV4        | Transcriptome SR | all       |
| realign/call | x3          | mappingV5 | rmdupY4/callV5       | MMETSP0098 cust  | all       |
| realign/call | x3          | mappingV5 | rmdupY3/callV5       | Genome SR v15    | all       |
| realign/call | x3          | mappingV5 | rmdupY2/callV5       | MMETSP0098 pub   | all       |
| realign/call | x3          | mappingV5 | rmdupY/callV5        | Transcriptome SR | all       |
| Hard filter  | SNPs+indels | mappingV6 | rmdupY3/callV4/hard2 | Genome SR v15    | all       |
| Hard filter  | SNPs+indels | mappingV6 | rmdupY5/callV4/hard2 | Combined assembl | all       |

The hard filtering step is done on the SNPs called after 2 sets of filtering. Meaning on the SNPs called with =mappingV4.sh=. The selected SNPs have been called after 2 sets of recalibration and one hard filtering step. They can be found on xsede in =rmdupY3/callV4/hard2=.

*** SNP processing
**** Desktop packages
1. [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815658/][Tablet 2010]] is a java package, it can be ran from a web-app [[http://bioinf.hutton.ac.uk/tablet/webstart/tablet.jnlp][here]]
2. IGV [[http://www.broadinstitute.org/igv/projects/current/igv_mm.jnlp][1GB]] [[http://www.broadinstitute.org/igv/projects/current/igv_lm.jnlp][2GB]] [[https://www.broadinstitute.org/software/igv/sites/cancerinformatics.org.igv/files/images/webstart_small2.jpg][10GB]] java web-apparent

One needs a bam file, indexed (w/ GATK, samtools, bwa ...), and a reference (fasta)
**** VCFTools
1. Setting up [[http://vcftools.sourceforge.net/examples.html][VCFTools]] and BioPerl (sat up earlier)
2. Dependencies: Tabix and bgzip (sudo apt-get install tabix)
3. examples using vcftools perl api [[http://vcftools.sourceforge.net/perl_examples.html][here]]
4. The following is done after hard filtering SNPs
5. Tables of SNPs can be rendered too, usefull for machine learning testing

Gunzipping a vcf file makes analysis faster. Tabix indexes the file.
#+BEGIN_SRC shell
bgzip file.vcf
tabix -p vcf file.vcf.gz
#+END_SRC

Compare vcf files entries. Meaning count the shared SNPs or indels between either libraries or SNPs/indels called by using different parameters and filters. The VCF files generated with =GATK= and hard-filtered afterward contain both =PASS= and =DISCARD= SNPs/indels.
The =-a= option will not compare the SNPs/indels that are tagged with DISCARD.
#+BEGIN_SRC shell
vcftools/bin/vcf-compare -a  file1.vcf.gz file2.vcf.gz
vcftools/bin/vcf-compare -a  file1.vcf.gz file2.vcf.gz | grep ^VN | cut -f 2- > compared.txt
#+END_SRC

***** Table rendering (opti1onal)
Remove =DISCARD= tagged SNPs with vcftools perl tool =vcf-annotate=. SNPs are hard-filtered with these tags. They are under the FILTER column in the vcf file. 
SNPs in the ALT (alternative column vs REF, the reference). Finally convert vcf to tab delimited file.
#+BEGIN_SRC shell
vcf-annotate --help
zcat file.vcf.gz | vcftools/bin/vcf-annotate -H | bgzip -c > pass.vcf.gz
zcat file.vcf.gz | vcftools/bin/vcf-annotate -H | vcftools/bin/vcf-to-tab > out.tab
#+END_SRC

Remove unnecessary label for each contig.
#+BEGIN_SRC shell
$ zcat mmetsp001433.filtered.snps.vcf.gz | \
../../../../vcftools_0.1.12b/bin/vcf-annotate -H | \
../../../../vcftools_0.1.12b/bin/vcf-to-tab > tables/mm1433.tab

$ cat mm1433.tab | sed 's/^U.*|//g' > mm1433.2.tab
sed 's/VA1/ALT/g' mm1002.2.tab > mm1002.txt
#+END_SRC

***** Concatenate files (optional)
For machine learning analysis SNPs from different strains must be compared together to distinguish which are absent and the nature of the one that do exist.
To concatenate filtered SNP files together, all columns must be the same. since each strain has been labelled differently during assembly, an additional step is implemented to standardise column names.
#+BEGIN_SRC shell
zcat mmetsp001433.filtered.snps.vcf.gz | sed 's/NY1/LIB/g' - | gzip -c > mm1433.snps.vcf.gz
## change MA1 in mm99_2
## change VA1 in mm1002
## change NY1 in mm98 and mm1433
#+END_SRC

Remove =DISCARD= labelled SNPs and change the label of each contig. Change this =QPX_v015_contig_= label if libraries are mapped with SR Genome v15. The below label is usefull for the combined assembly reference transcriptome.
#+BEGIN_SRC shell
zcat mm1433.snps.vcf.gz | ~/data/vcftools_0.1.12b/bin/vcf-annotate -H | sed 's/^U.*|//g' | bgzip -c > mm1433.vcf.gz
#+END_SRC

Concatenate all SNPs into a single file. (optional)
#+BEGIN_SRC shell
~/data/vcftools_0.1.12b/bin/vcf-concat mm992.vcf.gz mm98.vcf.gz mm1002.vcf.gz mm1433.vcf.gz | bgzip -c > all.snps.vcf.gz
#+END_SRC

Number of SNPs.
#+BEGIN_SRC shell
$ dim all.snps.vcf.gz 
3920
20
#+END_SRC

***** Extract custom columns 
Extract custom columns from =vcf.gz= compressed SNP file. =optional formatting=
#+BEGIN_SRC shell
~/data/vcftools_0.1.12b/bin/vcf-query mm1433.vcf.gz -f '%CHROM:%POS %ALT [m1433]\n' >> all.snps.sum.txt
#+END_SRC

Get description of VCF standard labels (columns and tags).
#+BEGIN_SRC shell
zcat mm1433.vcf.gz | grep "ID=DP" | head
## common tags GT:AD:DP:GQ:PL:FS
#+END_SRC

Get the columns names from the vcf file.
#+BEGIN_SRC shell
zcat mm1433.vcf.gz | grep "CHROM"
#+END_SRC

Extract columns from vcf file for machine learning analysis, with low number of samples (only 4, 1 for each assembled library).
#+BEGIN_SRC shell
~/data/vcftools_0.1.12b/bin/vcf-query mm98.vcf.gz -f '%CHROM %POS %ALT [%AD %DP %GQ %PL] m98\n' | sed 's/,/ /g' > m98.ml.txt
#+END_SRC

***** Additional GATK guidelines
If no hard-filtering was done, GATK generates one vcf file for both indels and SNPs. Comparing shared elements can be done with =vcf-compare -g=.
***** Get the number of shared SNPs between samples
Once numbers are extracted and shared data are summarized in a venn-friendly output, clean with the following command.
#+BEGIN_SRC shell
cat compared.txt | sed -e "s/.filtered.SNPs.vcf.gz //g" | sed -e "s/mmetsp00//g" > compared.cl.txt
#+END_SRC

All the previous tasks in one command.
#+BEGIN_SRC shell
vcftools_0.1.12b/bin/vcf-compare -a mmetsp0098.filtered.indel.vcf.gz mmetsp001002.filtered.indel.vcf.gz mmetsp00992.filtered.indel.vcf.gz mmetsp001433.filtered.indel.vcf.gz | grep ^VN | cut -f 2- | sed -e "s/.filtered.indel.vcf.gz //g" | sed -e "s/mmetsp00//g" > shared.indel.txt
#+END_SRC
***** Descriptive SNPs detection
5 libraries were used to look for relatedness from shared SNPs, ie., mmetsp0098, mmetsp00992, mmetsp001002, mmetsp001433, annd S. Roberts Genome assembly v15.
SNPs are extracted after realignment and hard filtering, scripts =mappingV5.sh= and =mappingV6.sh= respectively.
Contigs are mapped to S. Roberts genome v15. 
***** Preferential substitution of SNPs
Get stats of the number of time a nucleotide is preferentially changed into another specific nucleotide.
#+BEGIN_SRC shell
$ ~/data/vcftools_0.1.12b/bin/vcf-stats mmetsp001002.filtered.snps.vcf.gz | \
cut -f 1,2 -d '=' --output-delimiter=$'\t' - | \
sed -e 's/> //g' | \
grep '.>..*' | \
head -n 12 | \
sed -e "s/'//g" | \
sed -e "s/,//g" > vcf.stats.1002.txt
#+END_SRC

***** SNP densities
Get the total sum of the length of all assembled contigs. Get the count of SNPs called for that assembly. Divide the two scores then multiply by 1000 bp.
#+BEGIN_SRC shell
cat contigs.fa | grep "^>" | awk '{print $2}' | sed 's/len=//g' | awk '{s+=$1;  print s}' | tail -n 1
#+END_SRC


** Phase 4: Inferential analyzes and contig annotation
*** Clustering analysis of shared sequences
Export annotations of shared sequences between libraries and do some cleaning.
First, replace random reference number with library names.
#+BEGIN_SRC shell
cat table.csv| sed -e "s/4632846.3/mmetsp1002/g" | sed -e "s/4632845.3/mmetsp1433/g" | sed -e "s/4632739.3/mmetsp992/g" | sed -e "s/4632738.3/mmetsp98/g" | sed -e "s/4632737.3/QPX_v15/g" > table.txt
#+END_SRC
*** Functional phylogenomics based on transcriptome data
This [[http://angus.readthedocs.org/en/2014/genome-comparison-and-phylogeny.html][tutorial]] introduces some techniques and tools to address these objectives. Mainly this task relies on pairwise sequence comparisons.
1. Mauve as a multi aligner for different genomes
2. Search for TSS
3. Search for sRNAs
**** Drawing a circular genome
A long option is to draw a circos with perl modules. The fast way is to use =CGview=.
Its input is a an =xml= file. Can convert fasta, embl, genbank files to xml.
#+BEGIN_SRC shell
perl cgview/cgview_xml_builder/cgview_xml_builder.pl -sequence file.fa -output file.xml
java -jar cgview/cgview.jar -i file.xml -o file.png -f png 
#+END_SRC
**** MG-RAST
Upload assemblies to MG-RAST to get some stats and read description.  
**** Phylogeny analysis
Straightforward but not simple task. Takes a lot of hard disk size for database downloads.
***** Pipeline
1. Sequence samples
2. Assemble de novo
3. Find a nearest reference to the assembly on the tree of life
4. Order assembly contigs with the nearest reference
5. Find homologous contigs to a standardized list of =elite genes=
6. Align contigs to a list of maker genes
7. Infer a phylogeny based on aligned homologous shared genes
***** Packages needed
1. Mauve [[http://darlinglab.org/mauve/download.html][download page]]
2. Phylosift [[https://phylosift.wordpress.com/][web page]]
3. HMMER [[ftp://selab.janelia.org/pub/software/hmmer3/3.1b1/Userguide.pdf][userguide]]
4. Archaeotperyx from frontiers [[https://sites.google.com/site/cmzmasek/home/software/archaeopteryx][google site]]
5. Advanced shell skills
***** Phylogenetic placement 
All compiled data are found in phylosift directory under =PS_temp= folder.
Run =phylosift= to find the nearest neighbor on the tree of life.
#+BEGIN_SRC shell
./phylosift all file.fa
## or ... 
./phylosift all --besthit file.fa
#+END_SRC 
Or on all files =contigs.fa= placed inside phylosift directory together.
#+BEGIN_SRC shell
find . -maxdepth 1 -name "*fa" -exec ./philosift all {} \;
#+END_SRC

Visualize in firefox (krona) and archaeopteryx (xml).
#+BEGIN_SRC shell
firefox ./physlosift/PS_temp/file.fa/file.fa.html
java -cp ./forester_1038.jar org.forester.archaeopteryx.Archaeopteryx -c ./_aptx_configuration_file ./physlosift/PS_temp/file.fa/file.fa.xml
#+END_SRC

The =all= label will run the =Core marker set= for alignment. Fast and small sized. Add =extended= label for =Extended marker set= bigger (70 Gb).
#+BEGIN_SRC shell
./phylosift all --extended file.fa
#+END_SRC

=(optional)= Once the nearest reference is found and visualized with =archaeopteryx=, download from ncbi the species genome, then order our assembly scaffolds with =Mauve= using that genome.
#+BEGIN_SRC shell
./Mauve # GUI
# or ...
java -Xmx5000g -Djava.awt.headless=true -cp ./Mauve  org.gel.mauve.contigs.ContigOrderer -output ordered -ref reference.fa -draft contigs.fa
#+END_SRC

Finally use =phylosift= to build a phylogeny. Phylosift is based on:
1. pplacer = minimum likelihood and bayesian phylogenetic placement of sequences onto fixed reference tree.
2. Adaptive seeds to tame genomic sequence comparison
3. RNA alignment tool
4. Bowtie to align short DNA reads
5. HMMER 3.0
6. Phylogenetic diversity tools

Search for homologous sequences between assemblies with the =search= label. Assemblies are deposited in =phyogeny= directory inside =phylosift= directory (for convenience). =besthit= will remove lower scored hits and keep the highest. =isolate= label indicates distinct assemblies to be analyzed separately.
#+BEGIN_SRC shell
find ./phylogeny -maxdepth 1 -name "*fa" -exec ./phylosift search --isolate --besthit {} \;
#+END_SRC

Results are deposited inside phylosift directory =PS_temp=. Next align the homologous contigs found earlier together.
#+BEGIN_SRC shell
find ./phylogeny -maxdepth 1 -name "*fa" -exec ./phylosift align --isolate --besthit {} \;
#+END_SRC

At this step 2 folders are created in PS_temp. One for homlogy analysis and an other for alignment. Both contain lots of unique files for each contig. Inside the alignment repository we find a =concat.codon.updated.1.fasta= file that contain the collection of the homologously aligned contigs shared between assemblies. The following script will concatenate all =concat= file assemblies into 1 for phylogeny creation.
#+BEGIN_SRC shell
find ./PS_temp -type f -regex '.*concat.codon.updated.1.fasta' -exec cat {} \; | sed "s/\.1\..*//" > hom.aligned.fa
#+END_SRC

Create tree.
#+BEGIN_SRC shell
./phylosift/bin/FastTree -nt -gtr < hom.aligned.fa > hom.aligned.tre
#+END_SRC

Visualize the tree with =archaeopteryx=.
#+BEGIN_SRC shell
java -cp ./forester_1038.jar org.forester.archaeopteryx.Archaeopteryx -c ./_aptx_configuration_file hom.aligned.tre
#+END_SRC

*** Gene Finding
1. Apply homology on all strains either (alternate strategy)
   - pairwise sequence comparison
   - tree-based analysis
2. Infer orhtology
3. find gene duplication
4. find genome duplication

**** Blast 2 GO
***** Activation keys
PRO version: B2G-BASSSLEI-E4A5CD5459B0C0C3D75B6E853B518168
Basic: B2G-BASSSLEI-CC5E4E288A87C7C3E262BF0B0DC2EE9A
**** Contig annotation with HMMER
As a main strategy the functional annotation is done with HMMER, the alignment is based on hidden markov models that calculate posteriors to the similarity scores.
***** Library preparation
Download and Install HMMER
#+BEGIN_SRC shell
wget http://selab.janelia.org/software/hmmer3/3.1b2/hmmer-3.1b2-linux-intel-x86_64.tar.gz
./configure
sudo make
sudo make install
cd easel: sudo make install
#+END_SRC 

Download Pfam 28.0 database (as of 06/20/2015). It is possible to download the fasta database. But in this case an HMM profile must be built. The process will than take over 3 hours.
#+BEGIN_SRC shell
ftp ftp.ebi.ac.uk
anonymous
<<no password>>
cd pub/databases/Pfam/current_release/
get Pfam-A.hmm.gz
bye
gzip -d Pfam-A.hmm.gz
#+END_SRC

Index the Pfam.hmm database. this will produce 16,230 accessions.
#+BEGIN_SRC shell
hmmpress Pfam-A.hmm
#+END_SRC

=hmmscan= is a function used to search =Pfam-A.hmm= profiles. Otherwise if we had a sequence database =hmmsearch= would've been used. The query used is either a peptide or an HMM profile produced with =hmmbuild= or multiple HMM alignment profiles produced with =hmmalign= which generates a =stockholm= format alignment file. The stockholm file is then fed to hmmbuild to make an HMM query profile.

Pfam can be searched using keywords and =accession= numbers can be extracted with copy/paste into a txt file. Get the accession number from gene of interest.
***** Translate contigs to peptides
Using =Transeq= from Emboss. If an error occurs after the first =make install= try =ldconfig= then =make install= a second time. Make install can be replaced with =checkinstall= for creating a deb package that can be removed without =make uninstall=.
#+BEGIN_SRC shell
wget ftp://emboss.open-bio.org/pub/EMBOSS/old/6.5.0/EMBOSS-6.5.7.tar.gz
./configure
sudo make
sudo make install
sudo ldconfig
sudo make install
#+END_SRC

Translate in 6 frames from fasta file. [[http://www.sacs.ucsf.edu/Documentation/emboss/transeq.html][Documentation]]
#+BEGIN_SRC shell
## FIRST correct name of each sequence
cat assembled.contigs.fasta | sed 's/|.*len/ len/g' > assembled.contigs.fa
## SECOND translate in 6 frames
transeq assembled.contigs.fa peptides.fa -frame=6 -clean=yes
## THIRD remove length and description
cat peptides.fa | sed 's/ len.*$//g' > peptides.clean.fa
#+END_SRC

***** Annotating all peptides (pfam)
Annotation of the 4 strains peptides against a Pfam v28 updated database. Here we have two choices, first, annotate against the whole pfam library, second, annotate against a subset of selected HMM profiles of PFAM. The latter is mostly beneficial if one wants to extract =contig= number to find SNPs. However its not a straightforward process. Refer to p.50 of the HMMER3 userguide.
#+BEGIN_SRC shell
hmmscan --domtblout <output.txt> --cpu 4 <pfam.subset.hmm> <peptides.fa>
#+END_SRC

***** Summary 
#+CAPTION: Keywords used in PFAM
| Keyword        | Pfam-A |  a98 | s98 | a992 | s992 | a1002 | s1002 | a1433 | s1433 |
|----------------+--------+------+-----+------+------+-------+-------+-------+-------|
| virulence      |    655 | 5098 | 313 | 3075 |  261 |  4606 |   291 |  4794 |   308 |
| temperature    |    251 | 2484 | 168 | 1680 |  141 |  2283 |   164 |  2277 |   161 |
| salinity       |     22 |  163 |  13 |   91 |    9 |   123 |    10 |   137 |    12 |
| salt tolerance |     79 | 2231 |  70 | 1422 |   64 |  2097 |    66 |  2078 | 66    |

***** Selected protein domains (strategy 1)
=outdated pfam= Count the number of domains found inside the =analysis/extras/hmmer3.pfam.hits= output file for each strain. The code below will extract HMM profiles in the annotated output HMMER file.hits.
#+BEGIN_SRC shell
cut -f 1 ./query/virulence.pfam.txt | sed 's/ //g' | grep -Ff - ../analysis/extras98/hmmer3_pfam.hits | grep ">>" | wc -l
#+END_SRC  

=updated pfam= On the other hand, the new versions of pfam and HMMER3.2b dont add the accession number for each domain. this means: domain pattern search is done on =-w= whole words and using the domain keyword.
#+BEGIN_SRC shell
cut -f 2 ../query/virulence.pfam.txt | sed 's/  //g' | grep -Fwf - m98.pfamA.txt | grep ">>" | sort - | uniq | wc -l
#+END_SRC

=outdated pfam= Get the number of single domains found using old data. this number is particularly descriptive of the number of potential genes in the contig library.
#+BEGIN_SRC shell
cut -f 1 ./query/virulence.pfam.txt | sed 's/ //g' | grep -Ff - ../analysis/extras98/hmmer3_pfam.hits | grep ">>" | sort - | uniq | wc -l
#+END_SRC


The pipeline used with old annotated contigs is to extract gene of interest from already annotated contigs versus protein domain databases. The new pipeline with the new versions of HMMER3.2b and Pfam-A v28 is to annotate the contigs against a subset of Pfam gene of interest.

***** Subsetting Pfam database (strategy 2)
This step is necessary to get the contig numbers of the identified protein domains found above. All files are located in the HMMER directory under =analysis= or =libraries= folders.

First to get a subset out of =pfam.hmm= we need to index it for fast extraction. Pfam must be hmmpressed too.
#+BEGIN_SRC shell
hmmfetch --index pfam-A.hmm
#+END_SRC

Many fails can happen when constructing hmmscan pipelines for a subset of databases. See p50 of Hmmer Userguide.

Second, the list of desired sequences/profiles (got using keywword search [[http://pfam.xfam.org/search/keyword?query%3Dsalt%2Btolerance][here for example]]) must be formated like so: <NAME> - <ACCESSION> for each entry.
#+BEGIN_SRC shell
cut -f 1,2 ../../query/salinity.pfam.txt | awk '{ print $2 " - " $1 }' | head
#+END_SRC

Finally, =hmmfetch= desired domains, =hmmpress= them, then annotate the 4 strains. This process of creating subset is done on each list of domain. Output formats can be found [[http://www.unix.com/man-page/debian/1/hmmscan/][here (debian man page)]].
#+BEGIN_SRC shell
cut -f 1,2 ../../query/salinity.pfam.txt | awk '{ print $2 " - " $1 }' | hmmfetch -f Pfam-A.hmm - > pfam.subset.hmm
hmmpress pfam.subset.hmm 
hmmscan --domtblout <output.txt> --cpu 4 <pfam.subset.hmm> <peptides.fa>
hmmscan --domtblout C.txt --cpu 4 ../db/Pfam-A.hmm ./peptides/C.peptides.QPXv15.fa
#+END_SRC

The script above saves a table for each domain identified. Contains accession numbers for contigs and Pfam domains, as well as posterior statistics.
Extract the accession number of contigs that contains potential protein domains. The code below will remove the first 3 lines of the output file of hmmscan. For more =awk= oneliners, visit [[http://www.pement.org/awk/awk1line.txt][here]].
#+BEGIN_SRC shell
cat salinity.pfam/m1002.txt | awk '{ NF > 10; if ($8 > 350) print $4 "\t" $8}'
#+END_SRC

Get the number of domains identified in the subset annotation. In the code below the domains have a 10e-4 significance.
#+BEGIN_SRC shell
cat virulence.pfam/m1002.txt | awk '{ NR>3; if ($7 < 0.0001) print $2 }' | sort - | uniq | grep "^P" | wc -l
#+END_SRC

Get the number of contigs that match at least one domain. In the code below the contigs have a 10e-10 significance.
#+BEGIN_SRC shell
cat virulence.pfam/m1002.txt | awk '{ NR>3; if ($7 < 0.0000000001) print $4 }' | sort - | uniq | grep "^M" | wc -l
#+END_SRC

***** Locating SNPs on identified pfam domains
=note= useful perl and awk oneliners can be found [[http://bioinformatics.cvr.ac.uk/blog/short-command-lines-for-manipulation-fastq-and-fasta-sequence-files/][here]] and [[https://github.com/stephenturner/oneliners][here]]

=general instructions= Output sequence name and its length for every sequence within a fasta file.\
#+BEGIN_SRC shell
cat file.fa | awk '$0 ~ ">" {print c; c=0;printf substr($0,2,100) "\t"; } $0 !~ ">" {c+=length($0);} END { print c; }'
#+END_SRC

=general instruction= Get =one= sequence from fasta file with a known =id=.
#+BEGIN_SRC shell
perl -ne 'if(/^>(\S+)/){$c=grep{/^$1$/}qw(id1 id2)}print if $c' sample1.fa
#+END_SRC

=general instruction= Get a =list= of sequences from a fasta file. The id list contains one id per line without spaces (replace spaces with dots in sequence.fa and ids.txt).
#+BEGIN_SRC shell
cat sequences.fa | sed 's/^>.*|/>/g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' ids.txt -
#+END_SRC

=general command= Create a list of ids for each strain and for each category of protein. by filtering out peptides with an evalue higher than but not equal to 10e-10.
#+BEGIN_SRC shell
cat salinity.pfam/m98.txt | awk '{print $4}' | sed 's/^.*|//g' | sed 's/_1//g' | sort - | uniq | grep "[0-9]" | grep "^[^\.]" | grep "^[^/]" > salinity.contig/m98.id.txt
#+END_SRC

Get the nucleotide sequences for the identified pfam domains for each strain, but first, modify the header of each fasta sequence (fasta that contain the contigs).
#+BEGIN_SRC shell
cat contigs.fa | sed 's/^>.*|/>/g'
#+END_SRC

Get contigs for each identified domain. =note= Oftentimes the number of contigs is lower than the number of domains. One nucleotide sequence can produce more than one peptide sequence (3 frameshift possibilities x 2 strands) : [[http://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM%3Dblastp&PAGE_TYPE%3DBlastSearch&LINK_LOC%3Dblasthome][blastp]] [[http://web.expasy.org/translate/][exPasy (translate RNA)]] for testing.
#+BEGIN_SRC shell
cat virulence.pfam/m1002.txt | awk '{ if ($7 < 0.0000000001) print  $4 }' | sort - | uniq | grep "^M" | sed 's/^M.*|//g' | sed 's/_1//g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' - ../../data/qpx/mme98/contigs.mod.fa | grep ">" | wc -l
#+END_SRC

***** BLAT (Locating SNPs continued)
Blat can be found also on xsede. [[http://genome.ucsc.edu/goldenPath/help/blatSpec.html][Documentation]] and [[http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/blat/][Blat linux packages]]
Installation.
#+BEGIN_SRC shell
ftp hgdownload.cse.ucsc.edu
Name: anonymous
cd admin/exe/linux.x86_64/blat
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/faToTwoBit
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslSort
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslReps
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslPretty
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslStats
chmod +x ./blat
chmod +x ./faToTwoBit


## OR
git clone https://github.com/neocruiser/blat.git
#+END_SRC

Convert the genome to =2bit= (faster). This step will index the genome and place it in the computer memory for fast pulling of alignments.
#+BEGIN_SRC shell
./faToTwoBit ../genomev015/QPX_v015.fasta ../genomev015/qpxv15.2bit
#+END_SRC

Align RNAseq contigs to genome. =psl= is a tabulated output.
#+BEGIN_SRC shell
./blat ../genomev015/qpxv15.2bit ../db/salinity.pfam/m992.contigs.pfam.fa output.test.psl
#+END_SRC

Show the alignment in a human readable format.
#+BEGIN_SRC shell
./pslPretty 2> pslpretty.README.txt
./pslPretty <psl file> <genome target 2bit> <query fa> <output.txt>
#+END_SRC

Get overall statistics.
#+BEGIN_SRC shell
./pslStats -overallStats <psl file> <output>
#+END_SRC

Get the contigs. After =Blat= on the indexed genome the overall stats show the mean length of the aligned contigs. Since each contig can be found multiple times in the genome (at different alignment lengths of course) it is best if we choose the best contigs those that have a maximum alignment length. For those contigs must be mapped/aligned once and thus, no duplicate entries should be selected for whatever contig. For this reason choosing an alignment length equal to the half of the mean of the alignment length gives the minimum number of duplicate contigs.
#+BEGIN_SRC shell
# choose genome contigs that align to at least half of the mean of the alignment length
## get overall stats of one strain for each gene set
./pslStats -overallStats ../data/analysis/salinity.pfam/m1002.genomv15.psl m1002.pretty && cat m1002.pretty
cat m1002.pretty | awk 'NR>2' >> salinity.stats.txt
## watch the number of duplicates
cat m1002.genomv15.psl | awk '{if ($1 >= 900) print $10 }' | awk 'NR>3' | sort - |uniq | wc -l
#+END_SRC

Get SNPs. Script will select custom columns, necessary for the next step. This will create one file for each strain, total 4.
#+BEGIN_SRC shell
vcf-query m98.SNPs.passed.vcf.gz -f '%CHROM %POS %REF %ALT %QUAL [m98]\n' > m98.SNPs.custom.txt
#+END_SRC

=fast querying= Get genome contigs + SNPs. Those contigs where aligned to RNAseq contigs which means they contain an identified pfam protein domain. The mean Query sizes (meanQsize) from the overall stat can be visualized for all strains and gene sets in the R report section =Map RNA contigs to Genome v15 contigs=. At the end of this script we will get 1 file for each pfam subset and for each strain, equal to 14 files. All files for each strain will be merged together.
#+BEGIN_SRC shell
cat ~/data/analysis/virulence.pfam/m98.genomv15.psl | awk '{ if ($1 >= 2900) print $14}' | awk 'NR>3' | sort - | uniq | grep -Fwf - m98.snps.custom.txt | less
#+END_SRC

Create separate files for each strain and gene set. Script below is the first half of the one above. Choose a mean query size of at least 1500 that has been matched to the reference genome.
#+BEGIN_SRC shell
cat ~/data/analysis/virulence.pfam/m98.genomv15.psl | awk '{ if ($1 >= 1500) print $0}' > ~/data/analysis/hotspots/m98.vir.top.aln.txt
#+END_SRC

Merge the pfam subset files for each strain.
#+BEGIN_SRC shell
## add pfam label and strain name as last columns + remove wrong header and 
## concatenate files for the same strain.
cat m1002.salinity.top.aln.txt | awk 'NR>3' | sed 's/$/\tsalinity/g' | sed 's/$/\tm1002/g' >> m1002.top.aln.txt
#+END_SRC

At this point we have 4 SNPs/reference genome files and 4 RNAseq contigs/reference contigs/4 pfam subsets for each strain. We will use the genome contig labels to extract SNP positions.
#+BEGIN_SRC shell
cat m98.top.aln.txt | awk '{print $14}' | sort - | uniq | grep -Fwf - m98.SNPs.custom.txt > m98.SNPs.aln.position.txt
#+END_SRC

Merge aligned contigs to reference and the position of SNPs in the reference in R.
#+BEGIN_SRC R
## get file with genome aligned to rnaseq contigs
x <- read.table("./hotspots/m98.top.aln.txt")
header.x <- c("match", "mismatch", "repmatch",
            "N", "QgapCount", "QgapBases",
            "TgapCount", "TgapBases", "Strand",
              "Qname", "Qsize", "Qstart", "Qend", "Tname",
            "Tsize", "Tstart", "Tend", "BlockCount",
            "BlockSize", "qStarts", "tStarts",
            "pfam", "lib")
colnames(x) <- header.x

## get file with SNPs position
y <- read.table("./hotspots/m98.SNPs.aln.position.txt")
header.y <- c("Tname", "Position", "REF", "ALT",
              "Quality", "lib")
colnames(y) <- header.y

## merge
z <- merge(x, y, by = "Tname")

## save final concatenated file
write.table(z, "m98.contigs.SNPs.txt", quote = F, sep ="   ")
#+END_SRC

Merge the above file (containing SNPs + genome contigs + rnaseq contigs) with pfam domains.
Concatenate pfam subsets of each strain together
#+BEGIN_SRC shell
cat virulence.pfam/m1002.txt | awk '{ if ($7 < 0.0000000001) print  $1"\t"$2"\t"$3"\t"$4"\t"$6"\t"$7"\t"$8"\t"$12"\t"$13"\t"$14"\t"$18"\t"$19"\t"$22"\t"$23 }' | sed '/^#.*$/d' | sed 's/MME.*|\(.*\)_1/\1/g' | sed 's/$/\tvirulence\tm98/g' >> ./pfam.final/m98.pfam.txt
#+END_SRC

R code to merge the above file (the one with all pfam domains) with the SNP data.
#+BEGIN_SRC R
header.a <- c("Domain", "accession", "tLen", "qName",
              "qLen", "evalue", "score2", "cEvalue",
              "iEval", "score", "alnFrom",
              "alnTo", "acc", "description", "pfam", "lib")
a <- read.table("./hotspots/m1433.pfam.txt", fill = NA)
colnames(a) <- header.a
head(a)
dim(a)
b <- merge(z, a, by.x = "Qname", by.y = "qName")
write.table(b, "./hotspots/m1433.pfam.SNP.txt", quote = F, sep ="   ")
#+END_SRC

***** Key description
The final file above contains 43 columns or keys. Here is the description of each key and their job significance.
| Key         | Job   | Description                                                                                                      |
|-------------+-------+------------------------------------------------------------------------------------------------------------------|
| Qname       | BLAT  | Query identifier (RNAseq DNA contig)                                                                             |
| Tname       | BLAT  | Target identifier (Reference genome)                                                                             |
| match       | BLAT  | Number of nucleotides that match  between Q and T                                                                |
| mismatch    | BLAT  | Number of nucleotides that dont match                                                                            |
| repmatch    | BLAT  | Number of nucleotides that match a repetitive region                                                             |
| N           | BLAT  | N nucleotides in the query sequence                                                                              |
| QgapCount   | BLAT  | Number of gaps in Q                                                                                              |
| QgapBases   | BLAT  | Length of gaps in Q                                                                                              |
| TgapCount   | BLAT  | Number of gaps in T                                                                                              |
| TgapBases   | BLAT  | Length of gaps in T                                                                                              |
| Strand      | BLAT  | +/-                                                                                                              |
| Qsize       | BLAT  | Size of the RNAseq contigs                                                                                       |
| Qstart      | BLAT  | Alignment start position in RNAseq contigs                                                                       |
| Qend        | BLAT  | Alignment end position in RNAseq contigs                                                                         |
| Tsize       | BLAT  | Size of the Reference genome contig                                                                              |
| Tstart      | BLAT  | Alignment start position in Reference contigs                                                                    |
| Tend        | BLAT  | Alignment end position in Reference contigs                                                                      |
| BlockCount  | BLAT  | Number of aligned regions without gaps                                                                           |
| BlockSize   | BLAT  | Size of the aligned regions without gaps                                                                         |
| qStarts     | BLAT  | Start positions of the blocks in the RNAseq contigs                                                              |
| tStarts     | BLAT  | Start positions of the blocks in the reference genome                                                            |
| pfam.x      | BLAT  | Pfam category that map to the Rnaseq contigs                                                                     |
| lib.x       | BLAT  | Strain                                                                                                           |
| Position    | GATK  | Position of the SNP in the Reference genome                                                                      |
| REF         | GATK  | Reference nucleotide at one allele                                                                               |
| ALT         | GATK  | Alternate nucleotide at one allele                                                                               |
| Quality     | GATK  | Genomic quality at one allele                                                                                    |
| lib.y       | GATK  | Strain                                                                                                           |
| Domain      | HMMER | Identified pfam protein domain                                                                                   |
| accession   | HMMER | Accession number of each pfam domain                                                                             |
| tLen        | HMMER | Domain length in peptide count                                                                                   |
| qLen        | HMMER | RNAseq peptide length                                                                                            |
| evalue      | HMMER | Statistical significance of the match of the whole sequence (relative to Q size and T database size)             |
| score2      | HMMER | Log-odd of the whole RNAseq peptide (for evalue estimation, non relative to T database size)                     |
| cEvalue     | HMMER | Conditional-evalue, statistical significance for each domain                                                     |
| iEvalue     | HMMER | Independent-evalue, similar to the 1 domain evalue                                                               |
| score       | HMMER | Log-odd of each identified domain of the RNAseq peptide (for evalue estimation, non relative to T database size) |
| alnFrom     | HMMER | First RNAseq peptide that align to the pfam domain                                                               |
| alnTo       | HMMER | Last RNAseq peptide that align to the pfam domain                                                                |
| acc         | HMMER | Expected accuracy per residue of the alignment (posterior probability)                                            |
| description | HMMER | Short name description of the domain                                                                             |
| pfam.y      | HMMER | Pfam category that map to the RNAseq contigs                                                                     |
| lib         | HMMER | Strain                                                                                                           |

**** Locate SNPs hotspots
How many SNPs can be found outside and ahead of a protein domain?
#+BEGIN_SRC shell
cat all.pfam.snp.txt | awk '{if ($25 < $17) print $3,$25,$26,$27}' | sort - | uniq | wc -l
#+END_SRC

How many SNPs can be found outside and after a protein domain?
#+BEGIN_SRC shell
cat all.pfam.snp.txt | awk '{if ($25 > $18) print $3,$25,$26,$27}' | sort - | uniq | wc -l
#+END_SRC

How many SNPs can be found inside a protein domain?
#+BEGIN_SRC shell
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$25,$26,$27 }' | sort - | uniq | wc -l
#+END_SRC

How many SNPs can be found inside =virulence= domains?
#+BEGIN_SRC shell
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$25,$26,$27,$23 }' | sort - | uniq | awk '{if ($5 == "virulence") print $0}' | wc -l
#+END_SRC

How many SNPs can be found outside (before and after domain) of =virulence= domains?
#+BEGIN_SRC shell
cat all.pfam.snp.txt | awk '{if ($25 < $17) print $3,$25,$26,$27,$23}' | sort - | uniq | awk '{if ($5 == "virulence") print $0}' | wc -l
cat all.pfam.snp.txt | awk '{if ($25 > $18) print $3,$25,$26,$27,$23}' | sort - | uniq | awk '{if ($5 == "virulence") print $0}' | wc -l     
#+END_SRC

How many SNPs can be found in =m98= NY strain?
#+BEGIN_SRC shell
## inside domain
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$24,$25,$26,$27 }' | sort - | uniq | awk '{if ($2 == "m98") print $0}' | sort - | uniq | wc -l
#+END_SRC

Sum of the length of each contig with SNPs inside pfam domains for each strain. With the script above we can normalize the SNP counts.
#+BEGIN_SRC shell
## inside
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$24,$13 }' | sort - | uniq | awk '{if ($2 == "m98") print $0}' | sort - | uniq | awk '{s+=$3; print s}' | tail -n 1 
#+END_SRC

How many SNPs can be found for =m98= NY in =viruelnce= domains?
#+BEGIN_SRC shell
## outside after
cat all.pfam.snp.txt | awk '{if ($25 > $18) print $3,$24,$25,$26,$27,$23}' | sort - | uniq | awk '{if ($2 == "m98") print $0}' | sort - | uniq | awk '{if ($6 == "virulence") print $0}' | wc -l 

## inside
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$24,$25,$26,$27,$23}' | sort - | uniq | awk '{if ($2 == "m98") print $0}' | sort - | uniq | awk '{if ($6 == "virulence") print $0}' | wc -l
#+END_SRC

Sum of length of each contig for each pfam domain between strains. Total sum of sizes is used for normalization with script above.
#+BEGIN_SRC shell
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$24,$13,$23}' | sort - | uniq | awk '{if ($2 == "m98") print $0}' | sort - | uniq | awk '{if ($4 == "virulence") print $0}' | sort - | uniq | awk '{s+=$3; print s}' | tail -n 1 
#+END_SRC

=general= Using all the scripts above give the net number of SNPs. The number doesn't show the net number of domains that contain these SNPs. For example, the output below shows that the SNP at position 23 can map to 3 different pfam domains. However this same SNP is only found inside and in the salt tolerance pfam domains.
#+BEGIN_SRC shell
Reference    Strain    Position   REF   ALT   PFAM   Gene   Accession
QPX_v015_contig_1247 m1002 23 C G salt.tolerance AAA_21 PF13304.2
QPX_v015_contig_1247 m1002 23 C G salt.tolerance ABC_membrane PF00664.19
QPX_v015_contig_1247 m1002 23 C G salt.tolerance ABC_tran PF00005.23 
#+END_SRC

=general= What are the protein domains found with high or low SNP count for =all= strains? This script focuses on the proteins found not on SNP count, so the output will be longer. Examining the evalue is encouraged.
#+BEGIN_SRC shell
## inside
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$24,$25,$26,$27,$23,$30,$31,$34,$35}' | sort - | uniq | awk '{if ($6 == "virulence") print $0}' | wc -l
#+END_SRC

Preferential substitution inside/outside domains per pfam subset for each strain. Here we would like to count both the number of domains with differential mutation and the preferential nature of each mutation.
#+BEGIN_SRC shell
## ahead
cat all.pfam.snp.txt | awk '{if ($25 < $17) print $3,$24,$25,$26,$27,$23,$30,$31,$34,$35}' | sort - | uniq | awk '{if ($2 == "m98") print $0}' | sort - | uniq | awk '{if ($6 == "virulence") print $0}' | cut -f 4,5 -d " " --output-delimiter=$'>' | sort - | uniq -c

## inside
cat all.pfam.snp.txt | awk '{if ($25 >= $17) print $0}' | awk '{ if ($25 <= $18) print $3,$24,$25,$26,$27,$23,$30,$31,$34,$35}' | sort - | uniq | awk '{if ($2 == "m98") print $0}' | sort - | uniq | awk '{if ($6 == "virulence") print $0}' | cut -f 4,5 -d " " --output-delimiter=$'>' | sort - | uniq -c
#+END_SRC

**** BLAST+
Download NR library (protein database NCBI). Index it (this will generate .pin files. p for protein). If NT was downloaded and indexed .nin files will be generated.

Set the database path. ()
#+BEGIN_SRC shell
export BLASTDB="/media/sf_data/db/nr"
#+END_SRC

Or write path in login profile.
#+BEGIN_SRC shell
cat >> ~/.profile
BLASTDB=/media/sf_data/db:$BLASTDB; export BLASTDB
BLASTDB=/media/sf_data/db/nr:$BLASTDB; export BLASTDB
#+END_SRC

Blastx. Use nucleotide query and blast will translate it in 6 frames. Use Transeq only if using hmmer
#+BEGIN_SRC shell
blastx -query nodule/assembled/C.assembl.QPXgv5.fasta \
-db nr \
-outfmt "7 qseqid qlen sseqid slen qstart qend sstart send evalue bitscore length pident nident mismatch gaps staxids sscinames " \
-max_target_seqs 10 \
-out output.txt \n
-num_threads 16
#+END_SRC

Blast sequence similarity analysis are done with NR (protein), NT (nucleotide), SWISSPROT (protein).

***** Sequence homology analysis
How many assembled contigs have been aligned to a SWISSPROT entry (NCBI) with a minimum of 10e-10 evalue, 80% sequence similarity, and 1 mismatch. Repeat for NT and NR. Only done on Blast output not hummer.
#+BEGIN_SRC shell
cat A.swissprot.txt | grep "^GG" | awk '{if ($9 <= 0.0000000001) print $0}' | awk '{if ($12 >= 80) print $0}' | awk '{if ($14 <= 1) print $0}' | cut -f 1 | sed 's/|.*$//g' | sort - | uniq | wc -l
#+END_SRC
**** Gene-gene interaction
2. RAST [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3965101/][Overbeek 2013]] =seed=
3. Functional annotation, a list of databases [[http://compgenomics2015.biology.gatech.edu/index.php/Functional_Annotation_Group#Coding_Region_--_Prokka.5B3.5D][here]]
***** Databases

#+CAPTION: Sequence databaases in public repositiories
| database   | content    | tool  | function            | description           |
|------------+------------+-------+---------------------+-----------------------|
| [[http://pfam.xfam.org/][Pfam]]       | protein    | hmmer | domain              | protein similarities  |
| [[ftp://ftp.ncbi.nih.gov/blast/db/][NR]]         | protein    | blast | domain              | protein similarities  |
| [[ftp://ftp.ncbi.nih.gov/blast/db/][NT]]         | nucleotide | blast | classification      | phylogeny             |
| [[ftp://ftp.ncbi.nih.gov/blast/db/][Swiss-prot]] | protein    | blast | domain              | protein similarities  |
| [[http://www.phi-base.org/release_notes.php][Phi-base]]   | protein    | hmmer | interactions        | virulence             |
| [[http://www.mgc.ac.cn/VFs/main.htm][VFDB]]       | prot/nucl  | blast | virulence           |                       |
| [[http://string-db.org/newstring_cgi/show_download_page.pl?UserId%3D_yWWhZVtWw47&sessionId%3DDpBbN0jI1blB][STRING]]     | protein    | blast | interactions        | network analysis      |
| [[http://thebiogrid.org/][BioGRID]]    |            | shell | interactions        |                       |
| [[http://www.transcriptionfactor.org/index.cgi?Download][DBD]]        | protein    | shell | transcrption factor | acc. Pfam+superfamily |
| [[http://operondb.jp/][OperonDB]]   |            |       | operons             |                       |
| [[http://www.ncbi.nlm.nih.gov/COG/][COG]]        | protein    |       | classification      | phylogeny             |
| [[http://phospho.elm.eu.org/][Pospho-elm]] | protein    | shell | phosphorylation     | acc Uniprot+EnsEMBL   |
| [[http://www.jcvi.org/cgi-bin/tigrfams/index.cgi][TIGRFAM]]    | protein    | blast | subfamilies         | classification        |

- More databases can be found [[http://pir.georgetown.edu/cgi-bin/iproclass_stat][here]]
- Known and predicted protein-protein interactions [[http://string-db.org/newstring_cgi/show_download_page.pl?UserId%3D_yWWhZVtWw47&sessionId%3DDpBbN0jI1blB][STRING]]. Protein database. Searched with =blastx=. Indexed with =makeblastdb= but without =-parse_seqids= because its a network data. Proteins have duplicate seq ids. Download STRING alias id text file that include convectional protein names.
- [[ftp://ftp.jcvi.org/pub/data/TIGRFAMs/][TIGRFAM]] complete listings of functionally defined subfamilies. Database with multiple sequence alignments. To be used with =hmmer=. Use this script =find . -name "TIGR*" -exec cat {} > tigrfam.hmm \;= to create one hmm model. Database is searched with =hmmscan=.
- InterPro database for domains, GO terms, families. Downloading =interproscan= will also acquire hmm databases for =Gene3D= =HAMAP= =PIRSF= =PRINTS= =PRODOM= =PROSITE= =SMART= =SUPERFAMILY= =TIGRFAM=. Download and setup interproscan [[https://code.google.com/p/interproscan/wiki/HowToDownload][here]]. Download the database and GO terms not =interproscan=. 
#+BEGIN_SRC shell
ftp ftp.ebi.ac.uk
cd pub/databases/interpro/Current
get interpro2go
get entry.list
get names.dat 
get short_names.dat
get protein2ipr.dat.gz
#+END_SRC

Interpro scan. =Note= each analyzed sequence generates =SVG= output files. These files are gunzipped together. If protein is being analyzed the peptide sequence must not contain any special characters. Characters are usually due to =stop codons=. If =transeq= is being used to translate nucleotide sequences into peptides, use =-trim= function to replace all =*= with =X=.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=5:ppn=16:native
#PBS -l walltime=01:00:00
#PBS -N interproscan.C
#PBS -e interproscan.C.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

oasis=/oasis/projects/nsf/sun108

${oasis}/silo/interproscan/interproscan.sh \
-t p \
-appl ProDom,PANTHER,TIGRFAM,SUPERFAMILY,PRINTS,Gene3D,PIRSF,Pfam,Coils,SMART \
-i ${oasis}/silo/nodule/peptides/C.peptides.QPXv15.fa \
-iprlookup \
-goterms \
-pa \
-f TSV, SVG, GFF3, XML \
-b ${oasis}/silo/nodule/interpro/C/C.interpro.all
#+END_SRC

| database    | content   | tool | function       | description                |
|-------------+-----------+------+----------------+----------------------------|
| [[http://www.ebi.ac.uk/interpro/release_notes.html][InterPro]]    | nucl/prot | IPS  | classification | integrates 12 databases    |
| [[http://prodom.prabi.fr/prodom/current/html/home.php][ProDom]]      | protein   | IPS  | families       | uniprot domains            |
| [[http://hamap.expasy.org/][PANTHER]]     | protein   | IPS  | classification | domain+pathways            |
| [[http://www.jcvi.org/cgi-bin/tigrfams/index.cgi][TIGRFAM]]     | protein   | IPS  | subfamilies    | classification             |
| [[http://supfam.cs.bris.ac.uk/SUPERFAMILY/][SUPERFAMILY]] | protein   | IPS  | classification | domain+phylog+taxon        |
| [[http://www.bioinf.manchester.ac.uk/dbbrowser/PRINTS/index.php][PRINTS]]      | protein   | IPS  | fingerprinting | conserved motifs SwissProt |
| [[http://pir.georgetown.edu/pirwww/dbinfo/pirsf.shtml][PIRSF]]       | protein   | IPS  | phylogeny      | domain classification      |
| [[http://gene3d.biochem.ucl.ac.uk/Gene3D/][Gene3D]]      | protein   | IPS  | interactions   | domain families            |
| [[http://www.ch.embnet.org/software/COILS_form.html][Coils]]       | protein   | IPS  | domains        | coiled-coil conformation   |
| [[http://pfam.xfam.org/][Pfam]]        | protein   | IPS  | domain         | protein similarities       |
| [[http://smart.embl-heidelberg.de/][SMART]]       | protein   | IPS  | domains        | SwissProt Trembl Ensembl   |
| [[http://prosite.expasy.org/][PROSITE]]     | not used  | IPS  | domains        | +functions                 |
| [[http://hamap.expasy.org/][HAMAP]]       | not used  | IPS  | classification | uniprot classification     |

**** Getting annotation hits from interpro scan
Alignment hits are in a =tsv= output. Described [[https://code.google.com/p/interproscan/wiki/OutputFormats][here]].
1. Protein Accession (e.g. P51587)
2. Sequence MD5 digest (e.g. 14086411a2cdf1c4cba63020e1622579)
3. Sequence Length (e.g. 3418)
4. Analysis (e.g. Pfam / PRINTS / Gene3D)
5. Signature Accession (e.g. PF09103 / G3DSA:2.40.50.140)
6. Signature Description (e.g. BRCA2 repeat profile)
7. Start location
8. Stop location
9. Score - is the e-value of the match reported by member database method (e.g. 3.1E-52)
10. Status - is the status of the match (T: true)
11. Date - is the date of the run
12. (InterPro annotations - accession (e.g. IPR002093) - optional column; only displayed if -iprscan option is switched on)
13. (InterPro annotations - description (e.g. BRCA2 repeat) - optional column; only displayed if -iprscan option is switched on)
14. (GO annotations (e.g. GO:0005515) - optional column; only displayed if --goterms option is switched on)
15. (Pathways annotations (e.g. REACT_71) - optional column; only displayed if --pathways option is switched on)


Check if all hits are annotated.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($10 == "F") print $0 }' | wc -l
#+END_SRC

Get the name of the databases that contain hits. And the total number of unfiltered hits.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ print $4 }' | sort - | uniq -c | sort -n
## output
     14 ProDom
     20 PIRSF
     37 TIGRFAM
    159 SMART
    314 Coils
    391 PRINTS
    783 Pfam
    788 SUPERFAMILY
    874 Gene3D
   1190 PANTHER
#+END_SRC

Get the number of hits per database at different e-values. Although the number of hits is filtered by evalue, it is not filtered by unique sequence entries. For example, a single contig translated in 6 different frames might be matched to 2 different domains because of 2 separate frames shifts.
#+BEGIN_SRC shell
## some databases dont include description of the accession number
## accession numbers are registered under columns $8 or $9
## so we must filter the $9 and $8 by evalue.
## $4 is correct for all
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($9<=.0000000001) print $4}' | sort - | uniq -c | sort -n
## and
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($8 <= .0000000001) print $4}' | sort - | uniq -c | sort -n

#+END_SRC

In interpro output 5 databases have the full number of columns (shown above) and 5 others dont. filtering should be separated if the options depend on the columns that come after the 4th.
Create a list for each set of database.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($8 <= .0000000001) print $4}' | sort - | uniq > db.without.acc.txt 

# AND
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($9 <= .0000000001) print $4}' | sort - | uniq > db.with.acc.txt
#+END_SRC

Use these lists to filter separately the contigs by evalue and the sequence length of alignment. =hint= the calculated =x= returns an absolute value of the equation =end position - start - position=. Negative numbers might occur if the alignment is on the opposite strand.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | grep -Fwf ./db.without.acc.txt - | awk '{if($8 <= 0.00000000000000001) print $0}' | awk '{x=$6-$7?$7-$6:$6-$7; if(x>=10) print $4 }' | sort - | uniq -c | sort -n

#AND 
cat A.interpro.all.tsv | sed 's/ /./g' | grep -Fwf ./db.with.acc.txt - | awk '{if($9 <= 0.00000000000000000001) print $0}' | awk '{x=$7-$8?$8-$7:$7-$8; if(x>=20) print $4 }' | sort - | uniq -c | sort -n
#+END_SRC

**** GO annotation of orthologs
Get interpro data. They contain protein entries from over 10 public protein databases. Get all flat files from [[http://www.ebi.ac.uk/interpro/download.html%3Bjsessionid%3DD8833CB9AF91969493F14D7056B31083][here]].
Interpro dump files contain Protein names, interpro accession numbers and GO terms. So by searching for protein names then looking by interpro accession number we can get GO terms for every protein entry.
#+BEGIN_SRC shell
# get protein names from pfam.SNP.all.txt annotated SNPs
# next get interpro accession numbers
time zcat protein2ipr.dat.gz | grep -Fwf hotspots.interpro.go/1.pfam.tags.all.snps.txt - | sort - | uniq | awk '{print $2}' | hotspots.interpro.go/2.pfam.interpro.acc.txt
#+END_SRC


Get GO terms using interpro accession keys for orthologs shared between strains.
#+BEGIN_SRC shell
time cat interpro2go | grep -Fwf hotspots.interpro.go/2.pfam.interpro.acc.txt - | sort - > hotspots.interpro.go/3.pfam.go.txt 
#+END_SRC

Get a numbered list of GO terms.
#+BEGIN_SRC shell
## How many GO terms in total
cat 3.pfam.go.txt | sed 's/^.*>.GO:/GO:/g' | sed 's/;.*//g' | sort - | uniq -c | sort -nr | awk '{s+=$1; print s}' | tail -n 1

## Show numerically sorted list
cat 3.pfam.go.txt | sed 's/^.*>.GO:/GO:/g' | sed 's/;.*//g' | sort - | uniq -c | sort -nr | less
#+END_SRC


** Phase 5: Relational database (in progress)
*** Create a database for structured data
Ideas from [[http://sfg.stanford.edu/BLAST.html][here]] and [[https://trinotate.github.io/][here]]
** Phase 6: Genome annotation (in progress)
*** Genome annotation
*** SNP annotation
Annotation of variants is done with =SnpEff=. SNP annotation will help get the percentage of synonymous SNPs and non-synonymous ones, which helps estimates the pressure of purifying selection. (related [[http://beta.bmcgenomics.com/articles/10.1186/1471-2164-13-545][article]])
#+BEGIN_SRC shell
wget http://sourceforge.net/projects/snpeff/files/snpEff_latest_core.zip
7z x snpeff_latest_core.zip
#+END_SRC

Download a reference genome to annotate SNPs. Library contains over 2500 genomes. QPX is not among them. Therefore a custom genome must be constructed. Add the following lines in the =configuration= file.
#+BEGIN_SRC shell
# QPX genome S. Roberts v15
qpx.genome : QPX
#+END_SRC

A =GTF3= or =GFF3= or a =GenBank= genomic annotation file must be included in =snpeff= custom genome construction. [[http://snpeff.sourceforge.net/SnpEff_manual.html#databases][Tutorial]]

Create a =gff3= file from genomic fasta file.
#+BEGIN_SRC shell
gmod_fasta2gff3.pl --fasta_dir QPX_Genome_v017.fasta --gfffilename QPX_Genome_v017.gff3 --type CDS --nosequence
#+END_SRC
The gff3 file can be used to create a database for =snpEff=. The fasta sequences can be added to the end of the gff3.
#+BEGIN_SRC shell
cd /path/to/snpEff
mkdir -p data/qpx
mv QPX.gff3 snpEff/data/qpx/genes.gff
echo "##FASTA" >> genes.gff 
cat QPX_genome.fasta >> genes.gff 
#+END_SRC

Create the database (Fast). Warning may appear, but this does not mean the database is not created.
#+BEGIN_SRC shell
java -jar snpEff.jar build -gff3 qpx
#+END_SRC

Check the database. It contains several statistical summaries, the list of genes (...), and a section for exon|CDS|Protein.
#+BEGIN_SRC shell
java -jar snpEff.jar dump qpx | less
#+END_SRC

Annotate and generate an =HTML= summary.
#+BEGIN_SRC shell
#! /usr/bin/bash

sample[1]=mmetsp0098
sample[2]=mmetsp00992
sample[3]=mmetsp001002
sample[4]=mmetsp001433

dir=/home/neo/data/analysis/strains

for i in 1 2 3 4
do
sample=${sample[${i}]}

java -Xmx4g \
-jar snpEff.jar \
-stats ${dir}/annotateSNPEFF/${sample}.html \
qpx \
${dir}/vcf/${sample}.vcf.gz > ${dir}/annotateSNPEFF/${sample}.ann.vcf.gz
done

#+END_SRC

** Packages and alternatives
I need applications that load rna-seq data. 

1. [[https://trinityrnaseq.github.io/][Trinity]]
2. Bowtie [[http://tophat.cbcb.umd.edu/][TopHat]] [[http://cufflinks.cbcb.umd.edu/][Cufflinks]] Cuffdiff (Align Annotate Compare DEG for sequenced genome)
3. Trinity TransAbyss [[http://www.ebi.ac.uk/~zerbino/oases/][Oases]] (gene discovery on de novo transcriptome)
4. RSEM IsoEM (DEF on de novo transcriptome)
5. R/Bioconductor packages (ShortRead edgeR DESeq GenomicRanges GenomicFeatures DEXSeq BaySeq BBSeq NOISeq QuasiSeq BitSeq ShrinkSeq)
6. SAMtools [[http://www.htslib.org/doc/samtools.html][HomePage]] [[http://www.htslib.org/doc/samtools.html][Documentation]]
7. Count map reads [[http://www-huber.embl.de/users/anders/HTSeq/doc/overview.html][HTSeq]]
8. [[http://genome.sph.umich.edu/wiki/Bam_read_count#Download][ReadCount]] reads in a bam file
9. MEGA-CC (command line) and MEGA-PRO (GUI) -- [[http://megasoftware.net/MEGA7-CC-Quick-Start-Tutorial.pdf][Intro]]
10. BLASTX =command line=
11. CLC Genomics =Xeon Desktop= [[http://www.clcsupport.com/clcgenomicsworkbench/current/index.php?manual%3DIntroduction_CLC_Genomics_Workbench.html][tutorial]]
12. MG-RAST [[http://metagenomics.anl.gov/metagenomics.cgi?page%3DHome][homepage]]
13. [[http://bioconductor.org/][Bioconductor]]
14. Gene Ontology Packages: GOseq topGO GOstat Ontologizer DAVID ontoCAT([[http://www.ontocat.org/browser/trunk/ontoCAT/src/uk/ac/ebi/ontocat/examples/R/Example1.R][help]]) biomaRT
15. Draft genome assemblers: Velvet Oasis ABySS SOAPdenovo (reads to contigs) or hybrid methods: celera MIRA ALLPATHS-LG
16. Genome Finishing Tools: AHA SSPACE-LongRead (reduce contiguous assembled pair-ends) =scaffolding routine= [[http://www.biomedcentral.com/1471-2105/15/211][Boetzer 2014]]
17. Proteogenomics (w/ circos and D3) [[http://qcmg.org/bioinformatics/PGTools][PGTools]]
18. Integrated Genome Viewer [[https://www.broadinstitute.org/igv/][BroadInstitute]] [[http://www.broadinstitute.org/igv/projects/downloads/IGV_2.3.46.zip][IGV]]
19. Aliners of reads to transriptome (tophat2 GSNAP SpliceMap Subread STAR)
20. Burrows-Wheeler Aligner (BWA) [[http://sourceforge.net/projects/bio-bwa/files/][sourceforge.net]] or on [[https://github.com/lh3/bwa][github]]
21. MAUVE [[http://genome.cshlp.org/content/14/7/1394.short][paper]] for genomic alignment & identification of recombination and horizontal transfer.
22. QIIME for bacterial rna seq data processing [[http://qiime.org/][main site]]
23. [[http://www.scfbm.org/content/10/1/8/abstract][PrimerView]] CPAN module for primer design on multiple sequences
24. R and [[https://github.com/qinwf/awesome-R][awesomeR]] (summary of best used packages)
25. Bpipe [[http://docs.bpipe.org/][documentation]]
26. Emboss package for nucleotide seq analysis [[http://emboss.sourceforge.net/download/][(includes Transeq)]] for contig translation. Package includes [[http://emboss.sourceforge.net/apps/][a list of applications]]
27. Reverse translate aligned peptides into DNA sequences [[http://www.cbs.dtu.dk/services/RevTrans-2.0/web/download.php][RevTrans]]
28. Axel and aria2 alternatives for wget and curl [[http://www.cyberciti.biz/tips/download-accelerator-for-linux-command-line-tools.html][here]]
29. Parallel computing in GNU [[http://figshare.com/articles/GNU_parallel_for_Bioinformatics_my_notebook/822138][here]]

#+BEGIN_SRC shell
sudo apt-get install libx11-dev libpulse-dev libxcomposite-dev \
libxinerama-dev libv4l-dev libudev-dev libfreetype6-dev \
libfontconfig-dev libx264-dev \
libxcb-xinerama0-dev libxcb-shm0-dev libjack-jackd2-dev

libdbus-1-dev libglib2.0-dev libavahi-client-dev libxcb-xinerama0

libgl1-mesa-dev-lts-trusty
#+END_SRC

** Computational server
*** SDSC Gordon on XSEDE
Login and connect through secure network. To access the list of available software go [[https://portal.xsede.org/software#/][here]].
We access to blacklight, comet, gordon, oasis, and supercell.
#+BEGIN_SRC shell
ssh -l silo gordon.sdsc.xsede.org
#+END_SRC

Shared directory with bassem. 3T of free space
#+BEGIN_SRC shell
cd /oasis/project/nsf/sun108
#+END_SRC

Transfer files (upload)
#+BEGIN_SRC shell
scp file1 silo@gordon.sdsc.xsede.org:~/
scp -r folder ...
scp -C file # compress for fast transfer
#+END_SRC

Download files. (no need to create the destination folder)
#+BEGIN_SRC shell
rsync -auv bassem@gordon.sdsc.xsede.org:~/folder/ ./destination
#+END_SRC

Show remaining allocations and accounts. On SDSC 1 compute node for 1 hour = 16 SU (service unit) = 60 Gb ram = 16 cores. [[https://portal.xsede.org/sdsc-gordon#modules][Visit here]] for more modules and compiling instructions.
#+BEGIN_SRC shell
xdusage -r gordon
xdusage -up silo
show_accounts
#+END_SRC

Load modules. Packages that are installed.
#+BEGIN_SRC shell
module avail
module load R
module unload R
#+END_SRC

Create TORQUE batch file. 
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=1:ppn=16:native
#PBS -l walltime=1:00:00
#PBS -N makeblastdb
#PBS -o silo.out
#PBS -e silo.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

export PATH="$PATH:/home/bassem/blast/bin"
oasis=/oasis/projects/nsf/sun108
makeblastdb -in ${oasis}/bassem/db/nt/nt.fasta -out ${oasis}/bassem/db/nt/nt -dbtype nucl -parse_seqids
#+END_SRC

Monitor jobs. =qdel= to delete a running job with the job ID number.
#+BEGIN_SRC shell
qstat -a -u silo
qstat -f <job id>
#+END_SRC

Status of a job.
#+BEGIN_SRC shell
R = running
Q = queued
H = held
C = completed after having run
E = exiting after having run
#+END_SRC

Alter job properties. =important= One can reduce time remaining but not increase it.
#+BEGIN_SRC shell
qstat -a <job id>
qalter -l walltime=9:00 <job id>
qstat -a silo
#+END_SRC

Obtaining queue properties of a job.
#+BEGIN_SRC shell
qstat -q
#+END_SRC


*** Analysis
Data are stored in :
#+BEGIN_SRC shell
cd /oasis/projects/nsf/sun108/silo
#+END_SRC

Blastx on =NR= database (updated on July 2015). =important= When changing from nucleotide to peptide blast search the BLASTDB must be change too. The alternative is to merge all database files into one directory.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=10:ppn=16:native
#PBS -l walltime=48:00:00
#PBS -N blastx.A
#PBS -o blastxA.out
#PBS -e blastxA.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

export PATH="$PATH:/home/silo/blast/bin"
export BLASTDB="/oasis/projects/nsf/sun108/bassem/db/nr"
oasis=/oasis/projects/nsf/sun108

blastx -query ${oasis}/silo/nodule/assembled/A.assembl.QPXgv15.fasta \
-db nr \
-outfmt " 7 qseqid qlen sseqid slen qstart qend sstart send evalue bitscore length pident nident mismatch gaps staxids sscinames " \
-max_target_seqs 10 \
-out A.blastx.txt
#+END_SRC
** Bibliography
*** First set
1. New tool in machine learning that finds splice junctions related to autism [[http://www.sciencemag.org/content/early/2014/12/17/science.1254806.short][Xiong 2014]] =science=
2. Difference in genome annotation (RefSeq, UCSC, Ensembl) is responsible for differences in read mapping to genes and transcription quantification [[http://www.biomedcentral.com/1471-2164/16/97][Zhao 2015]] =gene model=
3. Non-parametric approach to detect DETs from rnaseq data [[http://bioinformatics.oxfordjournals.org/content/early/2015/02/24/bioinformatics.btv119.abstract][Shi 2015]] =r friendly=
4. Co-expression analysis require high number of samples [[http://bioinformatics.oxfordjournals.org/content/early/2015/02/24/bioinformatics.btv118.full.pdf%2Bhtml][Ballouz 2015]] =metanalysis networks=
5. Co-expression and network construction from rnaseq data [[http://bioinformatics.oxfordjournals.org/content/28/12/1592.short][Iancu 2012]]
6. Multifunctionality is better than association for network inference [[http://journals.plos.org/plosone/article?id%3D10.1371/journal.pone.0017258][Gillis 2011]] =Pavlidis amd machine learning + pleiotropy=
7. SimSeq non parametric simulation engine for real rnaseq data [[http://bioinformatics.oxfordjournals.org/content/early/2015/02/26/bioinformatics.btv124.abstract][Benidt 2015]]
8. Overlapping genes and analysis of rnaseq data [[http://www.biomedcentral.com/1471-2105/16/S1/S3][Sun 2015]]
9. Phylogenetic analysis of the marine microbial transcriptome [[http://journals.plos.org/plosbiology/article?id%3D10.1371/journal.pbio.1001889][Keeling 2014]] =metagenomics=
10. Detect rna editing events fron rnaseq data [[http://onlinelibrary.wiley.com/doi/10.1002/0471250953.bi1212s49/abstract][Picardi 2015]] =python=
11. Orthologs from related species w/ rnaseq data [[http://www.biomedcentral.com/1471-2164/15/343?utm_source%3Ddlvr.it&utm_medium%3Dtumblr][Zhu 2014]] =vertebrates=
12. Orthologs from rnaseq expression data clustering analysis [[http://www.biomedcentral.com/content/pdf/gb-2014-15-8-r100.pdf][Yan 2014]] =networks=
13. Analysis of rnaseq expression data in Nature Protocols w/ R [[http://www.nature.com/nprot/journal/v8/n9/abs/nprot.2013.099.html][Anders 2013]] and [[http://link.springer.com/protocol/10.1007/978-1-4939-2444-8_24][Loraine 2015]] [[http://www.nature.com/nprot/journal/v7/n3/full/nprot.2012.016.html#ref12][Trapnell 2012]]=protocol=
14. edgeR paper [[http://bioinformatics.oxfordjournals.org/content/26/1/139.short][Robinson 2009]] =R=
15. Comparative paper of rnaseq packages [[http://www.nature.com/nmeth/journal/v8/n6/abs/nmeth.1613.html][Garber 2011]] =tools=
16. Machine learning for predicting gene expression from epigenetic data [[http://lungcancernewstoday.com/2015/03/23/new-prediction-model-for-gene-expression-in-lung-cancer-based-on-epigenetics/][Li 2015]]
17. Look for dsRNAs from rnaseq data after genome alignment [[http://rnajournal.cshlp.org/content/early/2015/03/24/rna.048801.114.full.pdf%2Bhtml][Whipple 2015]]
18. Gene expression of virulence, metabolism, and growth of QPX are temperature dependent [[http://journals.plos.org/plosone/article?id%3D10.1371/journal.pone.0074196][Vedrenne 2013]] =bad paper=
19. Retrotransposons as effectors and transmittors of immune cancer cells in clam [[http://www.sciencemag.org/content/348/6231/170.full][Metzger 2015]]
20. 

*** Second set
:PROPERTIES:
:ID:       624baea5-62b1-40b1-813f-8f7350966d50
:END:
1. How to characterize SNPs affected by the reference bias? Align reads to personalized genomes [[http://journals.plos.org/plosone/article?id%3D10.1371/journal.pone.0126911][Wood 2015]] =also ref. 26 and 28 inside=
2. Genome and transcriptome sequencing of single cell [[http://www.nature.com/nmeth/journal/v12/n6/full/nmeth.3370.html][Macaulay 2015]]
3. the next 20 years in genome research [[http://biorxiv.org/content/early/2015/06/02/020289.large.jpg?rss%3D1][Schatz 2015]]
4. Basic strategy on annotating a genome [[http://www.nature.com/nrg/journal/v13/n5/full/nrg3174.html#B22][Yandall 2012]] =review=
5. Terraformation of mars: importance of genome annotation and visualization [[http://motherboard.vice.com/read/darpa-we-are-engineering-the-organisms-that-will-terraform-mars][Jacksons lab]] =DARPA are engineering organisms=
6. Reference transcriptome and database used for gene annotation both influence variant caling [[http://www.biomedcentral.com/1471-2164/16/S8/S2][Franckish 2015]]
7. Cross sample contamination, viral, and pathogenic database contamination are real threat to sequencing data analysis [[http://jvi.asm.org/content/early/2015/06/11/JVI.00822-15.abstract][Kazemian 2015]]
8. 5-formylCytosine a DNA modified sugar that regulates genes [[http://www.nature.com/nchembio/journal/vaop/ncurrent/full/nchembio.1848.html][Backman 2015]]
9. Classification of reads between parasite and host [[http://www.plantmethods.com/content/11/1/34][Ikeue 2015]] =plant=
10. Finding parasitic genes [[http://www.plantphysiol.org/content/166/3/1186.long][Ranjan 2014]] =plant=
11. 2 SNPs linked to depression [[http://www.nature.com/nature/journal/vaop/ncurrent/full/nature14659.html#affil-auth][Converge consortium 2015]] =Nature=
12. Comparison of interface-built pipelines for rna-seq data [[http://bib.oxfordjournals.org/content/early/2015/06/23/bib.bbv036.short][Poplawski 2015]] =review=
13. Gene expression quantification by LFC [[http://nar.oxfordjournals.org/content/early/2015/07/08/nar.gkv696.short][Erhard 2015]] =estimate fold change=
14. Transcript quantification, new fast pipeline [[http://www.biorxiv.org/content/early/2015/06/27/021592.abstract][Patro 2015]] =gene expression=
15. The need to sequence C. virginica genome [[http://www.sciencedirect.com/science/article/pii/S1050464815002211][Gomez 2015]] =review=
16. Crosstalk between snail and parasite [[http://www.sciencedirect.com/science/article/pii/S1050464815000509][Coustau 2015]] =review=
17. How to recognize host-pathogen mechanisms [[http://ac.els-cdn.com/S0166685109000267/1-s2.0-S0166685109000267-main.pdf?_tid%3D58e521fa-2ef4-11e5-9802-00000aacb35d&acdnat%3D1437406450_c52e14fbc087a1152765fa0696a28730][Bayne 2009]] =review=
18. FPKM (fragments per 1kb per million reads) vs TPM (transcripts per million) [[https://liorpachter.wordpress.com/2014/04/30/estimating-number-of-(transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/][here]] and [[http://www.biomedcentral.com/1471-2105/12/323/][Li 2011]] =transcript quantification= (FPKM = depth of coverage + sum length of contigs, TPM = sum length of contigs + depth of coverage).
19. Identified molecular involvement host-pathogen [[http://www.sciencedirect.com/science/article/pii/S1050464815002429][He 2015]] =virus-oyster=
20. Normalization of rna-seq samples [[http://www.hindawi.com/journals/bmri/2015/621690/][Walczak 2015]] =review=
21. Batch effect on rnaseq data and how to normalize [[http://nar.oxfordjournals.org/content/early/2015/07/21/nar.gkv736.long][Peixoto 2015]] =RUVseq in R=
*** Generalities
Lectins
1. Interaction with the complement
2. Key role in innate immune defense
3. Central role in filter feeding processes
4. Association with neurone morphology
5. Reduce functionality or absence cause diesease

Transposons
miRNAs
Virus

* Brain diseases and human sciences
** INTRODUCTION :noexport:
I need to know where I'm headed before starting. What is the purpose of the
study? What the objectives are? Define the scoop? What are the requirements that
I should start with? The project plan should be easy, significant, interesting
but not essentially special, it should be reasonable. I should not waist time
making my project plan perfect. Finally someone should be able to read it and
understand what I'm trying to accomplish.

The subject is substance dependencies. Hypothesis 1 is that addiction is genetic
with 75 % being hereditary.

I've got 6000 samples assembled with GWAS data and physiological data and an
other batch of 6000 data only assembled with physiological data. There is also 2
populations either african or european. 

I can either infer associations between the different physiological variables, do
a meta-analysis summary of all populations and diseases or investigate the
pleitrophic effect of genes. The latter is done under the assumption that these
genes contain at least one SNP.

The analysis will start with an unsupervised learning protocol to cluster
different recurrent patterns in the data. The data consists of
the GWAS dataset. This dataset contains clinical information and imputed rare
SNPs. First exonic SNPs can be up-weighted and the data transposed. GWAS data
can be used only with the descriptive clinical columns. Which means only the
phenotypic data with a disease/or-not phenotypic variation. Through this approach the analysis
will be fast, especially since the number of rows is relative to the number of
SNPs to be analyzed. 

Classification of the GWAS data can assume different weighting of regulatory
regions (splice sites, transcription factor binding sites, promoters,
enhancers/silencers), non-coding regions (intergenic, upstream/downstream,
3'UTR/5'UTR), exonic coding regions (stop loss. stop gain, missense,
frameshift). In addition GWAS related to mental illness can also be used to
classify the exonic SNPs.

Clustering can be from hierarchical or K-means and principal components, each
one used in unsupervised learning.

What are the significance of the results and their interest? First, after
categorizing through clustering of the sampled data, shrinkage is used to
eliminate irrelevant phenotypic (physiological and environmental factors)
features and reduce noise. Every cluster will then be defined by a number of
features less than the initial number used during supervised clustering.

For annotation purposes of SNPs i might find ANOVAR or ENSEMBL rich databases
for SNP classification and for mental disease data integration.

Maybe i can integrate a population structuring after clustering.

** PROJECTS :noexport:
Before starting to build an approach it is best to consider the GOAL of the
study, the HYPOTHESIS and its SIGNIFICANCE, the INNOVATION of the APPROACH, and

1. Col25A1 and comorbid substance dependence
2. Identify disease genes following the concept of common disease, unique variants
3. SNPs that can cause a disease in a population but also prevent another
4. Association between immune system and mental illness
5. New method for the functional analysis of variants associated with mental disorder
6. Unsupervised machine learning in childhood behavior for multiclass categorical data
7. Meta analysis and comorbid substance dependence
8. Full characterization of all genetic variants (statistical analysis of gVCF data)
9. Estimate the total number of disease genes (SNP simulation)
10. Predict how much heritability each SNP can have on a disease in a population
11. Group categorical data by sparse and ridged group lasso for personalized modelling
12. Combine genetic diseases related to mental illness while removing control for polygenic predictive analysis
13. Network analysis for pleiotropy to combine information from GWAS data, pathways

** Visualization
1. 36 best tools for data visualization [[http://www.creativebloq.com/design-tools/data-visualization-712402][CreativeBlog]]
2. Shiny for R interactive visualization [[http://shiny.rstudio.com/tutorial/][rStudio]] =tutorial=
3. Plotly for ggplot2 within knitr and markdown [[http://www.r-bloggers.com/plot-with-ggplot2-and-plotly-within-knitr-reports/][r-bloggers]]
4. ggViz an R package for interactive visualization [[http://ggvis.rstudio.com/][rStudio]]
5. d3Network an R package for JSON data [[http://christophergandrud.github.io/d3Network/][github]]
6. Data-projector for JSON data (PCA/SVD based projection) [[http://opensource.datacratic.com/data-projector/][datacratic]]
7. Wordle to create a weighed word cloud [[http://www.wordle.net/advanced][wordle]]
8. circos in Perl and as a webtool [[http://circos.ca/][circos]]
9. Hive plots in R Perl or Julia
10. Visual survey of text visualization techniques [[http://textvis.lnu.se/][textvis]]
11. Network data integration [[http://www.cytoscape.org/][Cytoscape]] and [[http://cytoscapeweb.cytoscape.org/][Cytoscape web]]
12. Data-driven documents in javascript [[http://d3js.org/][d3]]
13. Database of miscellaneous data [[http://www.data.gov/][Data.org]]

** Sequence and statistical tools
1. Variant association tool [[http://varianttools.sourceforge.net/Association/HomePage][VAT]]
2. High performance genomic feature operations [[https://github.com/bedops/bedops][BEDOPS]]
3. Haplotype analysis [[http://www.broadinstitute.org/scientific-community/science/programs/medical-and-population-genetics/haploview/haploview][Haploview]]
4. Shaun Purcells update of Plink for standard regression [[http://pngu.mgh.harvard.edu/~purcell/plink2/][version 2]] and [[http://pngu.mgh.harvard.edu/~purcell/plink/tutorial.shtml][version 1]]
5. Random forest is implemented in [[https://github.com/liamgriffiths/random-jungle][Random Jungle]]
6. Bayesian Partitioning for epistasis association mapping [[http://www.nature.com/ng/journal/v39/n9/full/ng2110.html][BEAM]]
7. Bioconductor Manual for R [[http://manuals.bioinformatics.ucr.edu/home/R_BioCondManual#TOC-Factors][UCR]]
8. Vectorization techniques in R [[http://www.burns-stat.com/pages/Tutor/R_inferno.pdf][R inferno]]
9. Openhelix tutorials for the Genome browser of [[http://www.openhelix.eu/cgi/freeTutorials.cgi][UCSC]]
10. AnoVAR
11. EnsEMBL
12. Platform to communicate and track research [[https://www.synapse.org/#][Synapse]]
13. Locating ancestry based on SNPs [[http://genome.sph.umich.edu/wiki/LASER][LASER]]
14. Simulating SNPs [[http://ccega.renci.org:8080/ccega_simulator/simulate][HapSample]] cited here [[http://archpsyc.jamanetwork.com/article.aspx?articleID%3D1859133][Nurnberger 2014]]

** Bibliography
*** Assessment of GWAS papers
1. Quality control GWAS [[http://www.nature.com/nprot/journal/v5/n9/pdf/nprot.2010.116.pdf][Anderson 2010]] =tutorial=
2. Basic analysis for GWAS [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3154648/][Clarke 2011]] =definitions= and =tutorial=
3. How to interpret GWAS [[http://jama.jamanetwork.com/article.aspx?articleid%3D181647][Pearson 2008]] =definitions=
4. Looking for common and rare variants in GWAS [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3198013/][Raychaudhuri 2012]] =rare SNPs=
5. Statistical methods of GWAS [[http://ac.els-cdn.com/S0002929709005321/1-s2.0-S0002929709005321-main.pdf?_tid%3D56734ab4-8d0a-11e4-af46-00000aacb35e&acdnat%3D1419603806_94dae954e721f90b33b8f81bff383fd8][Canton 2009]]
6. Population study for GWAS [[http://www.nature.com/nrg/journal/v7/n10/pdf/nrg1916.pdf][Balding 2006]]
7. Standardization and variant reporting in GWAS [[https://www.acmg.net/StaticContent/SGs/ACMG_recommendations_for_standards_for.9.pdf][Richards 2008]]
8. Future of GWAS [[http://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.012809.103723][Witte 2010]]
9. Guidelines for investigating causality in GWAS [[http://www.nature.com/nature/journal/v508/n7497/pdf/nature13127.pdf][McArthur 2014]]
10. Computational challenges for GWAS [[http://bioinformatics.oxfordjournals.org/content/early/2010/01/06/bioinformatics.btp713.full.pdf%2Bhtml][Moore 2010]]
11. Basic concepts of GWAS from disease study perspective [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2740629/?tool%3Dpubmed][Ding 2009]]
12. Importance of GWAS [[http://www.nejm.org/doi/pdf/10.1056/NEJMra0905980][Manolio 2010]]

*** High-dimensional categorical data with machine learning
1. Different loci with shared association with 5 mental illness diseases in mega-analysis GWAS [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3714010/pdf/nihms-470697.pdf][Smoller 2013]]
2. Fixation index and association of common variant with disease and offers evolutionary benefit simultaneously in GWAS [[http://www.sciencemag.org/content/suppl/2010/07/14/science.1193032.DC1/Genovese.SOM.pdf][Genovese 2010]] =methodology=
3. Classical GWAS with a nice story and attractive abstract [[http://www.nature.com/nature/journal/v461/n7262//full/nature08309.html#B16][Ge 2009]] =abstract= and =methodology=
4. Genetic networks and Bayesian gene-based likelihoods [[http://www.nature.com/nature/journal/v515/n7526/pdf/nature13772.pdf][Rubeis 2014]] =methodology=
5. Random forest is misleading when used with mixed data as in continuous and categorical for feature extraction [[http://www.biomedcentral.com/1471-2105/8/25][Strobl 2007]]
6. Multifactorial analysis and interaction terms review [[http://www.nature.com/nmeth/journal/v11/n12/pdf/nmeth.3180.pdf][Kyrziwinski 2014]]
7. Ridge regression better than repeated simple regression of GWAS [[http://downloads.hindawi.com/journals/bmri/aip/143712.pdf][Vlaming 2014]]
8. Comparative analysis of predictive accuracies for lasso and RR [[http://www.biomedcentral.com/1753-6561/6/S2/S10][Ogutu 2012]]
9. Twin study for individual genetic risk assessment for 24 diseases [[http://stm.sciencemag.org/content/4/133/133ra58.short][Roberts 2012]]
10. Solution for sparsity with lasso [[http://www.jstor.org/discover/10.2307/25464748?sid%3D21105531960423&uid%3D2&uid%3D4&uid%3D3739808&uid%3D3739256][Meinshausen 2009]]
11. Adaptive Lasso for sparse high dimensional data [[http://webdocs.cs.ualberta.ca/~mahdavif/ReadingGroup/Papers/tr374.pdf][Huang 2006]]
12. Lasso can account for bias and sparsity in data [[http://www.jstor.org/stable/25464684][Zhang 2008]]
13. Generalized linear mixed model with lasso and R package [[http://www.tandfonline.com/doi/abs/10.1080/10618600.2013.773239][Schelldorfer 2014]]
14. Gradient lasso to predict traits using SNPs [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3651372/][Kim 2013]]
15. Jointly analyzing dense markers, phenotypes, and pedigree with bayesian Lasso in R [[http://www.genetics.org/content/182/1/375.full.pdf%2Bhtml][Campos 2009]]
16. Deep unsupervised learning in shell [[http://fastml.com/deep-learning-made-easy/][Ngiam 2013]]
17. Bayesian lasso to predict phenotype based on SNPs while specific organ gene expressions increase accuracy [[http://www.plosone.org/article/info:doi/10.1371/journal.pone.0115532#s2][Takagi 2014]] =methods= for Gibbs sampling
18. C score and the deleteriousness of a variant [[http://www.nature.com/ng/journal/v46/n3/pdf/ng.2892.pdf][Witten 2014]] =databases=
19. New tool in machine learning that finds splice junctions related to autism [[http://www.sciencemag.org/content/early/2014/12/17/science.1254806.short][Xiong 2014]]
20. Overestimating the missing heritability and the interactive variant model [[http://www.pnas.org/content/109/4/1193.abstract][Zuk & Lander 2011]]
21. VAAST for prioritizing variants and identifying disease genes [[http://www.ncbi.nlm.nih.gov/pubmed/23836555][Hu 2013]]
22. Survey of tools: QC, alignment, identification, annotation, visualization  [[http://bib.oxfordjournals.org/content/15/2/256.full.pdf%2Bhtml][PAbinger 2013]]
23. SVM for prediction of type I diabetes [[http://www.plosgenetics.org/article/info:doi/10.1371/journal.pgen.1000678#pgen-1000678-g003][Wei 2009]]
24. Review and summary of many machine learning research papers [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3432206/][Kruppa 2012]]
25. Bagging with ML models for multicategorical outcomes [[http://onlinelibrary.wiley.com/doi/10.1002/bimj.201300068/full][Kruppa 2014]] =theory=
26. Probability estimation of multicategorical prediction ML [[http://onlinelibrary.wiley.com/doi/10.1002/bimj.201300077/abstract][Kruppa 2014]] =application=
27. Use network topology to group SNP before ML [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3606427/#B18][Okser 2013]] =review=
28. Group SNPs by gene-gene interactions [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3748153/][Mooney 2013]] =methodology=
29. Sparse group lasso and multicategorical data [[http://arxiv.org/pdf/1205.1245.pdf][Vincent 2014]] =methodology=
30. Higher number of SNPs for sparse prediction analysis can improve accuracy in risk estimation [[http://onlinelibrary.wiley.com/doi/10.1002/gepi.20509/full][Kooperberg 2010]]
31. Controls can become cases if our criteria of classifying a disease trait changes [[http://www.pnas.org/content/early/2014/12/25/1411893111.short][Rosenquist 2014]]
32. Rare variants and quality control review [[http://www.sciencedirect.com/science/article/pii/S0002929714002717][Lee 2014]]
33. Pitfalls and limitations in SNP prediction studies [[http://www.nature.com/nrg/journal/v14/n7/pdf/nrg3457.pdf][Wray 2013]]
34. How to increase the risk score of SNPs by polygenic analysis [[http://www.nature.com/ng/journal/v44/n5/full/ng.2232.html#supplementary-information][Stahl 2012]] =methods= for MCMC
35. Using cases for genetically correlated diseases increase accuracy [[http://link.springer.com/article/10.1007/s00439-013-1401-5/fulltext.html][Li 2014]]
36. Augmenting sample size for GWAS [[http://www.nature.com/ng/journal/v45/n11/full/ng.2758.html#methods][Zhan 2013]]
37. A network for the human diseases [[http://www.pnas.org/content/104/21/8685.full][Goh 2007]]
38. How to proceed in the post-GWAS era, Bayesian hierarchical, epistasis, & pathways, and dis/advantages of regression rules [[http://www.sciencedirect.com/science/article/pii/S0002929709005321][Cantor 2010]]
39. The selfish ribosomes might be the reason DNA replicates [[http://www.sciencedirect.com/science/article/pii/S0022519314006778][Bernstein 2014]]
** Quality control checks for GWAS
1) =Quality control= Genotype call rates (remove high error call rates <95%) and HWE. Manhattan plot to confirm homogeneity of calls ([[http://scienceblogs.com/geneticfuture/2010/07/07/serious-potential-flaws-in-lon/#more][web article]])
   - Removing genotypes with low call certainety introduce information missingness where low occuring alleles (rare homogenous) have low probabilities which reduces the correct allele frequencies
   - Population stratification introduce a variance in allele frequency due to ancestry not to case-control status
   - Sample heterozygosity outiliers (remove less than 5)
   - A handfull of samples with high error rates can be removed to increase power (the per-individual approach)
   - A certain percentage of makers can be removed to improve association [[http://www.nature.com/nprot/journal/v5/n9/pdf/nprot.2010.116.pdf][Anderson 2010]]
   - Removing 1 marker is better than removing one sample (makers can be imputed back in the analysis)
   - Calculate homozygosity rates between all X chromosome SNPs for each sample and comapre these with the expected rate
   - Calculate the maximum relatedness between pair samples (it should be less than the second degree relative). Identity by state analysis on independent SNPs, ie, hogh LD SNPs eg, in the HLA region, are removed (IBS=1 are removed)
   - Calculate for shared ancestry with the identity by decent IBD from the IBS (IBD>.1875 are removed)
   - HWE are calculated on controls only, so not to remove disease associated loci
   - Principal components and MDS can be used to adjust for population stratification
   - Population stratification inflates the variance and increase median
2) =Sample size= Low sample size means high variation.
   - Study samples originate from outbred population and unrelated individuals
   - Measure of the relative risk to identify the model of association of an allele (multiplicative, additive, AA, aa) in prospective studies (longitudinal) [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3154648/][Clarke 2011]]
   - Odds ratio of a disease associated with a risk variant can measure the strength of the association. odds are usually modest 1.2-1.3 
   - A chi squared test of association between rows and columns in a classical 2x3 contingency table with 2 degrees of freedom
   - Likelihood ratios can be used for test of association
   - In large smples the chi squared and the likelihod ratios are similar for test association.
   - Logistic regression for association testing is used when more covariates are added
   - Correction for mulptiple testing of the type one error of rejecting the null hypothesis and reducing FDR but decreasing power of detecting causality. bonferroni is conservative and assumes that variants are independently associated with the disease without acknowledging interaction between SNPs in LD
   - Permutation correction for multiple testing
   - Principal components can be added as covariates in a logisitc regression analysis [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3154648/][Clarke 2011]]
3) =Confounder= Variables that can split data into case-controls but they are different from the already assigned case-control, they are associated with the response variable but they are not responsible. QQ-plot to show the chi squared disribution between the expected and observed values
   - Get the origin of the population and adjust for population stratification (PCA and MDS)
4) =Replication= Using other samples and other platform technologies, similar study to the orignal report, or use of related phenotypes as an extension, new and different populations, or different study designs.
5) =Association= Association analysis usually use chi squared test or logistic regression.
   - Multinomial logistic regression and univariate logit are used when genotypes have probabilitic measures, ie, from imputation or Z-scores
   - Fishers exact test for association or Stouffers weighted Z-scores [[http://onlinelibrary.wiley.com/doi/10.1111/j.1420-9101.2005.00917.x/full][Whitlock 2005]]
   - MDS and logisitic regression
   - Conditional logistic regression adjusting for other variants in LD with the lead marker after fine mapping (densely genotyping the associated region)
   - A marker with a large effect has high OR

** Databases
| Database                              | URL                     |                                         |
|---------------------------------------+-------------------------+-----------------------------------------|
| Repository for human disease mutation | [[https://www.ncbi.nlm.nih.gov/clinvar/][NCBI ClinVar]]            |                                         |
| SNP-trait associations                | [[http://www.ebi.ac.uk/fgpt/gwas/][NHGRI GWAS catalog]]      |                                         |
| Database of genotypes and phenotypes  | [[http://www.ncbi.nlm.nih.gov/gap?db%3Dgap][NCBI dbGaP]] and [[http://www.ncbi.nlm.nih.gov/projects/gap/tutorial/dbGaP_demo_1.htm][tutorial]] |                                         |
| Lookup for all published GWAS         | [[http://hugenavigator.net/HuGENavigator/gWAHitStartPage.do][HuGE GWAS navigator]]     |                                         |
| Catalog for published GWAS            | [[http://www.genome.gov/gwastudies/][NHGRI]]                   |                                         |
| Associate genes with human diseases   | [[http://hapmap.ncbi.nlm.nih.gov/][HapMap]]                  |                                         |
| UCSC table for Genome Browser         | [[http://genome.ucsc.edu/cgi-bin/hgTables?command%3Dstart][UCSC]]                    |                                         |
| NCBI                                  | [[http://www.ncbi.nlm.nih.gov/SNP/][dbSNP]] database          |                                         |
| gVCF                                  | [[https://www.broadinstitute.org/gatk/guide/article?id%3D4017][GATK]]                    |                                         |
| 1000 Genomes                          | [[http://www.1000genomes.org/][Project]]                 | phase 3                                 |
| EMBL Database of Genomic Variants     | [[http://www.ebi.ac.uk/dgva/][archive]]                 |                                         |
| ENCODE (Encyclopedia of DNA Elements) | [[http://www.encodeproject.org][database]]                | human functional elements               |
| GENCODE                               | [[http://www.gencodegenes.org/][genes]]                   | annotations for genes and variants      |
| deCODE                                | [[http://www.decode.com/publications/][publication list]]        |                                         |
| International HapMap                  | [[http://hapmap.ncbi.nlm.nih.gov/][project]]                 |                                         |
| Kaiser Research Program               | [[https://rpgehportal.kaiser.org/][RPGEH]]                   |                                         |
| Latvian Genome Database               | [[http://biomed.lu.lv/en/about-us/related-organisations/genome-centre/][database]]                |                                         |
| NCBI                                  | [[http://www.ncbi.nlm.nih.gov/dbvar/][dbVar]]                   |                                         |
| NCBI                                  | [[http://www.ncbi.nlm.nih.gov/refseq/][RefSeq]]                  |                                         |
| Estonian                              | [[http://www.geenivaramu.ee/en/access-biobank][Biobank]]                 |                                         |
| UK                                    | [[http://www.ukbiobank.ac.uk/][Biobank]]                 |                                         |
| European human genome-phenome         | [[https://www.ebi.ac.uk/ega/home][archive]]                 |                                         |
| Online Mendelian Inheritance in Man   | [[http://omim.org/][OMIM]]                    | association between genes and disorders |
|                                       |                         |                                         |

** MODELING
*Random Forest*
Better than Fishers exqct test for gene-gene interaction, especially when a marginal effect is small. Marginal effect is the instantaneous effect on a dependent variable when there is a change of an independent variable, when all othe variables are kept constant. RF is robust in the case of noisy datasets and in the presence of false positive SNPs. ReliefF is used before RF or MDR to filter genetic variation before epistasis analysis [[http://bioinformatics.oxfordjournals.org/content/26/4/445.full.pdf%2Bhtml][Moore 2010]].

*ReliefF*
Jason Moore uses it a lot with MDR for epistatis and as a filtering tool [[http://link.springer.com/protocol/10.1007/978-1-4939-2155-3_17#page-1][Moore 2014]]

*Group LASSO*

*Multidimensial reduction*
Or multidimetial scaling [[http://www.statsoft.com/Textbook/Multidimensional-Scaling][MDS]]. It compliments the logistic regression and neural networks to detect interactions in the absence of marginal effect.

*Factorial analysis*

*k nearest neighbor*
It calculates the minimum distance between a set of training cases and a new case.

*Conditinal logistic regression*
It is used in stratified data because it is able to adjust for the matching of the variables with each other.

*Polymorphism interaction analysis*
PIA examines all possible SNP combinatins to find the interaction that best preducts the risk of the disease. It used the Gini index and the percentage of misclassified subjects (wrong) to find interactions. It uses 10k CV.

*SVM*
They are trained to maximize accuracy.

*LASSO*
When analyzing categorical data, there is an inability to estimates the standard errors. Bootstrap can be used to calculate the standard errors and confidence intervals [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2795963/][D'Angelo 2009]].

** KEYNOTES                                                          :Table:
The scheduled analysis is either on the 1000 genomes project [[http://www.1000genomes.org/][(link]]) or on 6.8K
GWAS for substance dependence.
GWA studies are based on Linkage disequilibrium which hypothesize a non-random
association between different loci. In the mean time the analysis involves
genetic assays of the functional exome or whole-exome sequencing data; the
variants in non-coding regions (regulomes) will be explored later on. The data
is imputed with a gene mutability score. A high score with a high mutation rate
lower the significance of a gene carrying a potential disruptive variant.

OMIM catalogues more than 3750 Mendelian disorders [[/media/Data/Bibliography/Bibliography2017/lindblom2011bioinformatics.pdf][lindblom2011bioinformatics]],
lists over 3500 diseases as genetically associated conditions, and over 4500
SNPs associated to them [[[http://www.biomedcentral.com/content/pdf/gb-2011-12-9-227.pdf][ref1]], [[http://m.bib.oxfordjournals.org/content/15/2/256.full][ref2]]]. The dbSNP catalog contains more than 40
million identified SNP [[/media/Data/Bibliography/Bibliography2017/de2013bioinformatics.pdf][de2013bioinformatics]]. Gathering data is not a problem.
This is the time of big-data where whole-genomes are sequenced fast, acurretly,
and at a lesser cost. However, data management, quality control (QC), and
analysis are hard to implement both in Mendelian disorders (oligogenic, germline
variants) and complex diseases (polygenic, somatic/cancer & mutlifactorial
disorders) and either in genomic or transcriptomic pipelines. We can sequence,
assemble, annotate, and visualize the results of a genome for example in a
matter of months. However, there is still difficulities in assessing the major
source of variance in this process [[[http://www.rna-seqblog.com/rna-seq-blog-poll-results-17/][poll results]]].

Significance of variants will be additionally estimated through other genomic
filters at the start of the analytical pipeline (\textit{to be updated}). The
pipeline integrates unsupervised learning models to filter out irrelevant
predictors. Consequently, this filtering approach reduces the
high-dimensionality of the data. Moreover, the second part depends on supervised
protocols to classify the patients on the basis of the nature of variants and
the minor allele frequency (common MAF>5%, rare MAF<5%, de novo mutations. Finally, the analysis is split into descriptive and inferential
statistics. The former explores the structure of the population and visualizes
the trends and patterns of the variation in the data. The later depends on the
association between variants and complex genetic traits; either through gene marker
selection cf., [[*Genetic.factors][Genetic.factors]] or environmental assessment cf.,
[[*Environmental.factors][Environmental.factors]]. Choosing which disease to be studied, depends on the
available format of the data.


Our research focuses either on *gene causality* or *haplotype characterization*.
Gene causality is best described by an haplo-insufficiency of *special protein
coding genes*. These genes would be associated with the developmental process of
the CNS or are related to critical epistatic functions. The presence of variants
in these genes contribute to a deleterious effect responsible for psychiatric
disorders. For this reason individuals are predisposed with higher risks of
complex genetic diseases because of relevant genomic elements. Although these
variants are susepected to be involved in phenotypic traits, their causal effect
is difficult to classify. First, the proximity of a gene to a suspicious variant
can mislead the researcher into considering a false positive. Second, increasing
the effect size of the variants improves greatly the power of the predictive
models. Finally, genetic effects on phenotype variability do not originate
solely from the heritbility of rare variants. Environmental factors are
understimated in these studies, for this reason common and unique factors grant
more insights for discovering of causal genes. Furthermore, disruptive variants
also exist in *noncoding regions* [[http://www.pnas.org/content/111/17/6131.short][kellis2014]]. Although noncoding regions escape
evolutionary conservation, recent studies corrobor the association between
noncoding rare heritable variants and diseases [[http://www.sciencemag.org/content/342/6154/1235587.short][khurana2013]]. Besides, conserved
regions of the human genome an show a lack in functionaliy and specialization. 

In our case we have more samples than predictors (n>>p). This is usefull when
using a linear model with low flexibility, ie. parametric and restricted to
sample variance. Considering the variance-bias tradeoff, variance is defined by
the difference between training sets and the bias is the difference between the
estimated predictors and the *true* observed variance. The variance is also low
at low flexibility but the bias is high. With less degrees of freedom comes less
flexibility. However by further training an adjusted model to the sampled data,
the bias drops faster than the increase in variance. The meeting point between
the bias and the variance meet captures thus the smallest score for both the
variance and bias. 

Allele frequency measures the existence of an allele relatively to the other
variants of a gene in a loci. SNPs can alter the allele frequency of a gene.
Consequently, the penetrance (effect size) and expressivity of the gene will
change in the population. This change in frequency can also come from selection,
other form of mutation, and genetic drift. However in the case where these
events are absent, a Hardey-Weinberg process can occur. At this stage, the new
allele frequency remains constant for future generations.  

SNP callers calculate the error of a SNP being a sequencing mismatch or a real
fixed polymorphism. Base calling or imputation in GWA studies increase the
prediction accuracy of trained model. Increasing the amount of information that
can be learnt through adjusting a program improve SNP calling and associations.
HapMap and the 1000 genomes project help with the imputation process. 

The search for variants provides an understanding of both complex diseases,
genetic genealogy, and ancestral origins. For example, haplotypes combine a
number of alleles inherited together from one parent ([[http://www.wikiwand.com/en/Haplotype][definitions]]). These
regions of the genome are in high linkage disequilibrium; SNPs tend to be
inherited together due to low recombination rates. Close related haplotypes
share common unique-event polymorphisms (UEP) like SNPs that designate
haplogroups. The most studied UEPs are those found in the Y-chromosome (Y-DNA)
haplogroups and mitochondrial lineages. These events are informative of the
mutability of a gene and the ancesteral origins. By comparing haplotypes with
new genomic data, we can distinguish between the derived and ancestral changes
in the Y-DNA. Consequently we can map SNPs to a chromosomal haplogroup tree ([[http://daver.info/ysub/analyze_data.htm][ref
here]]) using additional external sources. 

SNP callers provide a p-value for every variant which describes the odd ratios
of their risk association to the phenoytpe. This significance is calculated
using a X-test. For example, small odd ratios explain little of the
heritability variation of the disease. This is problematic in SNP association
studies. Imputation increases the power, significance, and speed of the association
study. 

Haplotypes assume allele correlation of inherited region in linkage
disequilibrium (LD). For example, smaller regions of LD increase the genetic
variance than bigger regions. Tag-SNPs are then identified in a haplotype, which
assumes an associatioin between rare variants in LD and the disorder
([[http://www.wikiwand.com/en/Tag_SNP][wiki]]1). Heritability analyses reveal first the chromosomal segment linked to
the disease. Then a haplotype is significantly assigned to the particular
genotype. Finally, uncommon or rare differential SNPs relative to that haplotype
are assigned as risk-factors and there allele frequency studied. The HapMap and
1000 genomes projects help imputate the studied genotype ([[http://www.wikiwand.com/en/International_HapMap_Project][wiki]]2). 

#+CAPTION: Description of human genetic repositories 
| Database                  | Description                                        |
|---------------------------+----------------------------------------------------|
| <25>                      | <50>                                               |
| HapMap                    | haplotypes + risk variants                         |
| 1000 genomes projects     | SNPs                                               |
| OMIM                      | naming scheme for genetic diseases                 |
| International classification of disease (ICD v10) | 240 hereditary diseases (from [[/media/Data/Bibliography/Bibliography2017/lindblom2011bioinformatics.pdf][lindblom2011bioinformatics]])                     |
| DECIPHER                  | Database of Chromosomal Imbalance and Phenotype in Humans Using Ensembl Resources |
| Human Variome Project ([[http://www.humanvariomeproject.org/][HVP]])   | ClinVar (US country node).                         |
| National Human Genome Research Institute (NHGRI) | GWAS catalogs (1350 studies [[/media/Data/Bibliography/Bibliography2017/de2013bioinformatics.pdf][de2013bioinformatics]])  |
|                           |                                                    |


The pipeline goes as follows: (Survey of tools for variant calling
(pabinger2014survey))
1. Individual whole-exon sequencing (exome targeted enrichment + NGS) or pooled
   sampled sequencing [[/media/Data/Bibliography/Bibliography2017/kim2010design.pdf][kim2010design]] (sometimes coupled with exon-capturing
   techniques and resequencing of promising makers, cf Table 1 in
   [[/media/Data/Bibliography/Bibliography2017/kim2010design.pdf][kim2010design]]). This includes a genotyping or a resequencing step.
2. Quality assessment and filtering (choose high coverage depth ie, nb of reads
   for each SNP and high variant calling confidence score). Error rate of
   true/false variants can be estimated with likelyhood ratio tests
   [[/media/Data/Bibliography/Bibliography2017/kim2010design.pdf][kim2010design]] or a Bayes approach (posterior for every variant assocation).
   LD, haplotype, and imputation data from other studies can be incorporated to
   improve performance. 
3. Mapping of the alignment reads to a reference genome (UCSC and GRC genome
   reference consortium) 
4. Variant calling [[file:~/Downloads/Pabinger_et_al_Supplementary.pdf][(Table of tools)]] and the use of heuristic approaches to
   distinguish between false and true positive variants. Under the
   Hardy-Weinberg assumption a G-test can give allele frequency ratios
   [[/media/Data/Bibliography/Bibliography2017/kim2010design.pdf][kim2010design]]. Kim 2010 showed that: "The agreement between callers was
   larger for SNPs compared with INDELS and larger for germline than for somatic
   mutations (tumor heterogeneity), respectively". It is best to use a consensus
   approach (pabinger2014survey) thus running multiple callers to capture the most
   of variants. 
5. Variant annotation and association (SNP, indels, CNV like short tandem
   repeats). Classification of variants is achieved by genomic annotation
   (unclassified are those that are difficult to interpret and cannot be
   unambiguously classified as pathogenic or neutral at the point of diagnosis
   [[/media/Data/Bibliography/Bibliography2017/lindblom2011bioinformatics.pdf][lindblom2011bioinformatics]]). Although this can be done at the end for
   discovery, it is done earlier for Machine Learning training. Discovery of
   common and rare variants (eg through imputation taking into account the
   sequencing technology and the experimental design, common variants used for
   training and discovery of rare risk variants, pedigree information with
   distantly related individuals (pabinger2014survey) &
   l(indblom2011bioinformatics). This means combining variant exonic calls with
   imputed data, phenotypic and pedigree information to find risk rare variants. 
8. Visualization: finishing tool for genome assembly, genome browsers (mapping
   of experimental data + annotation) or sequence alignments (comparative
   viewers) 
   
After variant calling, those to be included in later steps are i- never observed
in homogeneous form in the controls, ii- minor allele frequency. 

A large sample size and a low P-value for GWA studies increase the odds ratios
of identified loci [[/media/Data/Bibliography/Bibliography2017/citeulike:12250640.pdf][citeulike:12250640]]. Odds ratios represent the contribution of
a loci to a disorder. Generally, odds ratios are low for each genetic locus.
Moreover, percentages are the usual metric for quantitative traits. In addition
the missing heritability (estimated metric) of a trait assigned to estimated
variants is low. We can't explain all the variance of a disease due to
confounding. However some studies admit that common variations can explain most
of the heritability even when using quantitative trait [[/media/Data/Bibliography/Bibliography2017/yang2011genome.pdf][yang2011genome]]. 

Genetics is hypothesis-free according to [[/media/Data/Bibliography/Bibliography2017/citeulike:12250640.pdf][citeulike:12250640]] but GWASs are not
according to [[/media/Data/Bibliography/Bibliography2017/reich2001allelic.pdf][reich2001allelic]]. Common diseases are in part the result of common
genetic variation. As stated here [[/media/Data/Bibliography/Bibliography2017/de2013bioinformatics.pdf][de2013bioinformatics]] a disease with 30 %
heritability has a 30 % genetic effect. When the common variation have a small
effect size on the disease but high heritability, multiple genetic factors are
the cause. Common diseases like hypertension are shared through multiple
susceptibility alleles. Common SNPs in these alleles are the basis of the common
disease-common variant hypothesis. 

#+CAPTION: Factors of the hypothesis testing approach ([[/media/Data/Bibliography/Bibliography2017/de2013bioinformatics.pdf][de2013bioinformatics]])
| Factor                 | Element      | Description                                        |
|------------------------+--------------+----------------------------------------------------|
|                        | <12>         | <50>                                               |
| Common variants        | SNP CNV      | effect on common complex diseases but are they mono or poly-alleles |
| missing heritability   |              | adjust for confounding fact. multiple genetic factors + env factors |
| Allele heritability    | MAF          | population structure function of the minor allele & its MAF |
| Hereditary risk        |              | rare variants MAF<5% can play a role in diseases   |
| Stratification         |              | Genetic diversity amongst humans                   |
| Linkage disequilibrium | D' and r    | classification of SNP. non-random association and observed frequency of 2 alleles that occur together |
| Tag SNPs               | indirect association | classification of SNPS that are in strong LD with others surrounding them |
| Imputation             | meta-analysis | nonlinear interaction between SNP                  |
| Quantitative trait     | biobanks     | medical records, more phenotypic detail            |
|                        |              |                                                    |
** GOALS
*** SUMMARY
- =Questions= How to analyze heterogeneous data?
- =Goal= Single-locus analysis?
- =Hypothesis=
- =Significance=
- =Originality=
- =Approach=
- =Preliminary data=

+ How to merge data from sequencing, phenotypical, methylation and neuroimaging?
  (environment, proteome, and transcriptiome not available but useful too)
+ *Should we validate with molecular studies after GWAS* SNPs associated to a
  disease cannot be experimentally validated immediately 
  after GWAS. The SNP might be the result of a close indirect interaction. A
  nearby influential variant might be the reason of this effect.
+ *What should we adjust for during a GWAS* Results from GWAS should be adjusted
  for ancestry-derived principal components that detects potential population
  stratification.
+ *Is the data ethnically homogeneous*
+ *How to find pairs of subjects in our data that share excessive relatedness*
  Using the individual-pairwise identity-by-state (IBS) estimates from Plink

*** Causal rare variants & de novo mutations
**** Trait variability
- allelic spectrum [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][link]] (rare, common-SNPs or additive/non-additive genetic factors)
- narrow-sense heritability (common SNP-based heritability)
- individual risk-associated genes were identified from rare variation & de novo variation
- the same SNPs & CNVs can confer risk similarly in Autism and Schizo
- Two opportunities are presented, i) variants identified in the literature can be further prioritized or confirmed regarding their degree of variant causality, as Goldstein mentioned, ii) the existing sample diagnosis can be re-phenotyped to reflect their etiological similarity.
- Common SNP confer 50% heritability to assess relation between individuals.
- Rare SNPs confer 25% heritability to assess relation between individuals.
- Filtering out related individuals increases variance in the population hence a low biased assoiation between *causal* variants and traits. [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][link]]
- Loss of function mutations are under a weak purifying selection, meaning they are conserved and transmitted [[http://www.sciencemag.org/content/342/6154/1235587.short][khurana2013]].
- Common allele are a good medium to compare between populations, especially in linkage disequilibrium studies [[/media/Data/Bibliography/Bibliography2017/reich2001linkage.pdf][reich2001linkage]]
  - 
**** Genetic.factors
- additive factors (inherited common/rare SNPs)
- non-additive factors (dominant, recessive, epistatic)
- de novo mutations
**** Environmental.factors
- common (shared)
- unique (stochastic)
** Epigenetic  :noexport:
*** Descriptive exploration
**** Hereditary
***** Gene expression and splicing
gene expression and alternative splicing are independently the cause of difference due to a heterozygous variant.
Variants can be ranked with their p-values to distinguish the top variant with the most influence on gene epression
** Phenotype Definition  :noexport:
- Life chart of the patients (discovery setting) [[http://www.nejm.org/doi/full/10.1056/NEJMoa1212444#t%3DarticleDiscussion][Chen2014]]
** Repositories
- 1000 genome
- GEO
- UCSC
- ENCODE
- REFSEQ
- ENSEMBL
- Contributing projects for the [[http://exac.broadinstitute.org/][Exome Aggregation Consortium]]
  + 1000 Genomes
  + Bulgarian Trios
  + Finland-United States Investigation of NIDDM Genetics (FUSION)
  + GoT2D
  + Inflammatory Bowel Disease
  + METabolic Syndrome In Men (METSIM)
  + Myocardial Infarction Genetics Consortium:
    * Italian Atherosclerosis, Thrombosis, and Vascular Biology Working Group
    * Ottawa Genomics Heart Study
    * Pakistan Risk of Myocardial Infarction Study (PROMIS)
    * Precocious Coronary Artery Disease Study (PROCARDIS)
    * Registre Gironi del COR (REGICOR)
  + NHLBI-GO Exome Sequencing Project (ESP)
  + National Institute of Mental Health (NIMH) Controls
  + SIGMA-T2D
  + Sequencing in Suomi (SISu)
  + Swedish Schizophrenia & Bipolar Studies
  + T2D-GENES
  + Schizophrenia Trios from Taiwan
  + The Cancer Genome Atlas (TCGA)
  + Tourette Syndrome Association International Consortium for Genomics (TSAICG)

** Terms   :noexport:
- *expressions* complementary lines of evidence, elements under positive selection, allelic difference in heterozygous between haplotypes,
- *words* perturbations, haploinsufficiency, hemizygous, multi-nucleotide polymorphism (MNP), haplogroup, imputation
- *terms* deleterious variants, disruptive variants, purifying selection, spurious transcripts, DNase footprint, DNase hypersensitivity assays, genetic assays of function (my work), disease-relevant genomic elements, cellular circuitry, callset, heterozygous SNP, haplotype characterization, population variation, complex genetic trait, 
- *Knowledge* defines, describes, identifies, knows, labels, lists, matches, names, outlines, recalls, recognizes, reproduces, selects, states, reveal,
- *Comprehension* comprehends, converts, defends, distinguishes,estimates, explains, extends, generalizes, gives examples, infers, interprets, paraphrases, predicts, rewrites, summarizes, translates.
- *Application* applies, changes, computes, constructs, demonstrates, discovers, manipulates, modifies, operates, predicts, prepares, produces, relates, shows, solves, uses, rely, produce, propose
- *Analysis* analyzes, breaks down, compares, contrasts, diagrams, deconstructs, differentiates, discriminates, distinguishes, identifies, illustrates, infers, outlines, relates, selects, separates
- *Synthesis* categorizes, combines, compiles, composes, creates, devises, designs, explains, generates, modifies, organizes, plans, rearranges, reconstructs, relates, reorganizes, revises, rewrites, summarizes, tells, writes
- *Evaluation* appraises, compares, concludes, contrasts, criticizes, critiques, defends, describes, discriminates, evaluates, explains, interprets, justifies, relates, summarizes, supports, corroborate
- *data science* Confounding (detect spurious correlations), munging (convert formats to more human readable), KPI (key performance indicator)

** Workflow
1. Acquire the 1000 genomes in a file format that depend on the tool(s) for pre-processing
2. Preprocess the 1000 genomes for descriptive statistics (regressions, ordination stats)
3. Filtering comprises of LD around core SNPs, common variant and heritability of quantitative traits, rare variants (MAF <= ??) and LD, etc.
4. Post-processing for classification of variant types (novel, rare, and damaging)
5. Search for association between variants and traits
6. I need a Testing set (whatever dataset with samples and predictors) and a Validation set (already known outcomes for which complete observations are available,already clustered with description of a or multiple causal-effects. Either 2 classes, binomial, or i>2 for multivariate classification. This set is used to validate the reproducibility of the inferred clusters). The testing and validation sets can be split from one original study or they can have different origins. That is any library with structured predictors as classified outcomes (clustered phenotypes) can be used as a validation set.
7. Copy number and SNP profiling. This choice is dependent on the genomic variant considered in the data.
8. Variant association with genes at other sites of the genome. Generate a map of the variants CNVs or SNPs to examine their impact on the phenotypical variance. For example cis-acting variants are within 3Mb range surrounding the gene in question. Trans-acting are outside this range. cite{curtis2012genomic}
9. The chosen variants can be used next as predictors to elucidate phenotype variance (patient, normal, etc.)
10. Manipulate the weighting system (variant prioritization) either using the genes associated to disorders that have effect on one another, presence of indels near the polymorphism site, presence of regulation sites (TF binding sites TFBS, DNAaseI hypersensitive sites, ncRNAs, and enhancers [[http://www.sciencemag.org/content/342/6154/1235587.short][khurana2013]]). Additionally, TFBS for example can be divided into 2 more categories, proximal versus distal or cell-line-specific versus -nonspecific.
11. Find the structure of the population, through combined principal analysis and clusterization.
12. integration of additional data sets including RNA sequencing data, proteomics data and metabolomics data.


1. Use phenotypic data for snp classification not for filtering. Phenotypic data might have bad quality thereby lowering the filtering process.
2. Cases must share the same ethnicity. I can't compare cases and controls from different geographical areas.
3. Heterogeneity in a dataset is a drawback. Covariate adjustment can reduce it.
4. Samples added to a dataset must be independent
5. Individual studies added must be build with a common genome [[/media/Data/Bibliography/Bibliography2017/de2013bioinformatics.pdf][de2013bioinformatics]]
6. For imputation the reference panel (reference allele for the published data in Hapmap and 1000 genomes) and that of the study population (raw data) must be identical
7. Missing heritability: confounding, epistatic effect (mutlimarker interactive effect), LD (association between snps), epigenetic ([[/media/Data/Bibliography/Bibliography2017/slatkin2009epigenetic.pdf][slatkin2009epigenetic]])
** Multiple diagnoses   :noexport:
\textit{Dawei: Our patient samples have multiple disgnoses (a total of 3000 variables). To cluster them into more homogeneous subgroups seems a chanllenge (even in the field) and we don't know how many subgroups they should be....  Do you think if we should implement this model (or some other models) on our phenotypes or it is really worthy to try?  or this can also be used for genotypes}

Usually one can start with ordination analysis in order to visualize the variation in the dataset. For example principal component analysis (PCA) or constraint analysis (CCA, RDA). These methods will reduce the dimension of the dataset to better visualize the trends in the data.

Next, one can either cluster (fuzzy soft clustering, or hard hierarchical clustering) or classify the features.
Classification on the other hand is a complex and powerful technique. It will be either supervised or unsupervised. If one have a lot of patient samples (n>1000) unsupervised learning can be a nice choice. For this reason, the results will be clustered-like to show how patients are categorized regarding their features.
They will be grouped together depending on patterns in their phenotype. One can use support vector machines (SVM), splines, polynomials, local regressions etc...
Finally, to get most of the dataset, one implements new rules. For this, one can try some supervised learning protocols and extract the information out of those patterns.

For future use, one can map those patterns to the patients. The patients with recognized candidate pattern to a specific phenotype (illness or resistance) can be further diagnosed. That means, if one find that Patients 1 through 5 carry a special gene, their families can be further studied in a simplex or multiplex sampling.

** Table of tools
[[/media/Data/Bibliography/Bibliography2017/pabinger2014survey_supp.pdf][pabinger2014survey_supp]] [[/media/Data/Bibliography/Bibliography2017/pabinger2014survey.pdf][pabinger2014survey]]
** Generalities  :noexport:
- 1K genome project was done with a low-depth geep sequencing
- most GWAS loci lie in noncoding regions
- I should consider the ancestry (European, Australian, African) of the sequenced data
- 50% of the human genome is comprised of repetetive elements, often of high degeneracy
- ~4000 genes have been associated with human disease
- 4.5 deleterious mutations in every generation [[/media/Data/Bibliography/Bibliography2017/pabinger2014survey.pdf][pabinger2014survey]]
- Each genome carries 165 homozygous protein-truncating or stop loss variants in genes representing a diverse set of pathways [[/media/Data/Bibliography/Bibliography2017/pelak2010characterization.pdf][pelak2010characterization]]. That is  any SNV that results in the gain of a stop codon, and any indel that results in a frameshift coding change.
- Human genome is 3 Gb [[/media/Data/Bibliography/Bibliography2017/citeulike:12250640.pdf][citeulike:12250640]]
*** Linkage disequilibrium
The degree to which the allele of one SNP is observed with the allele of another
within a population

A non-random association between alleles at different loci. The human genome has a haplotype structure were neighbouring alleles correlate in LD [[/media/Data/Bibliography/Bibliography2017/citeulike:12250640.pdf][citeulike:12250640]]. Haplotype blocks extend less far in Africans than European descent.
*** Linkage analysis                                             :noexport:
The attempt to relate the transmission of an allele in families to the
inheritance of a disease
*** Odds ratio                                                   :noexport:
It is the measure of the extent of the relationship under two case/control
treatment conditions
*** Chi-square
Tests the null hypothesis that the distribution of the samples (responders) is
the same under both treatment conditions, for quantitative data.
*Contingency table for categorical data* is to test the null hypothesis that
there is an association between variables (ie Population: African, European,
American). If we refute the H0 than there is no association and there is no
difference between observed and expected values. 
*Degrees of freedom* is the number of categories minus one (ie Population:
African, European, American, and 2 treatment conditions df= 3-1 * 2-1 = 2) for
df=(r-1)*(c-1), r=rows, c=columns
*** Function
Function of a gene is defined differently relatively to the background of the interpreter [[http://www.pnas.org/content/111/17/6131.short][kellis2014]].
- Genetic: phenotypic plasticity from inherited polymorphism, while considering the cell type and its condition
- Evolutionary biology: interaction of selective constraints, while considering the environement effect on the phenotype
- Molecular biology: measure the activity of a molecule and its interactions
*** Sampling
larger samples = increase in statistical power of rare variants
*** Noncoding functional elements
Promoters, enhancers, silencers, insulatrors, noncoding RNAs, microRNAs, piRNAs, exRNAs, structural RNAs, and regulatory RNAs
*** Polymorphism
SNP, indels, microsatellite, short tandem repeats (STR), multinucleotide polymorphism (MNP), heterogous sequence, named variants.
*** Microsatellites
Repeats in the dna sequence, usually found in non coding regions [[/media/Data/Bibliography/Bibliography2017/rosenberg2002genetic.pdf][rosenberg2002genetic]]
*** Twin studies
when working with twins, the monozygotic or dizygotic concepts should be considered [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][link]]
*** Simplex vs multiplex studies
simplex family, comprises of one affected subject within the set of first and second degree relatives. Multiplex family is equal to at least 2. Simplex families decrease heritability of a database.Multiplex families increase its heritability. The liability of autism-associated alleles is greater in multiplex families. [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][link]]
*** CIGAR
(infor from SAM/BAM file) 3M1I3M1D5M query aligned to a reference contains insertions (I) and deletions (D) http://goo.gl/2cKi2q
*** Mutations
loss of function (LoF) amorphic - gain of function (GoF) neomorphic - dominant negative antimorphic - indels (frameshift, stop loss, missense) - composite insertions - substitution events (transition, transversions) - synonymous mutation
*** CNA
Somatic copy number aberrations are acquired genomic changes, studied mainly in cancer. They can have a cis or trans impact on their own expression or other genes respectively.
*** Linkage vs association 
Linkage is actually looking at physical segments of the genome that are associated with given traits. Association studies go from the other direction, saying, given different pieces of the genome, can we then look for different traits that are associated with those different segments of genome? So we know that individuals don't have the same genetic makeup. They have the same DNA, but the DNA has different sequences or is expressed differently, and thats what causes differences among different individuals. So the question is that if we have a trait, particularly a disease trait, can we find and associate that with differences among individuals in the population? 

So a linkage study is just saying, can we say that there is an association between pieces of the DNA and a trait of interest? Association studies are saying, what are the differences we see in order to find differences in the traits, particularly disease traits, among different individuals.
*** Allele frequencies and effect of CNVs on their dynamics 
If the frequency of an allele is 20% in a given population then among population members one in five chromosomes will carry that allele. Four out of five will be occupied by other variant of the gene. 

\textbf{The dynamics of allele and gene frequencies are affected by several factors such as migration, mutation, genetic drift, population size, mating [...] (wikipedia)} This concept follows the Hardy-Weinberg equilibrium, ie, *stability of allele frequencies over time*. The HW principle assesses the Mendelian inheritance of alleles and their dynamics. So allele frequencies should be considered in terms of inherited variants not de novo mutations. 

\textbf{[...] natural selection converts differences in fitness into changes in
allele frequency in a population over successive generations (wikipedia)}
Accordingly, genetic variants are fixed and propagated in terms of trait
selection. Since disorders are phenotypically disadvantageous and dont confer fitness to a body, these CNVs will have deleterious effects on the phenotype and will reduce the frequency of an allele in the sampled population. (reverse genetic hitchhiking)

If the frequency of an allele is 5% in a given population then 1 in 20 chromosomes will carry that allele, 19 out 20 will occupy other variant of the gene

A SNP can be associated to multiple alleles. The less common allele is known to have a minor allele frequency.
*** Low-depth whole genome sequencing
low depth WGS with larger sample can be more powerful than deep sequencing with fewer samples.<Lee 2014>
*** Hardy-Weinberg equilibrium
The allele frequency in a population will remain constant from generation to generation in the absence of other evolutionary influences. That is mate choice, mutation, genetic drift, selection, gene flow, and meiotic drive.

A random variation in the distribution of the allele frequency of a population. When the allele is present in a small number of copies, the effect of drift will be most important.
*** Genetic hitchhiking
*** UK10K exomes project
*** CCDS project
*** ENCODE project
*** Autism Genome Project (AGP)
*** Population-Based Autism Genetics and Environment Study (PAGES)
*** Database of Genotypes and Phenotypes (dbGaP)
*** National Institute of Mental Health (NIMH)
*** KNN-like clustering http://m.sciencemag.org/content/344/6191/1492.full.pdf
Test: Unsupervised learning model similar to K nearest neighbor (KNN) or support vector machines (SVM). 
Q: How many clusters are sufficient to project the variation of all the dataset ?

Sometimes we go with a subjective visual intuition to determine the number of clusters. Especially in fuzzy clustering (like in Bassim 2014b)

In noisy datasets analysis can bring a lot of false positives and for specialized protocols (targeted studies based on genetic markers) the analysis will lose information due to discriminating outliers.
We can't get enough information through sampling. Meaning one cant sample all the variation in a population. For this reason, outliers are not obligatory errors. There is only not enough information so the variation would be considered as a cluster
*** Population Structure                                         :noexport:
Look at the structure of the population through genomic data generated from multi-locus markers.
The software found [[http://pritchardlab.stanford.edu/structure.html][here]].
*** Repeat masking (idea from [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955739/][Matukumalli2006]])
Identification of [[http://www.ncbi.nlm.nih.gov/blast/Blast.cgi?CMD%3DWeb&PAGE_TYPE%3DBlastDocs&DOC_TYPE%3DFAQ#LCR][low and high complexity sequences]] of the genome. Artifacts can be associated with low complexity regions. For nucleotide queries it is determined by the [[http://www.ncbi.nlm.nih.gov/books/NBK1763/][DustMasker]] program.
Identification of common repeats that are specific for every species.
*** Sequence quality
Polymorphic sites (containing possible variant calls) can be observed because of a poor quality sequencing (poor quality base).
A relative polymophic region can be detected at either end of an alignment, which tend to be poor hence unreliable due to inherent limitations in current sequencing technologies [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955739/][Matukumalli2006]].

*** Formats
+ BAM (short-read binary alignment with position sorted, compressed, indexed, in binary form)
+ SAM (sequence aligment/map, human readable version)
+ BCF (Binary variant call format, likelihood of data given each possible genotype)
+ BAQ (base alignments quality)
+ VCF (variant call format, storing snps, short indels, and structural variations)
+ BEM (copy number variant map)
+ Fasta, FASTQ (genes and reference genomes)
+ MAPQ (contains the "phred-scaled posterior probability that the mapping position" is wrong)
*** Manhattan plot
It is used to draw association between SNPs, their chromosome location, and their effect on the phenotypic trait. [[https://en.wikipedia.org/wiki/Genome-wide_association_study#mediaviewer/File:Manhattan_Plot.png][image]]
*** Technologies
Illumina and Affymetrix [[/media/Data/Bibliography/Bibliography2017/ragoussis2009genotyping.pdf][ragoussis2009genotyping]]
*** Bioconductor
[[http://bioconductor.org/packages/release/bioc/html/FunciSNP.html][FunciSNP]]
*** GenePattern
the servername is not set with localhost
*** Parallel computing
Use doSNOW and foreach loops for parallel (not sequential computing)
*** Resampling
Use bootstrap and bagging, in addition to cross validation (even both) for iterating the variation of the population.
*** Descriptive statistics
**** CI
For 95% of the time (probability) the population parameter (variation) will fall between the boundaries of the stochastic interval. (the population mean may be outside the boundaries)
**** Phylogenetic tree visualization
http://en.wikipedia.org/wiki/List_of_sequence_alignment_software
*** Machine learning
- *Definition*: It is adjusting learning processes to observed data for acquisitin of hidden patterns and relationships. It helps organize correlations of the different parts of the problem to predict trends in the declared variables.
- *Classification*: sorting new observations through learning by adjusting of adaptive parameters that belong to already categorized data
- *Neural network*: supervised (backpropagation) and unsupervised training is used to classify patterns and for other problems(approximation, optimization)
- *Machine learning*: pattern discovery and inferences in order to extract relative decisions from them
- *SVM*: methods that rely on supervised learning to categorize discovered patterns through classification or regression
*** Linear regression 
1K genome description analysis
*** Phylogenetic tree
 from the 1K genome SNP indels (cf. nature13679)
*** treelet
covariance smoothing [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][Gaugler2014]]
*** PolyBayes
probability to compute a posterior for each called SNP for a given prior if a variation was observed at that postition [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955739/][Matukumalli2006]].
*** Tool 1
Improved exome prioritization of disease genes through cross-species  
phenotype comparison
*** Variant Master
Simultaneous identification and prioritization of variants in  
familial, de novo, and somatic genetic disorders with VariantMaster
*** MindTheGap
http://goo.gl/Ti5POK
*** VSEAMS
A pipeline for variant set enrichment analysis using summary GWAS data
http://www.ncbi.nlm.nih.gov/pubmed/25170024
*** GCTA
- *use*: estimation of allele frequency using common SNPs
- *method*: filter approach for patients using SNPs with minor allele frequency (MAF)
- GCTA estimates the variance explained by all the SNPs on a chromosome or on the whole genome for a complex trait rather than testing the association of any particular SNP to the trait. cite{yang2011gcta}
- Most genetic risk for autism resides with common variation, attached. See the Methods. You can use GCTA on our existing data as well. [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][Gaugler2014]]
- estimate the heritability due to common variants (SNP-based heritability)
- kinship greater than 5th-degree relatives are excluded
*** GREAT
Studying on-coding cis-acting regions, regulomes, assign function to cis-regulatory regions http://bejerano.stanford.edu/great/public/html/
*** Tool set 2
Plink, SNPtest, Beagle, Presto, *Mach*, ProbABEL, Impute, *datABEL*, GenABEL, R *ncdf* library (netcdf stored versions of the genotypes), gtool (for ped files)

*** Multiple Sequence Alignments
A natural extension from the pairwise alignments of BLAST  how are
all those sequences youve identified in your search related to one
another? In this session well cover tools such as ClustalW and
MUSCLE.
*** HMMER
More powerful sequence similarity searches and domain finding with
Hidden Markov Models
*** SAMtools
- tutorial 1 http://samtools.sourceforge.net/mpileup.shtml
*** Perl
App::XLSperl is a perl module with basic line commands found [[https://metacpan.org/pod/distribution/XLSperl/bin/XLSperl][here]]. It is slow. I should try either building a perl script using several modules and regular expressions or use sysadmin single-line commands based on grep, sed, ag, and xargs.
*** Shell
**** Regular expressions                                           :Table:

#+CAPTION: Basic and extended regex summary
| RegEx            | Class    | Type          | Meaning                             |
|------------------+----------+---------------+-------------------------------------|
| .                | all      | Character Set | A single character (except newline) |
| ^                | all      | Anchor        | Beginning of line                   |
| $                | all      | Anchor        | End of line                         |
| [...]            | all      | Character Set | Range of characters                 |
| *                | all      | Modifier      | zero or more duplicates             |
| \<               | Basic    | Anchor        | Beginning of word                   |
| \>               | Basic    | Anchor        | End of word                         |
| \(..\)           | Basic    | Backreference | Remembers pattern                   |
| \1..\9           | Basic    | Reference     | Recalls pattern                     |
| _+               | Extended | Modifier      | One or more duplicates              |
| ?                | Extended | Modifier      | Zero or one duplicate               |
| \{M,N\}          | Extended | Modifier      | M to N Duplicates                   |
| (...\vert...)    | Extended | Anchor        | Shows alteration                    |
| \(...\\vert...\) | EMACS    | Anchor        | Shows alteration                    |
| \w               | EMACS    | Character set | Matches a letter in a word          |
| \W               | EMACS    | Character set | Opposite of \w                      |

*** Scenario one
Taken from [[http://genomespot.blogspot.co.uk/2014/10/geneclouds-unconventional-genetics-data.html%20][this site]]. Script used to process data:
awk '$6>0 && $8<0.05 {print $1,$8}' DESeq.xls \
\vert awk '{printf "%4.3e\t%s\n", $3 , $2}' \
\vert sed 's/e-/@/' \
\vert cut -d '@' -f2- \
\vert awk '{print $2":"$1}' > ups.txt

awk '$6<0 && $8<0.05 {print $1,$8}' DESeq.xls \
\vert awk '{printf "%4.3e\t%s\n", $3 , $2}' \
\vert sed 's/e-/@/' \
\vert cut -d '@' -f2- \
\vert awk '{print $2":"$1}' > dns.txt
*** Scenario two
$ echo radar \vert sed 's/\([a-z]\)\([a-z]\)[a-z]\{3\}/\1/'
*** Disorders
**** Autism
- Autism features are also associated with Fragile X, Down and Klinefelter syndromes
- neurodevelopmental disorder, genetically typified by a mixture of de novo and inherited variation [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][link]].
- Autism Spectrum Disorder (ASD) has a genetic association with heritable rare variants [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][{Gaugler 2014}]]
- Clinical phenotypes can have an effect on the genetics of autism, for example IQ (higher vs lower functioning) [[http://www.nature.com/ng/journal/v46/n8/abs/ng.3039.html][Gaugler2014]]
**** Schizophrenia
- severe mood and behavioral psychiatric disturbances
- dismorphic features (#toBeVerified)
- mental retardation
- genetic affiliation cite{wilson2006dna}
- schizoaffective disorder
**** Bipolar disorder
- genetic affiliation acite{wilson2006dna}
**** Parkinson
*** Topics
- Integration of mutliple sources (RNA-seq, GWAS, methylation)
- Variant calling
- Gene-environment interactions to explain the missing heritability in complex diseases
** Resources
- ~6,800 GWAS and imputed data for substance dependence (alcohol, cocaine, opiate, nicotine and marijuana). More exome chip data will be ready (currently in QC). In total, we have > 13,000 samples with well defined such phenotypes. Each sample has about 3,000 phenotypic columns if you count every measurements. Most of the samples are adult unrelated cases controls but 10-20% are family samples.
- 1,500 GWAS data + brain images + phenotypes (children cognition/behavior diagnoses) from same samples.
- A small size of DNA methylation data + phenotypes (brain concussion) + brain images
- A few thousand of GWAS data + DNA methylation data + phenotypes (children behavior) + brain images: This is a longitudinal study, which means data from multiple time points are available.
- GWAS data for a few other phenotypes, like schizophrenia, Parkinson's disease, alzheimers disease. We may design secondary data analysis, such as genome-wide meta-analysis or any other analyses..
- next gen sequencing data, diseased or unknown phenotypes.
- As you know, the publicly available data, such as the 1000 genome data, is also a great resource to explore ideas.

- Our patient samples have multiple disgnoses (a total of 3000 variables). To cluster them into more homogeneous subgroups seems a chanllenge (even in the field) and we don't know how many subgroups they should be....  Do you think if we should implement this model (or some other models) on our phenotypes or it is really worthy to try?  or this can also be used for genotypes

* HADOOP :noexport:
** GENERAL
- In our lab we have similar but non-google scale data (it is then wisely to use
  HDFS technology)
- HDFS aussmes locality for perfomance (ie the cluster of data nodes should be
  situated in the same area, geographically)
- Hadoop can be user through Cloudera's VM
- To get started with Hadoop, first to setup a cluster and learn how to run a
  program on it, then write MapReduce code to access the data nodes.
- Using MapReduce i can manipulate the input/output file formats of my data.
- MapReduce parallelizes large computations easily ([[http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/mapreduce-osdi04.pdf][cite]])
- Fault tolerance in MapReduce is designed to save completed tasks and reset the
  failing worker (cluster). Then the remaining tasks are assigned to another
  worker (reschedule remaining tasks)
- MapReduce library can read data in many formats
- MapReduce enable the user to produce a summary of the running tasks (status information)
- Essential functions in MapReduce are the counter, grep, sort, and large scale indexing
- Parallel programming is made easy with higher performance with MapReduce
** CLOUDERA VM
The Cloudera VM has the Hadoop ecosystem already installed and configured. ([[http://www.cloudera.com/content/cloudera/en/downloads.html][site]])
** SETUP HADOOP
*** STANDALONE ONE CLUSTER
Follow this tutorial on how to install Hadoop and configure a local machine to
run HDFS and the Hadoop ecosystem.
*** HADOOP ECOSYSTEM
Hive and Pig are analytics tools used to translate common SQL and text based
commands into MapReduce jobs.
* PLINK
Download and setup PLINK from the [[http://pngu.mgh.harvard.edu/~purcell/plink/download.shtml][main site]].
#+BEGIN_SRC shell
export PATH=$PATH:~/plink-1.07-x86_64
plink
#+END_SRC

#+CAPTION: Number of iterated SNPs and 1 rare causal SNP 
| Nb of SNPs    | Label | Freq low | Freq up | OR (Aa) | OR (AA/aa) |
|---------------+-------+----------+---------+---------+------------|
| 1             | rare  |        0 |       1 |       2 |          4 |
| 1000--100,000 | null  |        0 |       1 |       1 |          1 |
|               | snpA  |        0 |    0.05 |       1 |          1 |
|               | snpB  |     0.05 |     0.1 |       1 |          1 |
|               | snpC  |      0.1 |     0.2 |       1 |          1 |
|               | snpD  |      0.2 |       1 |       1 |          1 |
|               | rare  |        0 |       1 |       1 |          2 |
|               |       |          |         |         |            |


#+CAPTION: Scenarios for 100 simulations
| Scenarios | Description | Simulations   |
|-----------+-------------+---------------|
| I         | SNP         | 1000--100,000 |
|           | Case        | 1000          |
|           | Control     | 1000          |
| II        | SNP         | 1000--100,000 |
|           | Case        | 1000--100,000 |
|           | Control     | 1000--100,000 |
| III       | SNP         | 100,000       |
|           | Case        | 1000--100,000 |
|           | Control     | 1000--100,000 |
| IV        | SNP         | 100,000       |
|           | Case        | 2000--200,000 |
|           | Control     | 1000--100,000 |
| V         | SNP         | 100,000       |
|           | Case        | 1000--100,000 |
|           | Control     | 2000--200,000 |
| VI        | 3 stages    |               |
| VII       | 4 stages    |               |

Simulate data
#+BEGIN_SRC shell
plink --simulate gwas.sim --simulate-ncases 251 --simulate-ncontrols 200 --simulate-label POP1 --make-bed --out hapmap1
#+END_SRC

Recode simulated data (format in .bed .fam .bim) to .ped =optional=. Only if to be used with =GenABEL=
#+BEGIN_SRC shell
plink --bfile hapmap1 --recode --tab --out hapmap1
#+END_SRC

Generate missing statistics for genotyping rates
#+BEGIN_SRC shell
plink --bfile hapmap1 --missing --out miss_stat 
#+END_SRC

Generate statistics for allele frequencies
#+BEGIN_SRC shell
plink --bfile hapmap1 --freq --out freq_stat 
#+END_SRC

Basic association analysis
#+BEGIN_SRC shell
plink --bfile hapmap1 --assoc --out as1
sort --key=7 -n as1.assoc | head
#+END_SRC

Correct for multiple testing and see the inflation factor
#+BEGIN_SRC shell
plink --bfile hapmap1 --assoc --adjust --out as2
cat as2.log
#+END_SRC

Calculate many tests for association (contingency tables, CATT...). If the genotypic 2x3 test is not generated run =--cell= command.
#+BEGIN_SRC shell
plink --bfile hapmap1 --model --snp disease_96 --out mod1
cat mod1.model
#+END_SRC

Stratification analysis
#+BEGIN_SRC shell
plink --bfile hapmap1 --cluster --mc 2 --ppc 0.05 --out str1 
#+END_SRC

Association analysis accounting for clusters
#+BEGIN_SRC shell
plink --bfile hapmap1 --mh --within str1.cluster2 --adjust --out aac1
head aac1.cmh.adjusted
#+END_SRC

Pair up the most significant individuals
#+BEGIN_SRC shell
plink --bfile hapmap1 --cluster --cc --ppc 0.01 --out version2
#+END_SRC

Set the number of cluster for the association testing (repeat the last 2 steps)
#+BEGIN_SRC shell
plink --bfile hapmap1 --cluster --K 2 --out version3 
plink --bfile hapmap1 --mh --within version3.cluster2 --adjust --out aac2
head aac2.cmh.adjusted
#+END_SRC

Export to R
#+BEGIN_SRC shell
plink --bfile hapmap1 --cluster --matrix --out ibd_view
#+END_SRC

Plot a PCA
#+BEGIN_SRC R
m <- as.matrix(read.table("ibd_view.mibs"))
mds <- cmdscale(as.dist(1-m))
k <- c( rep("green",45) , rep("blue",44) )
plot(mds,pch=20,col=k) 
#+END_SRC

*Script for simulating data*
#+BEGIN_SRC shell
#! /bin/sh


# identify simulation properties
echo "Name the simulation:"
read SIM

# simulate data
# run association analysis
# correct for multiple testing (generate pvals)
./plink --simulate gwas.sim --assoc --adjust --out $SIM

# show the first 10 lines of the association analysis
#sort --key=7 -n $SIM.assoc | head
#sort --key=7 -n $SIM.assoc.adjusted | head

# copy the disease SNP line from sim.assoc
ASSOC=$(gawk '/disease/ { $1=""; print $0 }' $SIM.assoc)
# copy the disease SNP line from sim.assoc.adjusted
ADJUST=$(gawk '/disease/ { $1=""; print $0 }' $SIM.assoc.adjusted)

# extract the number of SNPs simulated in the .bim file
SIMSNP=$(gawk '/bim file/ { print "  "$1 }' $SIM.log)
# extract the adjusted genomic inflation estimation
ADINFLATION=$(gawk '/inflation/ { print "  "$11 }' $SIM.log | sed 's/.$//')
# extract the number of simulated cases
CC=$(gawk '/cases/ {print "  "$4 "  "$8 }' $SIM.log)

# file containing scores for association without correction
echo "${SIM}${CC}${SIMSNP}${ASSOC}" >> uncorrected.txt
# file containing scores for association with correction for multiple testing
echo "${SIM}${CC}${SIMSNP}${ADJUST}${ADINFLATION}" >> corrected.txt

#+END_SRC 

