#+TITLE: Network analysis of immunity genes in neural and respiratory cells
#+AUTHOR: Sleiman Bassim, PhD
#+EMAIL: slei.bass@gmail.com

#+STARTUP: content
#+STARTUP: hidestars
#+OPTIONS: toc:5 H:5 num:3
#+LANGUAGE: english
#+LaTeX_HEADER: \usepackage[ttscale=.875]{libertine}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \sectionfont{\normalfont\scshape}
#+LaTeX_HEADER: \subsectionfont{\normalfont\itshape}
#+LATEX_HEADER: \usepackage[innermargin=1.5cm,outermargin=1.25cm,vmargin=3cm]{geometry}
#+LATEX_HEADER: \linespread{1}
#+LATEX_HEADER: \setlength{\itemsep}{-30pt}
#+LATEX_HEADER: \setlength{\parskip}{0pt}
#+LATEX_HEADER: \setlength{\parsep}{-5pt}
#+LATEX_HEADER: \usepackage[hyperref]{xcolor}
#+LATEX_HEADER: \usepackage[colorlinks=true,urlcolor=SteelBlue4,linkcolor=Firebrick4]{hyperref}
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

=to be updated=

This git repository includes the pipeline for RNA-seq data processing and sequence annotation (in =org= format for emacs. Can be imported as Markdown or viewed below), [[https://github.com/neocruiser/Rstats/blob/master/nodule/nodule.Rnw][the R code]] for graphing and data table management (in LaTeX-knitr format. Can be imported with RStudio), [[https://github.com/neocruiser/Rstats/blob/master/ganglia/ganglia.pdf][the PDF]] of the compiled R file, the data =to be updated= used for generating the figures, and the scripts =to be updated= (in bash) for RNA-seq reads pre-processing (detailed in the tutorial below).

* Preprocessing RNA-seq data
** Quality controls
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=1:ppn=16:native
#PBS -l walltime=02:00:00
#PBS -N fastqc.out
#PBS -e fastqc.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

#export PATH="$PATH:/home/silo/blast/bin"
#export BLASTDB="/oasis/projects/nsf/sun108/bassem/db/string"

oasis=/oasis/projects/nsf/sun108/


for i in {1..24}
do
    for j in 1 2
    do

zcat ${oasis}/silo/data/ganglia/gg.${i}.R${j}.fastq.gz | \
fastqc/fastqc ${oasis}/silo/data/ganglia/gg.${i}.R${j}.fastq.gz \
--outdir=${oasis}/silo/ganglia/fastqc/

    done
done
#+END_SRC
** Merge replication files and count the number of reads
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=1:ppn=16:native
#PBS -l walltime=15:00:00
#PBS -N merge.gg.out
#PBS -e merge.gg.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V
# because of large datasets
ulimit -s unlimited

scratch=/oasis/scratch/silo/temp_project
oasis=/oasis/projects/nsf/sun108/silo/data/ganglia
home=/home/silo

## merge all files
## total to 48 files for each condition
find ${oasis} -name "*gg*R1*gz" | xargs zcat | gzip -c9 > ${scratch}/ganglia/data/gg.all.R1.fq.gz
find ${oasis} -name "*gg*R2*gz" | xargs zcat | gzip -c9 > ${scratch}/ganglia/data/gg.all.R2.fq.gz

#r1=.R1.fastq.gz
#r2=.R2.fastq.gz

jobid=br.gg

## count the number of reads

## change the number of files
echo -e "Concatenating gg files. Read counts for reverse seq R1:" >> $home/count_${jobid}_output.log
zcat ${scratch}/ganglia/data/gg.all.R1.fq.gz | grep -i "@acb052" | wc -l >> $home/count_${jobid}_output.log
echo -e "Concatenating gg files. Read counts for fwd seq R1:" >> $home/count_${jobid}_output.log
zcat ${scratch}/ganglia/data/gg.all.R1.fq.gz | grep -i "@acb052" | wc -l >> $home/count_${jobid}_output.log

echo -e "Concatenating gg files. Read counts for reverse seq R2:" >> $home/count_${jobid}_output.log
zcat ${scratch}/ganglia/data/gg.all.R2.fq.gz | grep -i "@acb052" | wc -l >> $home/count_${jobid}_output.log
echo -e "Concatenating gg files. Read counts for fwd seq R2:" >> $home/count_${jobid}_output.log
zcat ${scratch}/ganglia/data/gg.all.R2.fq.gz | grep -i "@acb052" | wc -l >> $home/count_${jobid}_output.log

#+END_SRC

Merge a specific number of files.
#+BEGIN_SRC shell
## merge 8 files from BR and GG. Total=16
for f in br gg
do
    for i in {1..8}
    do
        zcat $f.$i.R1.P* | gzip -c >> $scratch/ganglia/merged.trimmed/R1.merged.fq.gz
        zcat $f.$i.R2.P* | gzip -c >> $scratch/ganglia/merged.trimmed/R2.merged.fq.gz
    done
done
#+END_SRC

** Sample from the merged file
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q vsmp
#PBS -l nodes=1:ppn=256:vsmp
#PBS -l walltime=6:00:00
#PBS -N sampling.80p.gg
#PBS -e sampling.80p.gg.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V
# set stack to unlimited
# because of large datasets
ulimit -s unlimited
# echo stdout to output file
set -x
# xsede directories
oasis=/oasis/projects/nsf/sun108
scratch=/oasis/scratch/silo/temp_project
home=/home/silo
##################################

##################################
jobid=gg.80p
fileid=gg.all
workdir=${scratch}/ganglia/data
# fastq raw files, reads
sense=${scratch}/ganglia/data/$fileid.R1.fastq
antisense=${scratch}/ganglia/data/$fileid.R2.fastq

$home/seqtk/seqtk sample -s1234 $sense 166283796 > $workdir/$jobid.R1.fastq
$home/seqtk/seqtk sample -s1234 $antisense 166283796 > $workdir/$jobid.R2.fastq
#+END_SRC
** Trimming reads
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=4:ppn=16,walltime=4:00:00
#PBS -N trim.all
#PBS -q long
#PBS -V

scratch=/gpfs/scratch/ballam
home=/gpfs/home/ballam
input=${scratch}/raw/ganglia
output=${scratch}/ganglia/trimmed

mkdir -p $output
mkdir -p /gpfs/scratch/ballam/ganglia/trimmed

sample[1]=br
sample[2]=gg

for s in {1..2}
do
    sample=${sample[${s}]}
    for f in {1..24}
    do
	java -jar Trimmomatic-0.33/trimmomatic-0.33.jar PE \
	    -phred33 \
	    ${input}/$sample.$f.R1.fastq.gz \
	    ${input}/$sample.$f.R2.fastq.gz \
	    ${output}/$sample.$f.R1.P.fastq.gz \
	    ${output}/$sample.$f.R1.U.fastq.gz \
	    ${output}/$sample.$f.R2.P.fastq.gz \
	    ${output}/$sample.$f.R2.U.fastq.gz \
	    ILLUMINACLIP:adapters.fa:2:30:10 \
	    LEADING:5 \
	    TRAILING:5 \
	    SLIDINGWINDOW:4:15 \
	    MINLEN:36
	done
    done
#+END_SRC

* Trinity transcriptome assembly
#+BEGIN_SRC shell
#!/bin/bash
#SBATCH --partition=LM
#SBATCH --nodes=1
#SBATCH -t 48:00:00
#SBATCH --job-name="trinSepa"
#SBATCH --output="trinity.%j.%N.out"
#SBATCH --export=ALL
#SBATCH --mail-user=sleiman.bassim@stonybrook.edu

### Would finish in 40 hours for 400 million reads and 65h for 1 billion reads

module load trinity
module load java
module load bowtie
module load samtools

# set stack to unlimited because of large datasets
ulimit -s unlimited
set -x
## direct temp files to scratch
#export TMPDIR=$LOCAL

# xsede directories
scratch=/pylon2/oc4ifip/bassim/
home=/home/bassim
backupdir=${scratch}/ganglia/trinity/trinity_out_dir_${SLURM_JOBID}
workdir=/dev/shm/trinity_out_dir_${SLURM_JOBID}
mkdir -p $workdir $backupdir
cd $workdir

# fastq raw files COMBINED (all R1 and all R2 files separately)
sense=$(find ${scratch}/ganglia/merged.trimmed -name "r*all.R1*q")
antisense=$(find ${scratch}/ganglia/merged.trimmed -name "r*all.R2*q")
#sense=$(find $scratch/ganglia/raw.reads -name "*R1*gz" | paste -s -d,)
#antisense=$(find $scratch/ganglia/raw.reads -name "*R2*gz" | paste -s -d,)

#############
# TRINITY
#############
JMb=3000G
bthreads=32
nthreads=32
heap=350G
gc=6
init=2G

#--normalize_by_read_set
#--normalize_max_read_cov 50
#--quality_trimming_params "LEADING:5 TRAILING:5 SLIDINGWINDOW:4:15 MINLEN:36"

Trinity --seqType fq --SS_lib_type FR --left ${sense} --right ${antisense}  --normalize_max_read_cov 50 --max_memory ${JMb} --CPU $nthreads --bflyCPU $bthreads --bflyHeapSpaceMax $heap --bflyHeapSpaceInit $init --bflyGCThreads $gc --min_contig_length 200 --output $workdir >& ${home}/trinity.${SLURM_JOBID}_output.log

mv $workdir/Trinity.fasta $backupdir
mv $workdir/Trinity.timing $backupdir
cd $workdir
perl -e 'for(<*>){((stat)[9]<(unlink))}'
rm -rf $workdir
#+END_SRC

* Get summary of the transcriptome content
** Detonate
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=1:ppn=16:native
#PBS -l walltime=10:00:00
#PBS -N detonate
#PBS -e detonate.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

# set stack to unlimited
# because of large datasets
ulimit -s unlimited
# echo stdout to output file
set -x
# xsede directories
oasis=/oasis/projects/nsf/sun108
scratch=/oasis/scratch/silo/temp_project
home=/home/silo
##################################
# output directories for trinity #
##################################
jobid=gg60
fileid=60p/gg.60p

########################
# Dont change anything #
########################
workdir=${scratch}/ganglia/detonate/trinity_stat_${jobid}/
mkdir -p ${workdir}
cd ${workdir}

# fastq raw files, reads
sense=${scratch}/ganglia/data/$fileid.R1.fastq
antisense=${scratch}/ganglia/data/$fileid.R2.fastq
target=$scratch/ganglia/omics/$jobid.contigs.fa

module load R
export PATH:"$PATH:/home/silo/detonate-1.10/rsem-eval"
export PATH=$PATH:/home/silo/bowtie2

# average length of transcipts
average=560

# memory used by samtools
JM=48

$home/detonate-1.10/rsem-eval/rsem-eval-calculate-score \
--seed 3471609 \
--samtools-sort-mem $JM \
--bowtie2 \
--strand-specific \
--num-threads 16 \
--time \
--paired-end \
$sense $antisense $target \
gg60 \
$average
#+END_SRC
** Bowtie
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=3:ppn=16,walltime=24:00:00
#PBS -N bowtie.all.rscf
#PBS -q long
#PBS -V

# set stack to unlimited
# because of large datasets
ulimit -s unlimited
# echo stdout to output file
set -x
# IACS directories
scratch=/gpfs/scratch/ballam
home=/gpfs/home/ballam
##################################
# output directories for trinity #
##################################
jobid=raw.all.rscf
fileid=all/raw.all

########################
# Dont change anything #
########################
workdir=${scratch}/ganglia/bowtie/trinity_stat_${jobid}/
mkdir -p ${workdir}
cd ${workdir}

# fastq raw files, reads
sense=${scratch}/raw/$fileid.R1.fastq
antisense=${scratch}/raw/$fileid.R2.fastq
target=$scratch/ganglia/assembled/$jobid.contigs.fa

# Run bowtie
$home/trinityrnaseq-2.1.1/util/bowtie_PE_separate_then_join.pl --seqType fq --SS_lib_type RF --left $sense --right $antisense --target $target --aligner bowtie -- -p 4 --all --best --strata -m 300 >& $home/bowtie_stats_${jobid}_namesorted.txt

# run trinity integrated stat algorithm
$home/trinityrnaseq-2.1.1/util/SAM_nameSorted_to_uniq_count_stats.pl $workdir/bowtie_out/bowtie_out.nameSorted.bam >& $home/bowtie_stats_${jobid}_namesorted.txt
#+END_SRC
* Gene expression
** Abundance of transcripts from raw reads
This will help remove false transcripts.
Install [[https://pachterlab.github.io/kallisto/download.html][Kallisto]] for fast analysis. (To run it with trinity add Kallisto to PATH) Or [[http://bio.math.berkeley.edu/eXpress/][eXpress]] for alignment base analysis (bowtie required).
#+BEGIN_SRC shell
wget https://github.com/pachterlab/kallisto/releases/download/v0.42.4/kallisto_linux-v0.42.4.tar.gz
#+END_SRC

Run kallisto or Salmon (both without =--aln_method=) and/or eXpress and count the transcript per million reads (TPM). For Bowtie-based alignment with eXpress.
#+BEGIN_SRC shell
#!/bin/bash
#SBATCH --partition=LM
#SBATCH --nodes=1
#SBATCH -t 48:00:00
#SBATCH --job-name="abundance"
#SBATCH --output="abundance.%j.%N.out" 
#SBATCH --export=ALL
#SBATCH --mail-user=sleiman.bassim@stonybrook.edu

## !! ##
# 20h for 26 eXpress datasets
module load trinity
module load java
module load bowtie
module load samtools

# CHANGE___FILE ID___METHOD eXpress kallisto salmon
transcriptome=salmon
method=salmon
lib=RF

## DONT___CHANGE
nthreads=64
pbs=$SLURM_JOBID
scratch=/pylon2/oc4ifip/bassim
home=/home/bassim
target=trinity_out_dir_$transcriptome
project=$scratch/ganglia/trinity/$target/abundance_${method}
reads=$scratch/ganglia/raw.reads
assembly=$scratch/ganglia/trinity/$target/Trinity.fasta
abundance=$home/trinityrnaseq-2.2.0/util/align_and_estimate_abundance.pl

time=$home/time
jobid=$transcriptome.$method.abundance
start=$(date); echo "Job started at: $start" > $time/$jobid.time

## Express uses bowtie, so its slow__ADD: --aln_method bowtie
## Kallisto is fast delivering short summary
#           --SS_lib_type $lib
#	    --aln_method bowtie \
for f in br gg
do
    for i in {1..24}
    do
	mkdir -p $project/$f$i
	perl $abundance --transcripts $assembly \
	    --SS_lib_type $lib --seqType fq \
	    --left $reads/$f/$f.${i}.R1.fastq.gz \
	    --right $reads/$f/$f.${i}.R2.fastq.gz \
	    --est_method $method \
	    --trinity_mode \
	    --thread_count $nthreads \
	    --output_dir $project/$f$i \
	    --output_prefix $f$i.$method \
	    --prep_reference
    done
done
end=$(date); echo "Job ended at: $end" >> $time/$jobid.time
#+END_SRC

Get the amount of transcripts from 0 TPM to 3000 TPM
#+BEGIN_SRC shell
for f in {0..3200..200}; do cat $output.tsv | awk -vf="$f" '{if($5>=f) print $0}' | wc -l; done
#+END_SRC

** Quantify assembled transcripts (R dependent)
Get differentially expressed genes. Compare shared transcripts and TPM between samples. If the script below is ran on a server an R module must be loaded first and =DESeq2= =limma= and =edgeR= installed.  Merge all gene expression profiles into one matrix. Get differentially expressed genes from the matrix. Install R packages from =Bioconductor=. Packages needed =edgeR, limma, DESeq2, ctc, Biobase, ROTS, and qvalue. Reproducibility-optimized test statistic for ranking genes (ROTS) is installed as following.
#+BEGIN_SRC shell
wget http://www.utu.fi/en/units/sci/units/math/Research/biomathematics/projects/Documents/ROTS_1.1.1.tar.tar 
R CMD INSTALL ROTS_1.1.1.tar.tar
#+END_SRC

This will run 6 different matrices for gene differential expression at 6 different p-value thresholds, 2 fold changes, for any alignment methods (eXpress, kallisto, salmon) and for R packages (edgeR, DESeq2, limma).
#+BEGIN_SRC shell
#!/bin/bash
#SBATCH --partition=LM
#SBATCH --nodes=1
#SBATCH -t 48:00:00
#SBATCH --job-name="degSalmon"
#SBATCH --output="deg.%j.%N.out"
#SBATCH --export=ALL
#SBATCH --mail-user=sleiman.bassim@stonybrook.edu

module load R

## CHANGE__PROJECT__ID
transcriptome=salmon

## DONT__CHANGE
version=trinityrnaseq-2.2.0
scratch=/pylon2/oc4ifip/bassim
home=/home/bassim
pbs=$SLURM_JOBID
target=trinity_out_dir_$transcriptome
project=$(find $scratch/ganglia/trinity/$target -name "abundance_*")
## Analyses
analyze=$home/$version/Analysis/DifferentialExpression/run_DE_analysis.pl
differential=$home/$version/Analysis/DifferentialExpression/analyze_diff_expr.pl
join=$home/$version/util/abundance_estimates_to_matrix.pl
TPM=$home/$version/util/misc/count_matrix_features_given_MIN_TPM_threshold.pl
FPKM=$home/$version/util/misc/count_features_given_MIN_FPKM_threshold.pl
prefix=trans_counts

# Get the alignment type and check if an abundance test is already done
if [ ! -z "$project" ]; then
    e=$(grep -oci "express" <(echo $project))
    k=$(grep -oci "kallisto" <(echo $project))
    s=$(grep -oci "salmon" <(echo $project))
    if [ "$e" == 1 ]; then
        method=eXpress
        files=$(find $project -name "results.xprs" | paste -s -d' ')
    elif [ "$k" == 1 ]; then
        method=kallisto
        files=$(find $project -name "abundance.tsv" | paste -s -d' ')
    elif [ "$s" == 1 ]; then
        method=salmon
        files=$(find $project -name "quant.sf" | paste -s -d' ')
    fi
else
    echo "An abundance test (abundance.sh) must be executed before running deg2.sh"
    scancel $pbs
fi

# Join gene counts between samples
cd $project
if [ ! -f $prefix.TPM.not_cross_norm.counts_by_min_TPM_$method ]; then
    perl $join --est_method $method --out_prefix $prefix --name_sample_by_basedir $files
    # merge matrices accross samples to get shared TPM scores
    perl $TPM $prefix.TPM.not_cross_norm > $prefix.TPM.not_cross_norm.counts_by_min_TPM_$method
    # merge matrices accross samples to get shared FPKM scores
    #perl $FPKM $prefix.TPM.not_cross_norm > $prefix.TPM.not_cross_norm.counts_by_min_FPKM_$method 
    else
    echo "Matrices have been already compiled"
fi

## Choose__matrices [i]
jobid[1]=tissue
jobid[2]=tissue-diet
jobid[3]=tissue-br
jobid[4]=tissue-gg
jobid[5]=tissue-br-females
jobid[6]=tissue-gg-females
jobid[7]=tissue-br-bucephalus

## Get differentially expressed genes
dir=$scratch/ganglia/trinity/$target/deg.$method.$pbs
mkdir -p $dir
for align in $method
do
    for Rpack in DESeq2 edgeR voom
    do
	for i in {1..7}
	do
	    for pval in {1..6}
	    do
		for cfold in {1..2}
		do
#	    project=$scratch/ganglia/trinity/$target/abundance_${method}
	    jobid=${jobid[${i}]}
	    matrix=$scratch/ganglia/trinity/matrix/$jobid.txt
	    contrast=$scratch/ganglia/trinity/matrix/contrast.$jobid

	    cd $project
	    $analyze --matrix $project/trans_counts.counts.matrix --method $Rpack --samples_file $matrix --output $dir/$Rpack.$align.$jobid.p$pval.c$cfold.$pbs --contrasts $contrast

	    cd $dir/$Rpack.$align.$jobid.p$pval.c$cfold.$pbs
	    $differential --matrix $project/trans_counts.TMM.EXPR.matrix -P 1e-$pval -C $cfold --samples $matrix
	        done
	    done
	done
    done
done

#--ROTS_B 250 --ROTS_K 1000


# Create a table for the number of differentially expressed genes
if [ -d "$dir" ]; then
cd $dir

for m in $method
do
    for i in DESeq2 edgeR voom
    do
        for t in tissue tissue-diet tissue-br tissue-gg tissue-br-females tissue-gg-females tissue-br-bucephalus
        do
            for p in {1..6}
            do
                for c in {1..2}
                do
                    for f in $i*$m*$t.p$p.c$c*
                    do
temp=summary.txt
final=summary.$method.$pbs.txt
summary=$scratch/ganglia/$target/$final
rm $final

# Get the number of genes per abundance test
cat ${f}/diffExpr*matrix.log2.dat | cut -f 1 >> raw.$m.$t.$p.$c
# count number of all and unique differentially expressed genes
all=$(grep "^TRINITY" raw.$m.$t.$p.$c | wc -l)
uniq=$(grep "^TRINITY" raw.$m.$t.$p.$c | sort - | uniq | wc -l)
paste <(printf "%s\n" "$f") <(printf "%s\n" "$all") <(printf "%s\n" "$uniq") >> $temp
# column names; trandform to tabulated format
cat $temp | sed -e 's/\./\t/g' -e '/\*/d' >> $summary
rm raw.$m.$t.$p.$c $temp
                    done
                done
            done
        done
    done
done

else 
    echo "A differential expression gene test must be executed first"
    scancel $pbs
fi
#+END_SRC

Approximate the number of transcripts.
#+BEGIN_SRC R
data = read.table("genes_matrix.TPM.not_cross_norm.counts_by_min_TPM", header=T)
plot(data, xlim=c(-100,0), ylim=c(0,100000), t='b')
filt_data = data[data[,1] > -100 & data[,1] < -10,] 
fit = lm(filt_data[,2] ~ filt_data[,1])
print(fit)
abline(fit, col='green', lwd=3)
#+END_SRC

** Get all differentially expressed gene IDs from R output 
#+CAPTION The different tests done for gene expression
| Alignment | Condition      | e-value | Fold change |
|-----------+----------------+---------+-------------|
| Kallisto  | tissue         |   10e-1 |         2^2 |
| eXpress   | tissue x diet  |   10e-2 |         2^1 |
|           | tissue gills   |   10e-3 |             |
|           | tissue ganglia |   10e-4 |             |
|           |                |   10e-5 |             |
|           |                |   10e-6 |             |

Get all gene IDs and output them without processing into file.
#+BEGIN_SRC shell
for f in *raw*; do cat ${f}/diffExpr*matrix.log2.dat >> $file | cut -f 1;done 
#+END_SRC


* Gene annotation
** Databases
Detailed and summarized [[https://github.com/neocruiser/Rstats/tree/master/nodule#gene-gene-interaction][here]]
*** Getting annotation hits from interpro scan
Alignment hits are in a =tsv= output. Described [[https://code.google.com/p/interproscan/wiki/OutputFormats][here]].
1. Protein Accession (e.g. P51587)
2. Sequence MD5 digest (e.g. 14086411a2cdf1c4cba63020e1622579)
3. Sequence Length (e.g. 3418)
4. Analysis (e.g. Pfam / PRINTS / Gene3D)
5. Signature Accession (e.g. PF09103 / G3DSA:2.40.50.140)
6. Signature Description (e.g. BRCA2 repeat profile)
7. Start location
8. Stop location
9. Score - is the e-value of the match reported by member database method (e.g. 3.1E-52)
10. Status - is the status of the match (T: true)
11. Date - is the date of the run
12. (InterPro annotations - accession (e.g. IPR002093) - optional column; only displayed if -iprscan option is switched on)
13. (InterPro annotations - description (e.g. BRCA2 repeat) - optional column; only displayed if -iprscan option is switched on)
14. (GO annotations (e.g. GO:0005515) - optional column; only displayed if --goterms option is switched on)
15. (Pathways annotations (e.g. REACT_71) - optional column; only displayed if --pathways option is switched on)


Check if all hits are annotated.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($10 == "F") print $0 }' | wc -l
#+END_SRC

Get the name of the databases that contain hits. And the total number of unfiltered hits.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ print $4 }' | sort - | uniq -c | sort -n
## output
     14 ProDom
     20 PIRSF
     37 TIGRFAM
    159 SMART
    314 Coils
    391 PRINTS
    783 Pfam
    788 SUPERFAMILY
    874 Gene3D
   1190 PANTHER
#+END_SRC

Get the number of hits per database at different e-values. Although the number of hits is filtered by evalue, it is not filtered by unique sequence entries. For example, a single contig translated in 6 different frames might be matched to 2 different domains because of 2 separate frames shifts.
#+BEGIN_SRC shell
## some databases dont include description of the accession number
## accession numbers are registered under columns $8 or $9
## so we must filter the $9 and $8 by evalue.
## $4 is correct for all
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($9<=.0000000001) print $4}' | sort - | uniq -c | sort -n
## and
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($8 <= .0000000001) print $4}' | sort - | uniq -c | sort -n

#+END_SRC

In interpro output 5 databases have the full number of columns (shown above) and 5 others dont. filtering should be separated if the options depend on the columns that come after the 4th.
Create a list for each set of database.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($8 <= .0000000001) print $4}' | sort - | uniq > db.without.acc.txt 

# AND
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($9 <= .0000000001) print $4}' | sort - | uniq > db.with.acc.txt
#+END_SRC

Use these lists to filter separately the contigs by evalue and the sequence length of alignment. =hint= the calculated =x= returns an absolute value of the equation =end position - start - position=. Negative numbers might occur if the alignment is on the opposite strand.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | grep -Fwf ./db.without.acc.txt - | awk '{if($8 <= 0.00000000000000001) print $0}' | awk '{x=$6-$7?$7-$6:$6-$7; if(x>=10) print $4 }' | sort - | uniq -c | sort -n

#AND 
cat A.interpro.all.tsv | sed 's/ /./g' | grep -Fwf ./db.with.acc.txt - | awk '{if($9 <= 0.00000000000000000001) print $0}' | awk '{x=$7-$8?$8-$7:$7-$8; if(x>=20) print $4 }' | sort - | uniq -c | sort -n
#+END_SRC
** Contig annotation with HMMER
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=80:ppn=16,walltime=8:00:00
#PBS -N hmm.BR.large
#PBS -q large
#PBS -V

scratch=/gpfs/scratch/ballam
home=/gpfs/home/ballam

hmmscan=/gpfs/home/ballam/hmmer-3.1b2-linux-intel-x86_64/binaries/hmmscan

## File names _CHANGE_
file=br100
input=$scratch/ganglia/peptides/$file.peptides.rscf.fa
output=$scratch/ganglia/pfam/$file.pfam.rscf.txt
db=$scratch/db/pfam/Pfam-A.hmm

## START HMMER _DONT CHANGE_
time=$home/time
jobid=hmmGG
start=$(date); echo "Job started at: $start" > $time/$file.$jobid.time

#### !!!!! ####
# its better to cut the original big file into smaller ones

$hmmscan --domtblout $output $db $input

end=$(date); echo "Job ended at: $end" >> $time/$file.$jobid.time
#+END_SRC

** Contig annotation with BLAST+
Download NR, NT, and SwissProt databases from NCBI. Either the fasta-one-file database from the NCBI [[ftp://ftp.ncbi.nlm.nih.gov/][ftp]] or use the perl module below to download an already indexed database. The fasta-one-file needs to be loaded in =makeblastdb= to index it.

=note= Download gene accession numbers ([[ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/][here]]) in case =taxdb= didnt work.

Update databases, extract data and index.
#+BEGIN_SRC shell
perl $BLAST/bin/update_blsatdb.pl nt
for f in *.tar; do tar xzvf $f; done
makeblastdb -in nt.fasta -out nt -dbtype nucl -parse_seqids -max_file_sz 2GB
#+END_SRC

Set the database path.
#+BEGIN_SRC shell
export BLASTDB="/media/sf_data/db/nr"
#+END_SRC

Or write path in login profile.
#+BEGIN_SRC shell
cat >> ~/.profile
BLASTDB=/media/sf_data/db:$BLASTDB; export BLASTDB
BLASTDB=/media/sf_data/db/nr:$BLASTDB; export BLASTDB
#+END_SRC

*** Transcriptome quality assessment with blast and Swissprot
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=2:ppn=16,walltime=24:00:00
#PBS -N NRblast.A
#PBS -q long
#PBS -V

# Files _CHANGE_
file=A.noClam.e5.True2Ref
exe=fa
extra=nodule/assembled/final
db=nr
ev=1e-20
maxT=1

scratch=/gpfs/scratch/$user
home=/gpfs/home/$user
project=$home/ganglia/blast
tophit=$home/trinityrnaseq-2.1.1/util/analyze_blastPlus_topHit_coverage.pl
mkdir -p $project
###############
# DONT CHANGE #
###############
# blast libraries
export PATH="$PATH:/gpfs/home/$user/ncbi-blast-2.2.31+/bin"
export BLASTDB="/gpfs/scratch/$user/db/swissprot"
# supercomputing power
nthreads=48
# blast output format index
n=6

blastx \
-db $db \
-query $scratch/$extra/$file.$exe \
-out $project/$file.$db.$ev.outfmt$n \
-evalue $ev \
-num_threads $nthreads \
-max_target_seqs $maxT \
-outfmt $n

perl $tophit \
$project/$file.$db.$ev.outfmt$n \
$scratch/$extra/$file.$exe \
$scratch/db/$db/$db \
>& $project/$file.$db.$ev.tophit
#+END_SRC
*** Splitting a FASTA file into multiple smaller files
Use a fasta file first to count the number of sequences. Its best if the files are cut in increment of 1 (easier to automate).
#+BEGIN_SRC shell
echo "$(grep "^>" $file.fa | wc -l) / 8" | bc
time awk -vf="filenames" -vn="100000" 'BEGIN {n_seq=0;} /^>/ {if(n_seq%n==0){file=sprintf(f"%d.fa",n_seq);} print >> file; n_seq++; next;} { print >> file; }' < $file.fa
#+END_SRC
*** Blast any database (NR, NT, Swissprot, String)
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=2:ppn=16,walltime=42:00:00
#PBS -N strBblx.DEtistp4
#PBS -j oe
#PBS -q long
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

# DONT CHANGE #
###############
scratch=/gpfs/scratch/ballam
home=/gpfs/home/ballam
project=$scratch/ganglia/blast
mkdir -p $project
pbs=$(echo $PBS_JOBID | cut -f 1 -d '.')
# supercomputing power
nthreads=$(expr 2 \* 16)

# Files _CHANGE_
db=string
blast=blastx
maxSeq=1
#file=raw.all.nt  ##< Used for when splitting big contig file for parallel queues
#exe=900000.fa
#query=$scratch/ganglia/$file.split.fasta/${file}.$exe

p=4
c=2
file=DESeq2.raw.all.eXpress.tissue.p$p.c$c
log=$scratch/ganglia/deg.raw.all/$file/diffExpr.P1e-${p}_C${c}.matrix.log2.dat
assembled=$scratch/ganglia/assembled/raw.all.rscf.contigs.fa
# get gene ids and gene fasta sequences
tmp=$project/tmp_$blast.$pbs
mkdir $tmp
cat $assembled | sed 's/.len*$//g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' <(cat $log | cut -f1 | grep "^TRINITY" | sort - | uniq) - > $tmp/$file.contigs.$pbs.fa

query=$tmp/$file.contigs.$pbs.fa
output=$project/$file.$db.$blast.$pbs.txt    

# blast libraries
export PATH="$PATH:/gpfs/home/ballam/ncbi-blast-2.2.31+/bin"
export BLASTDB="/gpfs/scratch/ballam/db/$db"

## Full blast
time=$home/time
jobid=$file.$db
start=$(date); echo "Job started at: $start" > $time/$jobid.time

cd $scratch/db/$db

$blast -query $query -db $db -outfmt " 7 qseqid qlen sseqid slen qstart qend sstart send evalue bitscore length pident nident mismatch gaps " -max_target_seqs $maxSeq -num_threads $nthreads -out $output

rm -r $tmp
end=$(date); echo "Job ended at: $end" >> $time/$jobid.time
#+END_SRC
*** Get gene annotations from NCBI accession IDs
Access NCBI database at =ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/=
Get NCBI annotation with GI ids. =gene2accession= is a daily updated file from NCBI.
#+BEGIN_SRC shell
cat <(grep "^TRINITY" $annotated_trx_by_blast) | grep -Ff <(cat $log_data | awk 'NR>1{print $1}' | sort - | uniq) - | cut -f3 | cut -f2 -d "|" | sort - | uniq | grep -Fwf - gene2accession > $output
#+END_SRC

*** Sequence homology analysis
How many assembled contigs have been aligned to a SWISSPROT entry (NCBI) with a minimum of 10e-10 evalue, 80% sequence similarity, and 1 mismatch. Repeat for NT and NR. Only done on Blast output not hummer.
#+BEGIN_SRC shell
cat A.swissprot.txt | grep "^GG" | awk '{if ($9 <= 0.0000000001) print $0}' | awk '{if ($12 >= 80) print $0}' | awk '{if ($14 <= 1) print $0}' | cut -f 1 | sed 's/|.*$//g' | sort - | uniq | wc -l
#+END_SRC

How many differentially expressed genes are annotated (after whole transcriptome annotation with NT database)
#+BEGIN_SRC shell
cat <(grep "^TRINITY" $annotated_transcriptome | grep -Ff <(cat $log2_data | awk 'NR>1{print $1}' | sort - | uniq) - | wc -l
#+END_SRC

Get the differentially expressed gene description of Pfam domains
#+BEGIN_SRC shell
cat $pfam_annotated | grep -Ff <(cat $log2_data | awk 'NR>1{print $1}') - | awk '{gene=""; for(i=23;i<=NF;i++){gene=gene" "$i}; print $1"\t",gene}' | sort - | uniq | wc -l
#+END_SRC
** Contig annotation with InterPro
Databases used =ProDom PANTHER TIGRFAM SUPERFAMILY PRINTS Gene3D PIRSF Pfam Coils SMART=

#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=2:ppn=16,walltime=42:10:00
#PBS -N ips.DEStisDietp5c2
#PBS -j oe
#PBS -q long
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

# DONT CHANGE #
scratch=/gpfs/scratch/ballam
home=/gpfs/home/ballam
ips=$scratch/db/ips/interproscan-5.16-55.0
pbs=$(echo $PBS_JOBID | cut -f 1 -d '.')

project=${scratch}/ganglia/interpro
tmp=$project/temp_$pbs
mkdir -p $tmp


# Files _CHANGE_
p=5
c=2
file=DESeq2.raw.all.eXpress.tissue.diet.p$p.c$c
log=$scratch/ganglia/deg.raw.all/$file/diffExpr.P1e-${p}_C${c}.matrix.log2.dat
assembled=$scratch/ganglia/assembled/raw.all.rscf.contigs.fa
contigs=$tmp/$file.contigs.fa
peptides=$tmp/$file.peptides.fa

# Get gene ids (only differentially expressed)
# Get gene sequences
cat $assembled | sed 's/.len.*$//g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' <(cat $log | cut -f1 | grep "^TRINITY" | sort - | uniq) - > $contigs
# Translate
transeq $contigs $peptides -frame=6 --clean=yes
# Run interpro Scans
$ips/interproscan.sh -t p \
-i $peptides \
-iprlookup -goterms --pathways \
-f TSV, SVG, GFF3, XML, HTML \
--tempdir $tmp -d $project
#+END_SRC

* Network inference
** Gene-gene interactions
*** STRING networks =database=
The pipeline goes like this:
1. Align contigs to STRING (protein sequences file)
2. Get contigs and string IDs from =blastx= output
3. Get string networks (protein links file)
4. Get string actions (protein actions file)
5. Get species ID (second column of protein sequences file)
6. Get COG IDs (COG mappings file)
7. Get protein name (COG mapping file)
8. Get COG links to other orthologous groups (COG links file)

Get full networks from =protein.links.full.v10= String file (private) with differentially expressed genes and annotated with =blastsx= at =e-val 10^-5=.
#+BEGIN_SRC shell
time cat protein.links.full.v10.txt | grep -Ff <(cat $blastx_output | grep "^TRINITY" | awk '{if($9<=0.00001)print$0}' | cut -f3 | sort - | uniq) - | wc -l
#+END_SRC

From the same blast output get the =COG= network (Clusters of Orthologous proteins) from =COG.links.detailed.v10= String file.
#+BEGIN_SRC shell
time cat COG.links.detailed.v10.txt | grep -Ff <(cat $blastx_output | awk '{print$4}' | sort - | uniq) - | wc -l
#+END_SRC

*** Weighted matrix =R=
** Data mining
