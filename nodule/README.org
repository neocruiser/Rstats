#+TITLE: Interplay between the parasite and host immune defense systems
#+AUTHOR: Sleiman Bassim, PhD
#+EMAIL: slei.bass@gmail.com

#+STARTUP: content
#+STARTUP: hidestars
#+OPTIONS: toc:5 H:5 num:3
#+LANGUAGE: english
#+LaTeX_HEADER: \usepackage[ttscale=.875]{libertine}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \sectionfont{\normalfont\scshape}
#+LaTeX_HEADER: \subsectionfont{\normalfont\itshape}
#+LATEX_HEADER: \usepackage[innermargin=1.5cm,outermargin=1.25cm,vmargin=3cm]{geometry}
#+LATEX_HEADER: \linespread{1}
#+LATEX_HEADER: \setlength{\itemsep}{-30pt}
#+LATEX_HEADER: \setlength{\parskip}{0pt}
#+LATEX_HEADER: \setlength{\parsep}{-5pt}
#+LATEX_HEADER: \usepackage[hyperref]{xcolor}
#+LATEX_HEADER: \usepackage[colorlinks=true,urlcolor=SteelBlue4,linkcolor=Firebrick4]{hyperref}
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport


This git repository includes [[https://github.com/neocruiser/Rstats/blob/master/nodule/README.org][the pipeline]] for RNA-seq data processing and sequence annotation (in =org= format for emacs. Can be imported as Markdown or viewed below), [[https://github.com/neocruiser/Rstats/blob/master/nodule/nodule.Rnw][the R code]] for graphing and data table management (in LaTeX-knitr format. Can be imported with RStudio), [[https://github.com/neocruiser/Rstats/blob/master/nodule/nodule.pdf][the PDF]] of the compiled R file, [[https://github.com/neocruiser/Rstats/tree/master/nodule/data][the data]] used for generating the figures, and [[https://github.com/neocruiser/Rstats/tree/master/nodule/scripts][the scripts]] (in bash) for RNA-seq reads pre-processing (detailed in the tutorial below).

* Aims and resulting headlines
1. Gene expression RNA-seq data were generated on infectious nodules (QPX + hard clam tissue), non-nodule tissue from infected animal, and tissue from non-infected clam
2. Hypothesis 1: The gene-for-gene concept associates every host resistance gene with a pathogen avirulence gene
3. First goal is to identify (QPX) parasite transcripts produced in clam tissue during infection
4. Published work on this data includes: analysis on most abundant transcripts in the nodules (e.g. clam transcripts) while discarding QPX transcripts.
5. Pipeline was constructed to include 2 filtering phases which helps discard non-QPX reads and contigs
6. Preliminary results include discovery of over 1400 proteins for QPX.
7. The clam genome or transcriptome are used in read assembly to identify and discard dominant clam genes.
8. Annotation of discovered contigs is done on 21 different public databases that includes nucleotide and protein sequences.
9. Second goal is to identify gene-for-gene interplay between the host and pathogen
10. Cluster analysis by gene expression patterns between conditions can highlight gene-for-gene interplay. However the low number of available samples will increase the statistical bias.
11. Alternative approach is to use the available information on protein families to trace connection between subfamilies, processes, and functions.
12. Search for mobility (flagellate) and secretion (outside and transmembrane) genes.
13. Scrap data from google scholar.

* Quality controls
1. Download FastQC =on linux=
2. Windows users download from [[http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/][here]]
3. Java simulation of Q/C
#+BEGIN_SRC shell
curl -O http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/fastqc_v0.11.2.zip
7z x fastqc_v0.11.2.zip
cd FastQC
chmod 755 fastqc
sudo ln -s /path/to/FastQC/fastqc /usr/local/bin/fastqc
#+END_SRC

Load =FastQC= directly or in the shell. (every line is an option)
#+BEGIN_SRC shell
fastqc & # open a GUI
fastqc <file>.txt
zcat file1.fastq.gz | fastqc file1.fastq.gz # stream the content of gz files
#+END_SRC

Have a fastq.gz of the sequences. Run fastqc. Results are outputed in html format.
#+BEGIN_SRC shell
./fastqc <file.fasta>
Firefox report.html
#+END_SRC

Run quality controls on A1 A2 A3 samples
#+BEGIN_SRC shell
zcat 1_Index_1.A1_R1.fastq.gz | fastqc 1_Index_1.A1_R1.fastq.gz && \
zcat 1_Index_2.A2_R1.fastq.gz | fastqc 1_Index_2.A2_R1.fastq.gz && \
zcat 2_Index_8.A3_R1.fastq.gz | fastqc 2_Index_8.A3_R1.fastq.gz && \
zcat HI.0615.001.Index_1.A1_R2.fastq.gz | fastqc HI.0615.001.Index_1.A1_R2.fastq.gz && \
zcat HI.0615.001.Index_2.A2_R2.fastq.gz | fastqc HI.0615.001.Index_2.A2_R2.fastq.gz && \
zcat HI.0615.002.Index_8.A3_R2.fastq.gz | fastqc HI.0615.002.Index_8.A3_R2.fastq.gz
#+END_SRC

* Trimming, mapping, sorting by reference position, duplicate removal
Run bwa over reference genome of QPX for every paired samples (A1 A2 A3). Forget not to index the reference with =bwa index= before mapping. Additional tools needed are HTSeq for sequence count (for reference) and samtools for conversion of sam bam files, indexing, removing duplications, and sorting reads (for samples). Trimming is done on paired ends on sequences sampled from the A1 A2 A3 data. Sequencer is Illumina HiSEQ. Very important to choose the adapter sequences. The adapters that have been used here are saved under =TrueSeq3-PE-3.fa=. The code below allows a batch processing of all 9 sequenced samples As, Bs, and Cs.

#+BEGIN_SRC shell
wget http://downloads.sourceforge.net/project/bio-bwa/bwakit/bwakit-0.7.12_x64-linux.tar.bz2
#+END_SRC

#+BEGIN_SRC shell
#! /user/bin/bash
:'
this script accomplishes 6 things:
0. trimm reads
1. map all paired end samples to reference with bwa
2. sort the mapped reads with picard
3. remove duplicate reads with picard
4. index reads with samtools
5. count reads with htseq
'
sample[1]=B1
sample[2]=B2
sample[3]=B3
ir=/media/madl/windows/analysis/nodule
irr=/media/madl/windows/analysis/nodule/trimmed
dir=/media/madl/windows/analysis/nodule/mapping_clam
ddir=/media/madl/windows/analysis/nodule/rmdup_clam

stats=/media/madl/windows/analysis/nodule/stats_clam
#count=/media/sf_data/genomeSRv015/QPX_v015.gff3

mkdir $dir $ddir $stats

extension=.trimmed.P.fastq.gz
reference=/media/madl/windows/analysis/nodule/clam/clam_60p_trinity.fa
## essential for calling SNPs
RG[1]='@RG\tID:noduleA1\tSM:MA\tPL:illumina\tLB:noduleA1\tPU:transcriptome'
RG[2]='@RG\tID:noduleA2\tSM:MA\tPL:illumina\tLB:noduleA2\tPU:transcriptome'
RG[3]='@RG\tID:noduleA3\tSM:MA\tPL:illumina\tLB:noduleA3\tPU:transcriptome'

# Trim reads
for i in 1 2 3
do
    sample=${sample[${i}]}
    time java -Xmx10g -jar /home/neo/data/Trimmomatic-0.33/trimmomatic-0.33.jar PE \
        ${ir}/${sample}R1.fastq.gz \
        ${ir}/${sample}R2.fastq.gz \
        ${irr}/${sample}.1.trimmed.P.fastq.gz \
        ${irr}/${sample}.1.trimmed.U.fastq.gz \
        ${irr}/${sample}.2.trimmed.P.fastq.gz \
        ${irr}/${sample}.2.trimmed.U.fastq.gz \
        ILLUMINACLIP:TrueSeq3-PE-3.fa:2:30:10 \
        SLIDINGWINDOW:4:15 \
        TRAILING:5 \
        CROP:70 \
        MINLEN:30

    rm -f ${irr}/${sample}.1.trimmed.U.fastq.gz
    rm -f ${irr}/${sample}.2.trimmed.U.fastq.gz

done


## create dictionary and index of reference
    time java -jar ~/picard-tools-1.140/picard.jar \
        CreateSequenceDictionary \
        R=${reference} \
        O=/media/madl/windows/analysis/nodule/clam/clam_60p_trinity.dict

    time samtools faidx ${reference}
    # important to include
    # when the script is ran in parallel only index once
    time bwa index ${reference}

## Map | Sort | remove duplicates
for i in 1 2 3
do
    sample=${sample[${i}]}
    RG=${RG[${i}]}
    time bwa mem -M \
        -R ${RG} \
        -p ${reference} \
        ${irr}/${sample}.1${extension} \
        ${irr}/${sample}.2${extension} \
    > ${dir}/${sample}.sam

    time java -Xmx10g -jar ~/picard-tools-1.140/picard.jar \
        SortSam \
        INPUT=${dir}/${sample}.sam \
        OUTPUT=${dir}/${sample}.sorted.bam \
        SORT_ORDER=coordinate

    time java -Xmx10g -jar ~/picard-tools-1.140/picard.jar \
        MarkDuplicates \
        INPUT=${dir}/${sample}.sorted.bam \
        OUTPUT=${ddir}/${sample}.nodup.bam \
        METRICS_FILE=${stats}/${sample}.dup.metrics \
        REMOVE_DUPLICATES=true \
        ASSUME_SORTED=true

#    htseq-count --format=bam \
#        --stranded=no \
#        --type=CDS --order=pos \
#        --idattr=Name ${ddir}/${sample}.nodup.bam ${count} \
#        > ${stats}/${sample}.htseq.counts.nodup.txt

done
#+END_SRC

When aligning to reference BWA will use its default value to consider 4 or fewer mismatch to a given read as a good score. Here I applied the default values of =4%=.

Display reads with =tview=. Press =?= for additional help inside tview.
#+BEGIN_SRC shell
samtools tview -d -H <file>.bam QPX_Genome_v017.fasta
#+END_SRC

Another lightweight tool for displaying alignments is =Tablet Viewer=. [[http://ics.hutton.ac.uk/tablet/][Link]] to download and manual.

Calculate the number of reads per sample. =htseq= is blazing fast and accurate.
#+BEGIN_SRC shell
time cat sample.htseq.counts.txt | awk '{s+=$2; print s}' | tail -n 1
## OR
time samtools view -c sample.bam
#+END_SRC

Get the number of mapped reads. [[https://broadinstitute.github.io/picard/explain-flags.html][Explain SAM flags]]
#+BEGIN_SRC shell
## mapped
samtools view -c -F 4 sample.bam
## unmapped
samtools view -c -f 4 sample.bam
#+END_SRC

Get the number of reads from paired ends where both the forward and reverse mate are mapped.
#+BEGIN_SRC shell
samtools view -c -f 0x02 -F 12 sample.bam
#+END_SRC

Get a summary on reads.
#+BEGIN_SRC shell
samtools flagstat sample.bam
#+END_SRC

* Genome-guided trinity transcriptome assembly
QPX can be considered as gene-dense genome. =--jacard-clip= can be used. In this case [[http://bowtie-bio.sourceforge.net/index.shtml][Bowtie]] have to be installed.

#+BEGIN_SRC shell
wget http://sourceforge.net/projects/bowtie-bio/files/bowtie/1.1.2/bowtie-1.1.2-linux-x86_64.zip
sudo ln -s /path/to/bowtie /usr/local/bin/bowtie
bowtie --help | less
#+END_SRC

Assemble reads that are filtered after mapping to reference genome. Those reads were trimmed, mapped, sorted, and duplicates removed from script in =trimmingNodules.sh=. =genome_guided_bam= (below) cannot take multiple bam files. If one has many replicates bam files can be merged together with =Picard MergeSamFiles= function. This step can be done after mapping with BWA to a reference or after Picard MarkDuplicates for discarding duplicate reads. =note= set the output to a destination that does not require root privileges. Merging 3 bam files takes 90 minutes. Assembling a 6Gb bam files can take up to 4h.
#+BEGIN_SRC shell
## Merge bam files for Trinity genome-guided assembly
#! /usr/bin/bash

dir=/media/sf_data/nodule/rmdup/
ddir=/home/neo/data/nodule/trinity

x=A1
y=A2
z=A3
b=A

    java -Xmx10g -jar /home/neo/data/picard/picard.jar \
        MergeSamFiles \
        I=${dir}${x}.nodup.bam \
        I=${dir}${y}.nodup.bam \
        I=${dir}${z}.nodup.bam \
        O=${dir}/${b}.bam \
        SO=coordinate \
        AS=true

/home/neo/data/QPX/trinityrnaseq/Trinity \
--genome_guided_bam ${dir}${b}.bam \
--genome_guided_max_intron 1000 \
--max_memory 10G \
--output ${ddir} \
--CPU 5
#+END_SRC

Check if bam file is sorted
#+BEGIN_SRC shell
samtools view -H file.bam | less
#+END_SRC

Calculate the N50 (1) and L50 (2) in bp.
#+BEGIN_SRC shell
cat mmetsp0098Cust.fasta | grep ">" | awk '{print $2}' | sed 's/len=//g' | sort -rn | awk '{sum += $0; print "N50:" $0"\t", sum}' | tac | awk 'NR==1 {halftot=$2/2} lastsize>halftot && $2<halftot {print} {lastsize=$2}'
#+END_SRC

Calculate the total size of contigs in bp.
#+BEGIN_SRC shell
cat mmetsp0098Cust.fasta | grep ">" | awk '{print $2}' | sed 's/len=//g' | awk '{sum+=$1}END{print "Total:", sum}out'
#+END_SRC

* Quantify assembled transcripts (R dependent)
This will help remove false transcripts.
Install [[https://pachterlab.github.io/kallisto/download.html][Kallisto]] for fast analysis. (To run it with trinity add Kallisto to PATH) Or [[http://bio.math.berkeley.edu/eXpress/][eXpress]] for alignment base analysis (bowtie required).
#+BEGIN_SRC shell
wget https://github.com/pachterlab/kallisto/releases/download/v0.42.4/kallisto_linux-v0.42.4.tar.gz
#+END_SRC

Run kallisto and count the transcript per million reads (TPM)
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=1:ppn=30
#PBS -l walltime=4:00:00
#PBS -N kallisto.all
#PBS -A sun108
#PBS -j oe
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V
module load R

nthreads=160

method=kallisto

# Files _CHANGE_
scratch=/crucible/oc48n1p/bassim
home=/home/bassim
project=$scratch/ganglia/abundance_${file}_$method
reads=$scratch/ganglia/trimmed
exe=P.fastq.gz

file=raw.all
assembly=$scratch/ganglia/assembled/${file}.rscf.contigs.fa
abundance=$home/trinity2/util/align_and_estimate_abundance.pl

time=$home/time
jobid=kallisto.abundance
start=$(date); echo "Job started at: $start" > $time/$file.$jobid.time
# run the alignment/estimation
for f in br gg
do
    for i in {1..24}
    do
	mkdir -p $project/$file$i
	
	perl $abundance --transcripts $assembly \
	    --SS_lib_type RF --seqType fq \
	    --left $reads/$f.${i}.R1.$exe \
	    --right $reads/$f.${i}.R2.$exe \
	    --est_method $method \
	    --trinity_mode \
	    --thread_count $nthreads \
	    --output_dir $project/$file$i \
	    --output_prefix $f$i.$method
  		--prep_reference    
    done
done

end=$(date); echo "Job ended at: $end" >> $time/$time$jobid.time
#+END_SRC


For Bowtie-based alignment with eXpress.
#+BEGIN_SRC shell
## Express uses bowtie, so its slow
## Kallisto is fast delivering short summary

method=eXpress
# run the alignment/estimation
for i in {1..2}
do
    perl $abundance --transcripts $assembly \
	--SS_lib_type RF --seqType fq \
	--left $reads/$file.${i}.R1.$exe \
	--right $reads/$file.${i}.R2.$exe \
	--est_method $method \
	--aln_method bowtie \
	--trinity_mode \
	--thread_count $nthreads \
	--output_dir $project \
	--output_prefix $file$i.$method \
	--prep_reference
done
#+END_SRC

Get the amount of transcripts from 0 TPM to 3000 TPM
#+BEGIN_SRC shell
for f in {0..3200..200}; do cat $output.tsv | awk -vf="$f" '{if($5>=f) print $0}' | wc -l; done
#+END_SRC

Compare shared transcripts and TPM between samples. If the script below is ran on a server an R module must be loaded first and =edgeR= installed. (Greenfield works)
#+BEGIN_SRC shell
matrix=$home/trinityrnaseq-2.1.1/util/abundance_estimates_to_matrix.pl
count=$home/trinityrnaseq-2.1.1/util/misc/count_matrix_features_given_MIN_TPM_threshold.pl

prefix=trans_counts

# Create matrix
cd $project

perl $matrix --est_method $method \
    --out_prefix $prefix \
    --name_sample_by_basedir \
     $project/br1/abundance.tsv \
     $project/br2/abundance.tsv \

# merge matrices accross samples to get shared TPM scores
perl $count $prefix.TPM.not_cross_norm > $prefix.TPM.not_cross_norm.counts_by_min_TPM_${file}_$method
#+END_SRC

Approximate the number of transcripts.
#+BEGIN_SRC R
data = read.table("genes_matrix.TPM.not_cross_norm.counts_by_min_TPM", header=T)
plot(data, xlim=c(-100,0), ylim=c(0,100000), t='b')
filt_data = data[data[,1] > -100 & data[,1] < -10,] 
fit = lm(filt_data[,2] ~ filt_data[,1])
print(fit)
abline(fit, col='green', lwd=3)

#+END_SRC
* Gene expression
Install R packages from =Bioconductor=. Packages needed =edgeR, limma, DESeq2, ctc, Biobase, ROTS, and qvalue. Reproducibility-optimized test statistic for ranking genes (ROTS) is installed as following.
#+BEGIN_SRC shell
wget http://www.utu.fi/en/units/sci/units/math/Research/biomathematics/projects/Documents/ROTS_1.1.1.tar.tar 
R CMD INSTALL ROTS_1.1.1.tar.tar
#+END_SRC

Get gene expression across biological conditions and samples
#+BEGIN_SRC shell
~/trinityrnaseq-2.1.1/Analysis/DifferentialExpression/run_DE_analysis.pl --matrix gg.kallisto.rscf.counts.matrix --method voom --samples_file gg.samples.txt
#+END_SRC

Extract differentially expressed genes
#+BEGIN_SRC shell
~/trinityrnaseq-2.1.1/Analysis/DifferentialExpression/analyze_diff_expr.pl --matrix ../gg.kallisto.TMM.EXPR.matrix -P 1e-1 -C 2 --samples ../gg.samples.txt
#+END_SRC

Full code to run on a server that includes both previous codes. This will run 4 different matrices for gene differential expression at 4 different p-value thresholds.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=3:ppn=16,walltime=48:00:00
#PBS -N deg4
#PBS -q long
#PBS -V

## Choose tissue br, gg, raw.all
file=raw.all

## DONT CHANGE
scratch=/gpfs/scratch/ballam
home=/gpfs/home/ballam

dir=$scratch/ganglia/deg4
analyze=$home/trinityrnaseq-2.1.1/Analysis/DifferentialExpression/run_DE_analysis.pl
differential=$home/trinityrnaseq-2.1.1/Analysis/DifferentialExpression/analyze_diff_expr.pl

## change matrices
jobid[1]=tissue
jobid[2]=tissue.diet
jobid[3]=tissue.br
jobid[4]=tissue.gg

## RUN R
## DONT CHANGE
mkdir -p $dir 

for method in kallisto eXpress
do
    for m in voom edgeR DESeq2
    do
  for i in {1..4}
  do
      for pval in {1..6}
      do
    for cfold in {1..2}
    do
      project=$scratch/ganglia/abundance_${file}_$method

      jobid=${jobid[${i}]}
      pval=${pval[${pval}]}

      matrix=$scratch/ganglia/matrix/$jobid.txt
      contrast=$scratch/ganglia/matrix/contrast.$jobid

      cd $project
      $analyze --matrix $project/trans_counts.counts.matrix --method $m --samples_file $matrix --output $dir/$m.$file.$method.$jobid.p$pval.c$cfold --contrasts $contrast

      cd $dir/$m.$file.$method.$jobid.p$pval.c$cfold
      $differential --matrix $project/trans_counts.TMM.EXPR.matrix -P 1e-$pval -C $cfold --samples $matrix
          done
      done
  done
    done
done
#+END_SRC

*** Get all differentially expressed gene IDs from R output 
#+CAPTION The different tests done for gene expression
| Alignment | Condition      | e-value | Fold change |
|-----------+----------------+---------+-------------|
| Kallisto  | tissue         |   10e-1 |         2^2 |
| eXpress   | tissue x diet  |   10e-2 |         2^1 |
|           | tissue gills   |   10e-3 |             |
|           | tissue ganglia |   10e-4 |             |
|           |                |   10e-5 |             |
|           |                |   10e-6 |             |

Get all gene IDs and output them without processing into file.
#+BEGIN_SRC shell
for f in *raw*; do cat ${f}/diffExpr*matrix.log2.dat >> $file | cut -f 1;done 
#+END_SRC

Get stats for each alignment, condition, e-value, and cFold.
#+BEGIN_SRC shell
for f in edgeR*4.cn; do cat ${f}/diffExpr*matrix.log2.dat | cut -f 1 >> raw; done; grep "^TRINITY" raw | wc -l; grep "^TRINITY" raw | sort - | uniq | wc -l; rm raw
#+END_SRC
* Gene Finding
** Contig annotation with HMMER
As a main strategy the functional annotation is done with HMMER, the alignment is based on hidden markov models that calculate posteriors to the similarity scores.
*** Library preparation
Download and Install HMMER
#+BEGIN_SRC shell
wget http://selab.janelia.org/software/hmmer3/3.1b2/hmmer-3.1b2-linux-intel-x86_64.tar.gz
./configure
sudo make
sudo make install
cd easel: sudo make install
#+END_SRC 

Download Pfam 28.0 database (as of 06/20/2015). It is possible to download the fasta database. But in this case an HMM profile must be built. The process will than take over 3 hours.
#+BEGIN_SRC shell
ftp ftp.ebi.ac.uk
anonymous
<<no password>>
cd pub/databases/Pfam/current_release/
get Pfam-A.hmm.gz
bye
gzip -d Pfam-A.hmm.gz
#+END_SRC

Index the Pfam.hmm database. this will produce 16,230 accessions.
#+BEGIN_SRC shell
hmmpress Pfam-A.hmm
#+END_SRC

=hmmscan= is a function used to search =Pfam-A.hmm= profiles. Otherwise if we had a sequence database =hmmsearch= would've been used. The query used is either a peptide or an HMM profile produced with =hmmbuild= or multiple HMM alignment profiles produced with =hmmalign= which generates a =stockholm= format alignment file. The stockholm file is then fed to hmmbuild to make an HMM query profile.

Pfam can be searched using keywords and =accession= numbers can be extracted with copy/paste into a txt file. Get the accession number from gene of interest.
*** Translate contigs to peptides
Using =Transeq= from Emboss. If an error occurs after the first =make install= try =ldconfig= then =make install= a second time. Make install can be replaced with =checkinstall= for creating a deb package that can be removed without =make uninstall=.
#+BEGIN_SRC shell
wget ftp://emboss.open-bio.org/pub/EMBOSS/old/6.5.0/EMBOSS-6.5.7.tar.gz
sudo apt-get install libplplot-dev
./configure --without-x
sudo make
sudo make install
#+END_SRC

Translate in 6 frames from fasta file. [[http://www.sacs.ucsf.edu/Documentation/emboss/transeq.html][Documentation]]
#+BEGIN_SRC shell
## correct name of each sequence. OPTIONAL
time transeq <(cat $contigs.fa | sed 's/|.*len/ len/g') $output.fa -frame=6
#+END_SRC

*** Annotating all peptides (pfam)
Annotation of the 4 strains peptides against a Pfam v28 updated database (June 2015). Here we have two choices, first option, annotate against the whole pfam library, second option, annotate against a subset of selected HMM profiles of PFAM (eg., virulence, temperature). The latter is mostly beneficial if one wants to extract =contig= number to find SNPs, the first being better for discovery. Refer to p.50 of the HMMER3 userguide.
#+BEGIN_SRC shelle
time hmmscan --domtblout $output.txt --cpu 6 $Pfam-A.hmm $peptides.fa
#+END_SRC

How many contigs have been HMMER annotated?
#+BEGIN_SRC shell
cat $file.fa | grep "^[^#]" | wc -l
#+END_SRC

Get contig IDs from HMMER results with minimum e-value of e-10
#+BEGIN_SRC shell
cat $hmmerOutput | grep "^[^#]" | awk '{if ($7<=0.0000000001) print $4}' | sort - | uniq | sed 's/_.$//g' | uniq > $ids
# Number of contigs (nb of genes and isoforms)
cat $hmmerOutput | grep "^[^#]" | awk '{if ($7<=0.0000000001) print $4}' | sort - | uniq | sed 's/_.$//g' | uniq | wc -l
# Number of peptides (1 contig with many translated frames)
cat $hmmerOutput | grep "^[^#]" | awk '{if ($7<=0.0000000001) print $4}' | sort - | uniq | sed 's/.len.*$//g' | uniq | wc -l
# Number of genes (without isoforms created with trinity)
cat $hmmerOutput | grep "^[^#]" | awk '{if ($7<=0.0000000001) print $4}' | sort - | uniq | sed 's/.c.*$//g' | uniq | wc -l
#+END_SRC

Get nucleotide sequences from fasta file that contains all contigs annotated in HMMER. We take the contig not the isoforms because not all isoforms are annotated.
#+BEGIN_SRC shell
cat $trinityOutput.fa | sed 's/.len.*$//g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' $ids - > $pfam.e10.fa
#+END_SRC

Compare =$ids= and =$pfam.e10.fa= to get if any sequences are missing
#+BEGIN_SRC shell
## show only entries unique to $ids
comm -23 <(sort $ids) <(cat $pfam.e10.fa | grep "^>" | sed 's/^>//g' | sort -) | wc -l
#+END_SRC

*** BLAT
Blat can be found also on xsede. [[http://genome.ucsc.edu/goldenPath/help/blatSpec.html][Documentation]] and [[http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/blat/][Blat linux packages]]
Installation.
#+BEGIN_SRC shell
ftp hgdownload.cse.ucsc.edu
Name: anonymous
cd admin/exe/linux.x86_64/blat
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/faToTwoBit
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslSort
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslReps
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslPretty
wget http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/pslStats
chmod +x ./blat
chmod +x ./faToTwoBit
## OR
git clone https://github.com/neocruiser/blat.git
#+END_SRC

Convert the genome to =2bit= (faster). This step will index the genome and place it in the computer memory for fast pulling of alignments.
#+BEGIN_SRC shell
./faToTwoBit ../genomev015/QPX_v015.fasta ../genomev015/qpxv15.2bit
#+END_SRC

Align RNAseq contigs to genome. =psl= is a tabulated output.
#+BEGIN_SRC shell
./blat ../genomev015/qpxv15.2bit ../db/A.contigs.pfam.fa output.test.psl
#+END_SRC

Show the alignment in a human readable format.
#+BEGIN_SRC shell
./pslPretty 2> pslpretty.README.txt
./pslPretty <psl file> <genome target 2bit> <query fa> <output.txt>
#+END_SRC

Get overall statistics.
#+BEGIN_SRC shell
./pslStats -overallStats <psl file> <output>
#+END_SRC

How many contigs were mapped to reference with 0--1 gaps and where at least one of both gaps align to 200 or more nucleotide of the reference.
#+BEGIN_SRC shell
cat $file.psl | awk 'NR>5' | awk '{print $10,$11,$15,$18,$19}'| awk '{if($4<=2) print$0}' | awk '{if($2>=250)print $0}' | sed 's/,/\t/g' |  awk '{if($5>=200 || $6>=200) print $0}' | wc -l
#+END_SRC

Extract clam contigs with max 4 gaps and at least 450 nucleotides aligned to reference for any of the blocks. These contigs are then mapped to QPX contigs. The common in both lists are considered then as clam genes.
#+BEGIN_SRC shell
cat $trinityOutput.fa | sed 's/.len.*$//g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' <(cat $file.psl | awk 'NR>5' | awk '{print $10,$11,$15,$18,$19}'| awk '{if($4<=5) print$0}' | awk '{if($2>=250)print $0}' | sed 's/,/\t/g' |  awk '{if($5>=450||$6>=450||$7>=450||$8>=450||$9>=450) print $1}' | sort - | uniq) - > $contigs.fa
#+END_SRC

Clean QPX contigs from i- clam contigs, ii- non mapped contigs to QPX reference (blat), and iii- pfam low e-value domains. Then extract fasta sequence of QPX-only genes.
#+BEGIN_SRC shell
## remove clam contigs
cat $trinity.fa | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' <(comm -23 <(grep "^>" $trinity.fa | sed 's/^>//g' | sed 's/.len.*$//g'| sort -) <(cat $QPXvsClam.blat.psl | awk 'NR>5' | awk '{print $10}' | sort - | uniq)) - > $noClam.fa
## remove pfam domains > e.10-5
cat $noClam.fa | sed 's/|.*$//g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' <(cat $hmmer.out | grep "^[^#]" | awk '{if ($7<=0.00001) print $4}' | sort - | uniq | sed 's/_.$//g' | uniq) - $noClam.e5.fa
## remove non mapped to QPX reference genome w/ max 4 gaps, min 100 Query size, min 250 blocksize
cat $noClam.e5.fa | sed 's/.len.*$//g' | perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' <(cat $QPX.blat.psl | awk 'NR>5' | awk '{print $10,$11,$15,$18,$19}'| awk '{if($4<=5) print$0}' | awk '{if($2>=100)print $0}' | sed 's/,/\t/g' |  awk '{if($5>=250||$6>=250||$7>=250||$8>=250||$9>=250) print $1}' | sort - | uniq) - > $noClam.e5.True2Ref.fa
#+END_SRC

** BLAST+
Download NR, NT, and SwissProt databases from NCBI. Either the fasta-one-file database from the NCBI [[ftp://ftp.ncbi.nlm.nih.gov/][ftp]] or use the perl module below to download an already indexed database. The fasta-one-file needs to be loaded in =makeblastdb= to index it.

=note= Download gene accession numbers ([[ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/][here]]) in case =taxdb= didnt work.

Update databases, extract data and index.
#+BEGIN_SRC shell
perl $BLAST/bin/update_blsatdb.pl nt
for f in *.tar; do tar xzvf $f; done
makeblastdb -in nt.fasta -out nt -dbtype nucl -parse_seqids -max_file_sz 2GB
#+END_SRC

Set the database path.
#+BEGIN_SRC shell
export BLASTDB="/media/sf_data/db/nr"
#+END_SRC

Or write path in login profile.
#+BEGIN_SRC shell
cat >> ~/.profile
BLASTDB=/media/sf_data/db:$BLASTDB; export BLASTDB
BLASTDB=/media/sf_data/db/nr:$BLASTDB; export BLASTDB
#+END_SRC

Blastx. Use nucleotide query and blast will translate it in 6 frames. Use Transeq only if using hmmer
#+BEGIN_SRC shell
blastx -query nodule/assembled/C.assembl.QPXgv5.fasta \
-db nr \
-outfmt "7 qseqid qlen sseqid slen qstart qend sstart send evalue bitscore length pident nident mismatch gaps staxids sscinames " \
-max_target_seqs 10 \
-out output.txt \
-num_threads 16
#+END_SRC

Blast sequence similarity analysis are done with NR (protein), NT (nucleotide), SWISSPROT (protein).

*** Transcriptome quality control with Blast
The script below shows how many contigs can be found in ncbi libraries nr, nt, swissprot.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -l nodes=2:ppn=16,walltime=24:00:00
#PBS -N NRblast.A
#PBS -q long
#PBS -V

# Files _CHANGE_
file=A.noClam.e5.True2Ref
exe=fa
extra=nodule/assembled/final
db=nr
ev=1e-20
maxT=1

scratch=/gpfs/scratch/$user
home=/gpfs/home/$user
project=$home/ganglia/blast
tophit=$home/trinityrnaseq-2.1.1/util/analyze_blastPlus_topHit_coverage.pl
mkdir -p $project
###############
# DONT CHANGE #
###############
# blast libraries
export PATH="$PATH:/gpfs/home/$user/ncbi-blast-2.2.31+/bin"
export BLASTDB="/gpfs/scratch/$user/db/swissprot"
# supercomputing power
nthreads=48
# blast output format index
n=6

blastx \
-db $db \
-query $scratch/$extra/$file.$exe \
-out $project/$file.$db.$ev.outfmt$n \
-evalue $ev \
-num_threads $nthreads \
-max_target_seqs $maxT \
-outfmt $n

perl $tophit \
$project/$file.$db.$ev.outfmt$n \
$scratch/$extra/$file.$exe \
$scratch/db/$db/$db \
>& $project/$file.$db.$ev.tophit
#+END_SRC
*** Sequence homology analysis
How many assembled contigs have been aligned to a SWISSPROT entry (NCBI) with a minimum of 10e-10 evalue, 80% sequence similarity, and 1 mismatch. Repeat for NT and NR. Only done on Blast output not hummer.
#+BEGIN_SRC shell
cat A.swissprot.txt | grep "^GG" | awk '{if ($9 <= 0.0000000001) print $0}' | awk '{if ($12 >= 80) print $0}' | awk '{if ($14 <= 1) print $0}' | cut -f 1 | sed 's/|.*$//g' | sort - | uniq | wc -l
#+END_SRC

** Gene-gene interaction
*** Databases

#+CAPTION: Sequence databaases in public repositiories
| database    | content    | tool      | function            |   | description                |
|-------------+------------+-----------+---------------------+---+----------------------------|
| [[http://pfam.xfam.org/][Pfam]]        | protein    | hmmer/IPS | domain              |   | protein similarities       |
| [[ftp://ftp.ncbi.nih.gov/blast/db/][NR]]          | protein    | blast     | domain              |   | protein similarities       |
| [[ftp://ftp.ncbi.nih.gov/blast/db/][NT]]          | nucleotide | blast     | classification      |   | phylogeny                  |
| [[ftp://ftp.ncbi.nih.gov/blast/db/][Swiss-prot]]  | protein    | blast     | domain              |   | protein similarities       |
| [[http://www.phi-base.org/release_notes.php][Phi-base]]    | protein    | hmmer     | interactions        |   | virulence                  |
| [[http://www.mgc.ac.cn/VFs/main.htm][VFDB]]        | prot/nucl  | blast     | virulence           |   |                            |
| [[http://string-db.org/newstring_cgi/show_download_page.pl?UserId%3D_yWWhZVtWw47&sessionId%3DDpBbN0jI1blB][STRING]]      | protein    | blast     | interactions        |   | network analysis           |
| [[http://thebiogrid.org/][BioGRID]]     |            | shell     | interactions        |   |                            |
| [[http://www.transcriptionfactor.org/index.cgi?Download][DBD]]         | protein    | shell     | transcrption factor |   | acc. Pfam+superfamily      |
| [[http://operondb.jp/][OperonDB]]    |            |           | operons             |   |                            |
| [[http://www.ncbi.nlm.nih.gov/COG/][COG]]         | protein    |           | classification      |   | phylogeny                  |
| [[http://phospho.elm.eu.org/][Pospho-elm]]  | protein    | shell     | phosphorylation     |   | acc Uniprot+EnsEMBL        |
| [[http://www.jcvi.org/cgi-bin/tigrfams/index.cgi][TIGRFAM]]     | protein    | blast/IPS | subfamilies         |   | classification             |
| [[http://prodom.prabi.fr/prodom/current/html/home.php][ProDom]]      | protein    | IPS       | families            |   | uniprot domains            |
| [[http://hamap.expasy.org/][PANTHER]]     | protein    | IPS       | classification      |   | domain+pathways            |
| [[http://supfam.cs.bris.ac.uk/SUPERFAMILY/][SUPERFAMILY]] | protein    | IPS       | classification      |   | domain+phylog+taxon        |
| [[http://www.bioinf.manchester.ac.uk/dbbrowser/PRINTS/index.php][PRINTS]]      | protein    | IPS       | fingerprinting      |   | conserved motifs SwissProt |
| [[http://pir.georgetown.edu/pirwww/dbinfo/pirsf.shtml][PIRSF]]       | protein    | IPS       | phylogeny           |   | domain classification      |
| [[http://gene3d.biochem.ucl.ac.uk/Gene3D/][Gene3D]]      | protein    | IPS       | interactions        |   | domain families            |
| [[http://www.ch.embnet.org/software/COILS_form.html][Coils]]       | protein    | IPS       | domains             |   | coiled-coil conformation   |
| [[http://smart.embl-heidelberg.de/][SMART]]       | protein    | IPS       | domains             |   | SwissProt Trembl Ensembl   |
| [[http://prosite.expasy.org/][PROSITE]]     | not used   | IPS       | domains             |   | +functions                 |
| [[http://hamap.expasy.org/][HAMAP]]       | not used   | IPS       | classification      |   | uniprot classification     |

- Known and predicted protein-protein interactions [[http://string-db.org/newstring_cgi/show_download_page.pl?UserId%3D_yWWhZVtWw47&sessionId%3DDpBbN0jI1blB][STRING]]. Protein database. Searched with =blastx=. Indexed with =makeblastdb= but without =-parse_seqids= because its a network data. Proteins have duplicate seq ids. Download STRING alias id text file that include convectional protein names.
- [[ftp://ftp.jcvi.org/pub/data/TIGRFAMs/][TIGRFAM]] complete listings of functionally defined subfamilies. Database with multiple sequence alignments. To be used with =hmmer=. Use this script =find . -name "TIGR*" -exec cat {} > tigrfam.hmm \;= to create one hmm model. Database is searched with =hmmscan=.
- InterPro database for domains, GO terms, families. Downloading =interproscan= will also acquire hmm databases for =Gene3D= =HAMAP= =PIRSF= =PRINTS= =PRODOM= =PROSITE= =SMART= =SUPERFAMILY= =TIGRFAM=. Download and setup interproscan [[https://code.google.com/p/interproscan/wiki/HowToDownload][here]]. Download the database and GO terms not =interproscan=. 
#+BEGIN_SRC shell
ftp ftp.ebi.ac.uk
cd pub/databases/interpro/Current
get interpro2go
get entry.list
get names.dat 
get short_names.dat
get protein2ipr.dat.gz
#+END_SRC

Interpro scan. =Note= each analyzed sequence generates =SVG= output files. These files are gunzipped together. If protein is being analyzed the peptide sequence must not contain any special characters. Characters are usually due to =stop codons=. If =transeq= is being used to translate nucleotide sequences into peptides, use =-trim= function to replace all =*= with =X=.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=5:ppn=16:native
#PBS -l walltime=01:00:00
#PBS -N interproscan.C
#PBS -e interproscan.C.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

oasis=/oasis/projects/nsf/sun108

${oasis}/silo/interproscan/interproscan.sh \
-t p \
-appl ProDom,PANTHER,TIGRFAM,SUPERFAMILY,PRINTS,Gene3D,PIRSF,Pfam,Coils,SMART \
-i ${oasis}/silo/nodule/peptides/C.peptides.QPXv15.fa \
-iprlookup \
-goterms \
-pa \
-f TSV, SVG, GFF3, XML \
-b ${oasis}/silo/nodule/interpro/C/C.interpro.all
#+END_SRC

*** Getting annotation hits from interpro scan
Alignment hits are in a =tsv= output. Described [[https://code.google.com/p/interproscan/wiki/OutputFormats][here]].
1. Protein Accession (e.g. P51587)
2. Sequence MD5 digest (e.g. 14086411a2cdf1c4cba63020e1622579)
3. Sequence Length (e.g. 3418)
4. Analysis (e.g. Pfam / PRINTS / Gene3D)
5. Signature Accession (e.g. PF09103 / G3DSA:2.40.50.140)
6. Signature Description (e.g. BRCA2 repeat profile)
7. Start location
8. Stop location
9. Score - is the e-value of the match reported by member database method (e.g. 3.1E-52)
10. Status - is the status of the match (T: true)
11. Date - is the date of the run
12. (InterPro annotations - accession (e.g. IPR002093) - optional column; only displayed if -iprscan option is switched on)
13. (InterPro annotations - description (e.g. BRCA2 repeat) - optional column; only displayed if -iprscan option is switched on)
14. (GO annotations (e.g. GO:0005515) - optional column; only displayed if --goterms option is switched on)
15. (Pathways annotations (e.g. REACT_71) - optional column; only displayed if --pathways option is switched on)


Check if all hits are annotated.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($10 == "F") print $0 }' | wc -l
#+END_SRC

Get the name of the databases that contain hits. And the total number of unfiltered hits.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ print $4 }' | sort - | uniq -c | sort -n
## output
     14 ProDom
     20 PIRSF
     37 TIGRFAM
    159 SMART
    314 Coils
    391 PRINTS
    783 Pfam
    788 SUPERFAMILY
    874 Gene3D
   1190 PANTHER
#+END_SRC

Get the number of hits per database at different e-values. Although the number of hits is filtered by evalue, it is not filtered by unique sequence entries. For example, a single contig translated in 6 different frames might be matched to 2 different domains because of 2 separate frames shifts.
#+BEGIN_SRC shell
## some databases dont include description of the accession number
## accession numbers are registered under columns $8 or $9
## so we must filter the $9 and $8 by evalue.
## $4 is correct for all
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($9<=.0000000001) print $4}' | sort - | uniq -c | sort -n
## and
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($8 <= .0000000001) print $4}' | sort - | uniq -c | sort -n

#+END_SRC

In interpro output 5 databases have the full number of columns (shown above) and 5 others dont. filtering should be separated if the options depend on the columns that come after the 4th.
Create a list for each set of database.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($8 <= .0000000001) print $4}' | sort - | uniq > db.without.acc.txt 

# AND
cat A.interpro.all.tsv | sed 's/ /./g' | awk '{ if ($9 <= .0000000001) print $4}' | sort - | uniq > db.with.acc.txt
#+END_SRC

Use these lists to filter separately the contigs by evalue and the sequence length of alignment. =hint= the calculated =x= returns an absolute value of the equation =end position - start - position=. Negative numbers might occur if the alignment is on the opposite strand.
#+BEGIN_SRC shell
cat A.interpro.all.tsv | sed 's/ /./g' | grep -Fwf ./db.without.acc.txt - | awk '{if($8 <= 0.00000000000000001) print $0}' | awk '{x=$6-$7?$7-$6:$6-$7; if(x>=10) print $4 }' | sort - | uniq -c | sort -n

#AND 
cat A.interpro.all.tsv | sed 's/ /./g' | grep -Fwf ./db.with.acc.txt - | awk '{if($9 <= 0.00000000000000000001) print $0}' | awk '{x=$7-$8?$8-$7:$7-$8; if(x>=20) print $4 }' | sort - | uniq -c | sort -n
#+END_SRC
*** STRING networks
The pipeline goes like this:
1. Align contigs to STRING (protein sequences file)
2. Get contigs and string IDs from =blastx= output
3. Get string networks (protein links file)
4. Get string actions (protein actions file)
5. Get species ID (second column of protein sequences file)
6. Get COG IDs (COG mappings file)
7. Get protein name (COG mapping file)
8. Get COG links to other orthologous groups (COG links file)

Get NCBI annotation with GI ids. =gene2accession= is a daily updated file from NCBI.
#+BEGIN_SRC shell
cat <(grep "^TRINITY" $blasted_contigs) | grep -Ff <(cat $R_log_dat_file | awk 'NR>1{print $1}' | sort - | uniq) - | cut -f3 | cut -f2 -d "|" | sort - | uniq | grep -Fwf - gene2accession > $output
#+END_SRC

* Create a database for structured data
Ideas from [[http://sfg.stanford.edu/BLAST.html][here]] and [[https://trinotate.github.io/][here]]
* XSEDE
** SDSC Gordon
Login and connect through secure network.
#+BEGIN_SRC shell
ssh -l silo gordon.sdsc.xsede.org
#+END_SRC

Shared directory with bassem. Huge space.
#+BEGIN_SRC shell
cd /oasis/project/nsf/sun108
#+END_SRC

Transfer files
#+BEGIN_SRC shell
scp file1 silo@gordon.sdsc.xsede.org:~/
scp -r folder ...
scp -C file # compress for fast transfer
#+END_SRC

Download files. (no need to create the destination folder)
#+BEGIN_SRC shell
rsync -auv bassem@gordon.sdsc.xsede.org:~/folder/ ./destination
#+END_SRC

Show remaining allocations and accounts. On SDSC 1 compute node for 1 hour = 16 SU (service unit) = 60 Gb ram = 16 cores. [[https://portal.xsede.org/sdsc-gordon#modules][Visit here]] for more modules and compiling instructions.
#+BEGIN_SRC shell
xdusage
show_accounts
#+END_SRC

Load modules. Packages that are installed.
#+BEGIN_SRC shell
module avail
module load R
module unload R
#+END_SRC

Create TORQUE batch file. 
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=1:ppn=16:native
#PBS -l walltime=1:00:00
#PBS -N makeblastdb
#PBS -o silo.out
#PBS -e silo.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

export PATH="$PATH:/home/bassem/blast/bin"
oasis=/oasis/projects/nsf/sun108
makeblastdb -in ${oasis}/bassem/db/nt/nt.fasta -out ${oasis}/bassem/db/nt/nt -dbtype nucl -parse_seqids
#+END_SRC

Monitor jobs. =qdel= to delete a running job with the job ID number.
#+BEGIN_SRC shell
qstat -a -u silo
qstat -f <job id>
#+END_SRC

Status of a job.
#+BEGIN_SRC shell
R = running
Q = queued
H = held
C = completed after having run
E = exiting after having run
#+END_SRC

Alter job properties. =important= One can reduce time remaining but not increase it.
#+BEGIN_SRC shell
qstat -a <job id>
qalter -l walltime=9:00 <job id>
qstat -a silo
#+END_SRC

Obtaining queue properties of a job.
#+BEGIN_SRC shell
qstat -q
#+END_SRC
** Analysis
Data are stored in :
#+BEGIN_SRC shell
cd /oasis/projects/nsf/sun108/silo
#+END_SRC

Blastx on =NR= database (updated on July 2015). =important= When changing from nucleotide to peptide blast search the BLASTDB must be change too. The alternative is to merge all database files into one directory.
#+BEGIN_SRC shell
#!/bin/bash
#PBS -q normal
#PBS -l nodes=10:ppn=16:native
#PBS -l walltime=48:00:00
#PBS -N blastx.A
#PBS -o blastxA.out
#PBS -e blastxA.err
#PBS -A sun108
#PBS -M sleiman.bassim@stonybrook.edu
#PBS -m abe
#PBS -V

export PATH="$PATH:/home/silo/blast/bin"
export BLASTDB="/oasis/projects/nsf/sun108/bassem/db/nr"
oasis=/oasis/projects/nsf/sun108

blastx -query ${oasis}/silo/nodule/assembled/A.assembl.QPXgv15.fasta \
-db nr \
-outfmt " 7 qseqid qlen sseqid slen qstart qend sstart send evalue bitscore length pident nident mismatch gaps staxids sscinames " \
-max_target_seqs 10 \
-out A.blastx.txt
#+END_SRC

* Bibliography
** First set
1. New tool in machine learning that finds splice junctions related to autism [[http://www.sciencemag.org/content/early/2014/12/17/science.1254806.short][Xiong 2014]] =science=
2. Difference in genome annotation (RefSeq, UCSC, Ensembl) is responsible for differences in read mapping to genes and transcription quantification [[http://www.biomedcentral.com/1471-2164/16/97][Zhao 2015]] =gene model=
3. Non-parametric approach to detect DETs from rnaseq data [[http://bioinformatics.oxfordjournals.org/content/early/2015/02/24/bioinformatics.btv119.abstract][Shi 2015]] =r friendly=
4. Co-expression analysis require high number of samples [[http://bioinformatics.oxfordjournals.org/content/early/2015/02/24/bioinformatics.btv118.full.pdf%2Bhtml][Ballouz 2015]] =metanalysis networks=
5. Co-expression and network construction from rnaseq data [[http://bioinformatics.oxfordjournals.org/content/28/12/1592.short][Iancu 2012]]
6. Multifunctionality is better than association for network inference [[http://journals.plos.org/plosone/article?id%3D10.1371/journal.pone.0017258][Gillis 2011]] =Pavlidis amd machine learning + pleiotropy=
7. SimSeq non parametric simulation engine for real rnaseq data [[http://bioinformatics.oxfordjournals.org/content/early/2015/02/26/bioinformatics.btv124.abstract][Benidt 2015]]
8. Overlapping genes and analysis of rnaseq data [[http://www.biomedcentral.com/1471-2105/16/S1/S3][Sun 2015]]
9. Phylogenetic analysis of the marine microbial transcriptome [[http://journals.plos.org/plosbiology/article?id%3D10.1371/journal.pbio.1001889][Keeling 2014]] =metagenomics=
10. Detect rna editing events fron rnaseq data [[http://onlinelibrary.wiley.com/doi/10.1002/0471250953.bi1212s49/abstract][Picardi 2015]] =python=
11. Orthologs from related species w/ rnaseq data [[http://www.biomedcentral.com/1471-2164/15/343?utm_source%3Ddlvr.it&utm_medium%3Dtumblr][Zhu 2014]] =vertebrates=
12. Orthologs from rnaseq expression data clustering analysis [[http://www.biomedcentral.com/content/pdf/gb-2014-15-8-r100.pdf][Yan 2014]] =networks=
13. Analysis of rnaseq expression data in Nature Protocols w/ R [[http://www.nature.com/nprot/journal/v8/n9/abs/nprot.2013.099.html][Anders 2013]] and [[http://link.springer.com/protocol/10.1007/978-1-4939-2444-8_24][Loraine 2015]] [[http://www.nature.com/nprot/journal/v7/n3/full/nprot.2012.016.html#ref12][Trapnell 2012]]=protocol=
14. edgeR paper [[http://bioinformatics.oxfordjournals.org/content/26/1/139.short][Robinson 2009]] =R=
15. Comparative paper of rnaseq packages [[http://www.nature.com/nmeth/journal/v8/n6/abs/nmeth.1613.html][Garber 2011]] =tools=
16. Machine learning for predicting gene expression from epigenetic data [[http://lungcancernewstoday.com/2015/03/23/new-prediction-model-for-gene-expression-in-lung-cancer-based-on-epigenetics/][Li 2015]]
17. Look for dsRNAs from rnaseq data after genome alignment [[http://rnajournal.cshlp.org/content/early/2015/03/24/rna.048801.114.full.pdf%2Bhtml][Whipple 2015]]
18. Gene expression of virulence, metabolism, and growth of QPX are temperature dependent [[http://journals.plos.org/plosone/article?id%3D10.1371/journal.pone.0074196][Vedrenne 2013]] =bad paper=
19. Retrotransposons as effectors and transmittors of immune cancer cells in clam [[http://www.sciencemag.org/content/348/6231/170.full][Metzger 2015]]
20. 

** Second set
1. How to characterize SNPs affected by the reference bias? Align reads to personalized genomes [[http://journals.plos.org/plosone/article?id%3D10.1371/journal.pone.0126911][Wood 2015]] =also ref. 26 and 28 inside=
2. Genome and transcriptome sequencing of single cell [[http://www.nature.com/nmeth/journal/v12/n6/full/nmeth.3370.html][Macaulay 2015]]
3. the next 20 years in genome research [[http://biorxiv.org/content/early/2015/06/02/020289.large.jpg?rss%3D1][Schatz 2015]]
4. Basic strategy on annotating a genome [[http://www.nature.com/nrg/journal/v13/n5/full/nrg3174.html#B22][Yandall 2012]] =review=
5. Terraformation of mars: importance of genome annotation and visualization [[http://motherboard.vice.com/read/darpa-we-are-engineering-the-organisms-that-will-terraform-mars][Jacksons lab]] =DARPA are engineering organisms=
6. Reference transcriptome and database used for gene annotation both influence variant caling [[http://www.biomedcentral.com/1471-2164/16/S8/S2][Franckish 2015]]
7. Cross sample contamination, viral, and pathogenic database contamination are real threat to sequencing data analysis [[http://jvi.asm.org/content/early/2015/06/11/JVI.00822-15.abstract][Kazemian 2015]]
8. 5-formylCytosine a DNA modified sugar that regulates genes [[http://www.nature.com/nchembio/journal/vaop/ncurrent/full/nchembio.1848.html][Backman 2015]]
9. Classification of reads between parasite and host [[http://www.plantmethods.com/content/11/1/34][Ikeue 2015]] =plant=
10. Finding parasitic genes [[http://www.plantphysiol.org/content/166/3/1186.long][Ranjan 2014]] =plant=
11. 2 SNPs linked to depression [[http://www.nature.com/nature/journal/vaop/ncurrent/full/nature14659.html#affil-auth][Converge consortium 2015]] =Nature=
12. Comparison of interface-built pipelines for rna-seq data [[http://bib.oxfordjournals.org/content/early/2015/06/23/bib.bbv036.short][Poplawski 2015]] =review=
13. Gene expression quantification by LFC [[http://nar.oxfordjournals.org/content/early/2015/07/08/nar.gkv696.short][Erhard 2015]] =estimate fold change=
14. Transcript quantification, new fast pipeline [[http://www.biorxiv.org/content/early/2015/06/27/021592.abstract][Patro 2015]] =gene expression=
15. The need to sequence C. virginica genome [[http://www.sciencedirect.com/science/article/pii/S1050464815002211][Gomez 2015]] =review=
16. Crosstalk between snail and parasite [[http://www.sciencedirect.com/science/article/pii/S1050464815000509][Coustau 2015]] =review=
17. How to recognize host-pathogen mechanisms [[http://ac.els-cdn.com/S0166685109000267/1-s2.0-S0166685109000267-main.pdf?_tid%3D58e521fa-2ef4-11e5-9802-00000aacb35d&acdnat%3D1437406450_c52e14fbc087a1152765fa0696a28730][Bayne 2009]] =review=
18. FPKM (fragments per 1kb per million reads) vs TPM (transcripts per million) [[https://liorpachter.wordpress.com/2014/04/30/estimating-number-of-(transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/][here]] and [[http://www.biomedcentral.com/1471-2105/12/323/][Li 2011]] =transcript quantification= (FPKM = depth of coverage + sum length of contigs, TPM = sum length of contigs + depth of coverage).
19. Identified molecular involvement host-pathogen [[http://www.sciencedirect.com/science/article/pii/S1050464815002429][He 2015]] =virus-oyster=
20. Normalization of rna-seq samples [[http://www.hindawi.com/journals/bmri/2015/621690/][Walczak 2015]] =review=
** Generalities
Lectins
1. Interaction with the complement
2. Key role in innate immune defense
3. Central role in filter feeding processes
4. Association with neurone morphology
5. Reduce functionality or absence cause diesease

Transposons
miRNAs
Virus

